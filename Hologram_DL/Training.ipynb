{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c42c35-b517-4e5c-8f79-033054decfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#현재 실행 중인 커널을 강제로 종료하는 명령어\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1aa2287-5a83-4baa-ac4d-dd1bb9690682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Import Libraries\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# 데이터 증강 및 전처리 설정 (수정)\n",
    "# 노이즈가 있는 환경에 강건하도록 다양한 증강 기법 추가\n",
    "phase_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.RandomHorizontalFlip(),  # 좌우 반전 추가\n",
    "    transforms.RandomVerticalFlip(),    # 상하 반전 추가\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b728a16-8585-4405-bec9-baf378973e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. Device Setup\n",
    "# =============================================================================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Available device: {device}')\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae7c776-53fd-47cd-b269-61a2a441fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. Custom Dataset Class\n",
    "# =============================================================================\n",
    "class HologramDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train'):\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        self.samples = sorted([\n",
    "            os.path.join(self.root_dir, f) for f in os.listdir(self.root_dir)\n",
    "            if f.startswith('sample_')\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_path = self.samples[idx]\n",
    "\n",
    "        phases = []\n",
    "        for i in range(4):\n",
    "            p = os.path.join(sample_path, f'phase_{i}.png')\n",
    "            img = Image.open(p)\n",
    "\n",
    "            # (선택) 크기 통일이 필요하면 여기서 리사이즈\n",
    "            img = img.resize((256, 256), resample=Image.BILINEAR)\n",
    "\n",
    "            arr = np.array(img)\n",
    "    \n",
    "            # 만약 HxWx3 형태(실수로 컬러 저장)면 첫 채널만 사용\n",
    "            if arr.ndim == 3:\n",
    "                arr = arr[..., 0]\n",
    "\n",
    "            # dtype별 정규화\n",
    "            if arr.dtype == np.uint16:\n",
    "                arr = arr.astype(np.float32) / 65535.0\n",
    "            elif arr.dtype == np.uint8:\n",
    "                arr = arr.astype(np.float32) / 255.0\n",
    "            else:\n",
    "                arr = arr.astype(np.float32)  # 이미 0~1이면 그대로\n",
    "\n",
    "            phases.append(arr)\n",
    "\n",
    "        phases = np.stack(phases, axis=0).astype(np.float32)  # (4, H, W)\n",
    "\n",
    "        input_phase   = torch.from_numpy(phases[0:1])  # (1, H, W)\n",
    "        target_phases = torch.from_numpy(phases[1:])   # (3, H, W)\n",
    "        return input_phase, target_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6cf8af-717a-4875-bb3b-9f866b253eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. Model Architecture Components (stride=2, kernel=3x3 반영)\n",
    "# =============================================================================\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.GroupNorm(32,out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=False),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 🔹 업샘플 먼저\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1, bias=False)  # 🔹 Conv2d로 후처리\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d803ed14-a458-44b1-b18e-b6d4480558c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. Main Model Architecture (stride=2 유지, 출력 크기 복원 포함)\n",
    "# =============================================================================\n",
    "class FINCH_DLPS_Net(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, 2, 1, bias=False),\n",
    "            nn.GroupNorm(32, 64), nn.LeakyReLU(0.2, inplace=False)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 2, 1, bias=False),\n",
    "            nn.GroupNorm(32, 128), nn.LeakyReLU(0.2, inplace=False)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, 2, 1, bias=False),\n",
    "            nn.GroupNorm(32, 256), nn.LeakyReLU(0.2, inplace=False)\n",
    "        )\n",
    "\n",
    "        self.dec1 = self._make_decoder()\n",
    "        self.dec2 = self._make_decoder()\n",
    "        self.dec3 = self._make_decoder()\n",
    "\n",
    "    def _make_decoder(self):\n",
    "        return nn.Sequential(\n",
    "            DecoderBlock(256, 128),                     # up1\n",
    "            EncoderBlock(256, 128),                     # conv1 (with skip)\n",
    "            DecoderBlock(128, 64),                      # up2\n",
    "            EncoderBlock(128, 64),                      # conv2 (with skip)\n",
    "            DecoderBlock(64, 64),                       # up3\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1), # final conv\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)  # (B, 64, H/2, W/2)\n",
    "        e2 = self.enc2(e1) # (B, 128, H/4, W/4)\n",
    "        e3 = self.enc3(e2) # (B, 256, H/8, W/8)\n",
    "\n",
    "        input_shape = x.shape[2:]  # (H, W)\n",
    "\n",
    "        d1 = self._decode(self.dec1, e1, e2, e3, input_shape)  # predict phase_1\n",
    "        d2 = self._decode(self.dec2, e1, e2, e3, input_shape)  # predict phase_2\n",
    "        d3 = self._decode(self.dec3, e1, e2, e3, input_shape)  # predict phase_3\n",
    "\n",
    "        return d1, d2, d3  # Each is (B, 1, H, W)\n",
    "\n",
    "    def _decode(self, dec, e1, e2, e3, input_shape):\n",
    "        x = dec[0](e3)                            # up to H/4\n",
    "        if x.shape[2:] != e2.shape[2:]:\n",
    "            x = F.interpolate(x, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = dec[1](torch.cat([x, e2], dim=1))     # fuse\n",
    "\n",
    "        x = dec[2](x)                             # up to H/2\n",
    "        if x.shape[2:] != e1.shape[2:]:\n",
    "            x = F.interpolate(x, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = dec[3](torch.cat([x, e1], dim=1))     # fuse\n",
    "\n",
    "        x = dec[4](x)                             # up to H\n",
    "        x = dec[5](x)                             # final 1ch\n",
    "        if x.shape[2:] != input_shape:\n",
    "            x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "        return x                                  # 🔸 linear 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2fa75a-d0df-4478-9672-ef835345a04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 완료:\n",
      "  - Train: 3500개 샘플\n",
      "  - Validation: 1000개 샘플\n",
      "  - Test: 500개 샘플\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. Data Loading and Preparation\n",
    "# =============================================================================\n",
    "# 홀로그램 데이터셋 로드\n",
    "dataset_root = 'hologram_dataset_images'\n",
    "batch_size = 10 # 홀로그램 데이터는 크기가 크므로 배치 크기 줄임\n",
    "\n",
    "train_dataset = HologramDataset(dataset_root, 'train')\n",
    "val_dataset = HologramDataset(dataset_root, 'validation')\n",
    "test_dataset = HologramDataset(dataset_root, 'test')\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"데이터 로드 완료:\")\n",
    "print(f\"  - Train: {len(train_dataset)}개 샘플\")\n",
    "print(f\"  - Validation: {len(val_dataset)}개 샘플\")\n",
    "print(f\"  - Test: {len(test_dataset)}개 샘플\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95970d42-85fc-4f10-a191-b202cadfed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SSIM + RMSE + 위상 일관성 손실 함수\n",
    "# =============================================================================\n",
    "def ssim(img1, img2, C1=0.01**2, C2=0.03**2):\n",
    "    mu1 = F.avg_pool2d(img1, 3, 1, 1)\n",
    "    mu2 = F.avg_pool2d(img2, 3, 1, 1)\n",
    "    sigma1_sq = F.avg_pool2d(img1 * img1, 3, 1, 1) - mu1 ** 2\n",
    "    sigma2_sq = F.avg_pool2d(img2 * img2, 3, 1, 1) - mu2 ** 2\n",
    "    sigma12 = F.avg_pool2d(img1 * img2, 3, 1, 1) - mu1 * mu2\n",
    "    ssim_map = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
    "               ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "    return ssim_map.mean()\n",
    "    \n",
    "def improved_physics_loss(output1, target1, output2, target2, output3, target3, epoch=0):\n",
    "    eps = 1e-8\n",
    "\n",
    "    # --- RMSE 계산 ---\n",
    "    rmse1 = torch.sqrt(torch.mean((output1 - target1) ** 2) + eps)\n",
    "    rmse2 = torch.sqrt(torch.mean((output2 - target2) ** 2) + eps)\n",
    "    rmse3 = torch.sqrt(torch.mean((output3 - target3) ** 2) + eps)\n",
    "    base_rmse = (rmse1 + rmse2 + rmse3) /3\n",
    "\n",
    "    if epoch<10:\n",
    "        return base_rmse, {'rmse': base_rmse.item(), 'consistency':0, 'ssim':0,\n",
    "                           'weight_consistency':0, 'weight_ssim':0}\n",
    "\n",
    "    # --- 위상 일관성 손실 (phase consistency) ---\n",
    "    pred_diff_1 = output1 - output3\n",
    "    pred_diff_2 = output2 - output3\n",
    "    target_diff_1 = target1 - target3\n",
    "    target_diff_2 = target2 - target3\n",
    "    consistency_loss = F.mse_loss(pred_diff_1, target_diff_1) + F.mse_loss(pred_diff_2, target_diff_2)\n",
    "\n",
    "    # --- SSIM 손실 계산 ---\n",
    "    ssim_loss1 = 1 - ssim(output1, target1)\n",
    "    ssim_loss2 = 1 - ssim(output2, target2)\n",
    "    ssim_loss3 = 1 - ssim(output3, target3)\n",
    "    ssim_total = (ssim_loss1 + ssim_loss2 + ssim_loss3) / 3\n",
    "\n",
    "    consistency_weight = 0.01\n",
    "    ssim_weight = 0.1\n",
    "\n",
    "    # --- 총 loss ---\n",
    "    total_loss = base_rmse + consistency_weight * consistency_loss + ssim_weight * ssim_total\n",
    "\n",
    "    # --- NaN 방지 ---\n",
    "    if torch.isnan(total_loss):\n",
    "        total_loss = base_rmse\n",
    "\n",
    "    # --- 리턴 (loss + 상세정보) ---\n",
    "    return total_loss, {\n",
    "        'rmse': base_rmse.item(),\n",
    "        'consistency': consistency_loss.item(),\n",
    "        'ssim': ssim_total.item(),\n",
    "        'weight_consistency': consistency_weight,\n",
    "        'weight_ssim': ssim_weight\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b838053d-6736-4296-b2a2-809fcaf6b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 사용: cuda:0\n",
      "=== 설정 ===\n",
      "학습률: 0.001\n",
      "훈련 에포크: 80\n",
      "가중치 감소: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 모델 초기화 및 옵티마이저/스케줄러 설정\n",
    "# =============================================================================\n",
    "model = FINCH_DLPS_Net().to(device)\n",
    "print(f\"GPU 사용: {next(model.parameters()).device}\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 80\n",
    "weight_decay = 0.0001\n",
    "\n",
    "print(\"=== 설정 ===\")\n",
    "print(f\"학습률: {learning_rate}\")\n",
    "print(f\"훈련 에포크: {training_epochs}\")\n",
    "print(f\"가중치 감소: {weight_decay}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8,\n",
    "    amsgrad=False\n",
    ")\n",
    "\n",
    "max_batches = len(train_loader)\n",
    "# 변경\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=2e-3,  # 최대로 올릴 학습률\n",
    "    steps_per_epoch=max_batches,\n",
    "    epochs=training_epochs,\n",
    "    pct_start=0.05, # 전체 학습의 10% 구간에서 최대 lr 도달\n",
    "    div_factor=5,\n",
    "    final_div_factor=200,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience_limit = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7ec154-2ff9-47db-b384-6d0b039220ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수: 3500\n",
      "배치 크기: 10\n",
      "총 배치 개수: 100\n",
      "============================================================\n",
      "\n",
      "📊 Epoch 1/80 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_10580\\3889278418.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_10580\\3889278418.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 1/100] Loss: 0.4378 | RMSE: 0.4378 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 386652.281\n",
      "[Epoch 1, Batch 2/100] Loss: 0.3863 | RMSE: 0.3863 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 365825.719\n",
      "[Epoch 1, Batch 3/100] Loss: 0.3164 | RMSE: 0.3164 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 335615.375\n",
      "[Epoch 1, Batch 4/100] Loss: 0.2720 | RMSE: 0.2720 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 293985.781\n",
      "[Epoch 1, Batch 5/100] Loss: 0.2100 | RMSE: 0.2100 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 223979.375\n",
      "[Epoch 1, Batch 6/100] Loss: 0.1866 | RMSE: 0.1866 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 158926.500\n",
      "[Epoch 1, Batch 7/100] Loss: 0.2014 | RMSE: 0.2014 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 158089.141\n",
      "[Epoch 1, Batch 8/100] Loss: 0.1988 | RMSE: 0.1988 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 174487.969\n",
      "[Epoch 1, Batch 9/100] Loss: 0.2220 | RMSE: 0.2220 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 243316.453\n",
      "[Epoch 1, Batch 10/100] Loss: 0.2079 | RMSE: 0.2079 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 203681.047\n",
      "[Epoch 1, Batch 11/100] Loss: 0.2225 | RMSE: 0.2225 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 249262.734\n",
      "[Epoch 1, Batch 12/100] Loss: 0.2024 | RMSE: 0.2024 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 176312.516\n",
      "[Epoch 1, Batch 13/100] Loss: 0.1844 | RMSE: 0.1844 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 119779.555\n",
      "[Epoch 1, Batch 14/100] Loss: 0.1715 | RMSE: 0.1715 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 109742.273\n",
      "[Epoch 1, Batch 15/100] Loss: 0.1761 | RMSE: 0.1761 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102297.852\n",
      "[Epoch 1, Batch 16/100] Loss: 0.1688 | RMSE: 0.1688 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100677.461\n",
      "[Epoch 1, Batch 17/100] Loss: 0.1672 | RMSE: 0.1672 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 89027.570\n",
      "[Epoch 1, Batch 18/100] Loss: 0.1786 | RMSE: 0.1786 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 107086.625\n",
      "[Epoch 1, Batch 19/100] Loss: 0.1776 | RMSE: 0.1776 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 126986.695\n",
      "[Epoch 1, Batch 20/100] Loss: 0.1719 | RMSE: 0.1719 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85768.531\n",
      "[Epoch 1, Batch 21/100] Loss: 0.1729 | RMSE: 0.1729 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87808.453\n",
      "[Epoch 1, Batch 22/100] Loss: 0.1713 | RMSE: 0.1713 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105547.531\n",
      "[Epoch 1, Batch 23/100] Loss: 0.1640 | RMSE: 0.1640 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 99085.594\n",
      "[Epoch 1, Batch 24/100] Loss: 0.1769 | RMSE: 0.1769 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 104414.359\n",
      "[Epoch 1, Batch 25/100] Loss: 0.1595 | RMSE: 0.1595 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102868.836\n",
      "[Epoch 1, Batch 26/100] Loss: 0.1709 | RMSE: 0.1709 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 106423.227\n",
      "[Epoch 1, Batch 27/100] Loss: 0.1672 | RMSE: 0.1672 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69048.562\n",
      "[Epoch 1, Batch 28/100] Loss: 0.1682 | RMSE: 0.1682 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59326.094\n",
      "[Epoch 1, Batch 29/100] Loss: 0.1605 | RMSE: 0.1605 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45293.262\n",
      "[Epoch 1, Batch 30/100] Loss: 0.1663 | RMSE: 0.1663 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 38229.688\n",
      "[Epoch 1, Batch 31/100] Loss: 0.1670 | RMSE: 0.1670 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73151.445\n",
      "[Epoch 1, Batch 32/100] Loss: 0.1639 | RMSE: 0.1639 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 96985.828\n",
      "[Epoch 1, Batch 33/100] Loss: 0.1678 | RMSE: 0.1678 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58924.152\n",
      "[Epoch 1, Batch 34/100] Loss: 0.1593 | RMSE: 0.1593 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 112140.422\n",
      "[Epoch 1, Batch 35/100] Loss: 0.1549 | RMSE: 0.1549 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 75666.750\n",
      "[Epoch 1, Batch 36/100] Loss: 0.1556 | RMSE: 0.1556 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74171.555\n",
      "[Epoch 1, Batch 37/100] Loss: 0.1559 | RMSE: 0.1559 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82449.594\n",
      "[Epoch 1, Batch 38/100] Loss: 0.1526 | RMSE: 0.1526 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 33872.758\n",
      "[Epoch 1, Batch 39/100] Loss: 0.1517 | RMSE: 0.1517 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65684.820\n",
      "[Epoch 1, Batch 40/100] Loss: 0.1635 | RMSE: 0.1635 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86883.406\n",
      "[Epoch 1, Batch 41/100] Loss: 0.1647 | RMSE: 0.1647 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68945.188\n",
      "[Epoch 1, Batch 42/100] Loss: 0.1552 | RMSE: 0.1552 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 50741.289\n",
      "[Epoch 1, Batch 43/100] Loss: 0.1616 | RMSE: 0.1616 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86067.719\n",
      "[Epoch 1, Batch 44/100] Loss: 0.1563 | RMSE: 0.1563 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65199.426\n",
      "[Epoch 1, Batch 45/100] Loss: 0.1592 | RMSE: 0.1592 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 52042.301\n",
      "[Epoch 1, Batch 46/100] Loss: 0.1553 | RMSE: 0.1553 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85268.438\n",
      "[Epoch 1, Batch 47/100] Loss: 0.1607 | RMSE: 0.1607 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105343.477\n",
      "[Epoch 1, Batch 48/100] Loss: 0.1511 | RMSE: 0.1511 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105565.719\n",
      "[Epoch 1, Batch 49/100] Loss: 0.1511 | RMSE: 0.1511 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78224.133\n",
      "[Epoch 1, Batch 50/100] Loss: 0.1537 | RMSE: 0.1537 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47052.520\n",
      "[Epoch 1, Batch 51/100] Loss: 0.1549 | RMSE: 0.1549 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65147.566\n",
      "[Epoch 1, Batch 52/100] Loss: 0.1605 | RMSE: 0.1605 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 104157.938\n",
      "[Epoch 1, Batch 53/100] Loss: 0.1575 | RMSE: 0.1575 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88610.508\n",
      "[Epoch 1, Batch 54/100] Loss: 0.1517 | RMSE: 0.1517 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 39749.055\n",
      "[Epoch 1, Batch 55/100] Loss: 0.1559 | RMSE: 0.1559 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 131203.156\n",
      "[Epoch 1, Batch 56/100] Loss: 0.1583 | RMSE: 0.1583 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62122.227\n",
      "[Epoch 1, Batch 57/100] Loss: 0.1556 | RMSE: 0.1556 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93877.242\n",
      "[Epoch 1, Batch 58/100] Loss: 0.1568 | RMSE: 0.1568 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 96394.312\n",
      "[Epoch 1, Batch 59/100] Loss: 0.1555 | RMSE: 0.1555 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86611.195\n",
      "[Epoch 1, Batch 60/100] Loss: 0.1569 | RMSE: 0.1569 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66996.227\n",
      "[Epoch 1, Batch 61/100] Loss: 0.1472 | RMSE: 0.1472 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 97640.891\n",
      "[Epoch 1, Batch 62/100] Loss: 0.1482 | RMSE: 0.1482 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 36548.770\n",
      "[Epoch 1, Batch 63/100] Loss: 0.1475 | RMSE: 0.1475 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93805.758\n",
      "[Epoch 1, Batch 64/100] Loss: 0.1467 | RMSE: 0.1467 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102699.773\n",
      "[Epoch 1, Batch 65/100] Loss: 0.1559 | RMSE: 0.1559 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64340.691\n",
      "[Epoch 1, Batch 66/100] Loss: 0.1546 | RMSE: 0.1546 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 97573.117\n",
      "[Epoch 1, Batch 67/100] Loss: 0.1612 | RMSE: 0.1612 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 113437.867\n",
      "[Epoch 1, Batch 68/100] Loss: 0.1544 | RMSE: 0.1544 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 99307.516\n",
      "[Epoch 1, Batch 69/100] Loss: 0.1511 | RMSE: 0.1511 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81214.602\n",
      "[Epoch 1, Batch 70/100] Loss: 0.1647 | RMSE: 0.1647 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 122670.281\n",
      "[Epoch 1, Batch 71/100] Loss: 0.1614 | RMSE: 0.1614 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57218.270\n",
      "[Epoch 1, Batch 72/100] Loss: 0.1578 | RMSE: 0.1578 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54975.000\n",
      "[Epoch 1, Batch 73/100] Loss: 0.1494 | RMSE: 0.1494 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 56995.117\n",
      "[Epoch 1, Batch 74/100] Loss: 0.1527 | RMSE: 0.1527 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 111424.516\n",
      "[Epoch 1, Batch 75/100] Loss: 0.1677 | RMSE: 0.1677 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47275.512\n",
      "[Epoch 1, Batch 76/100] Loss: 0.1500 | RMSE: 0.1500 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67810.102\n",
      "[Epoch 1, Batch 77/100] Loss: 0.1516 | RMSE: 0.1516 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85428.852\n",
      "[Epoch 1, Batch 78/100] Loss: 0.1425 | RMSE: 0.1425 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62056.742\n",
      "[Epoch 1, Batch 79/100] Loss: 0.1485 | RMSE: 0.1485 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87050.469\n",
      "[Epoch 1, Batch 80/100] Loss: 0.1563 | RMSE: 0.1563 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85506.109\n",
      "[Epoch 1, Batch 81/100] Loss: 0.1523 | RMSE: 0.1523 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70422.812\n",
      "[Epoch 1, Batch 82/100] Loss: 0.1485 | RMSE: 0.1485 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55577.223\n",
      "[Epoch 1, Batch 83/100] Loss: 0.1480 | RMSE: 0.1480 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42844.965\n",
      "[Epoch 1, Batch 84/100] Loss: 0.1505 | RMSE: 0.1505 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64253.750\n",
      "[Epoch 1, Batch 85/100] Loss: 0.1482 | RMSE: 0.1482 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 115780.828\n",
      "[Epoch 1, Batch 86/100] Loss: 0.1505 | RMSE: 0.1505 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100771.734\n",
      "[Epoch 1, Batch 87/100] Loss: 0.1571 | RMSE: 0.1571 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 147396.469\n",
      "[Epoch 1, Batch 88/100] Loss: 0.1492 | RMSE: 0.1492 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 101764.508\n",
      "[Epoch 1, Batch 89/100] Loss: 0.1481 | RMSE: 0.1481 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 97161.578\n",
      "[Epoch 1, Batch 90/100] Loss: 0.1458 | RMSE: 0.1458 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48940.117\n",
      "[Epoch 1, Batch 91/100] Loss: 0.1543 | RMSE: 0.1543 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 125924.188\n",
      "[Epoch 1, Batch 92/100] Loss: 0.1473 | RMSE: 0.1473 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90478.625\n",
      "[Epoch 1, Batch 93/100] Loss: 0.1476 | RMSE: 0.1476 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87779.078\n",
      "[Epoch 1, Batch 94/100] Loss: 0.1513 | RMSE: 0.1513 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73124.445\n",
      "[Epoch 1, Batch 95/100] Loss: 0.1417 | RMSE: 0.1417 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 35548.301\n",
      "[Epoch 1, Batch 96/100] Loss: 0.1351 | RMSE: 0.1351 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71507.828\n",
      "[Epoch 1, Batch 97/100] Loss: 0.1518 | RMSE: 0.1518 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100203.234\n",
      "[Epoch 1, Batch 98/100] Loss: 0.1409 | RMSE: 0.1409 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86045.484\n",
      "[Epoch 1, Batch 99/100] Loss: 0.1463 | RMSE: 0.1463 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82160.094\n",
      "[Epoch 1, Batch 100/100] Loss: 0.1543 | RMSE: 0.1543 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 98776.750\n",
      "✅ Epoch 1 완료!\n",
      "   📈 평균 Loss: 0.169013\n",
      "   📊 평균 RMSE: 0.169013\n",
      "   ⚙️  학습률: 0.000077\n",
      "   🏆 NEW BEST! RMSE: 0.169013\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 2/80 시작...\n",
      "[Epoch 2, Batch 1/100] Loss: 0.1403 | RMSE: 0.1403 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74221.945\n",
      "[Epoch 2, Batch 2/100] Loss: 0.1560 | RMSE: 0.1560 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61543.504\n",
      "[Epoch 2, Batch 3/100] Loss: 0.1516 | RMSE: 0.1516 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58076.438\n",
      "[Epoch 2, Batch 4/100] Loss: 0.1477 | RMSE: 0.1477 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74210.727\n",
      "[Epoch 2, Batch 5/100] Loss: 0.1450 | RMSE: 0.1450 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100503.648\n",
      "[Epoch 2, Batch 6/100] Loss: 0.1426 | RMSE: 0.1426 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71032.133\n",
      "[Epoch 2, Batch 7/100] Loss: 0.1535 | RMSE: 0.1535 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83161.391\n",
      "[Epoch 2, Batch 8/100] Loss: 0.1512 | RMSE: 0.1512 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86757.125\n",
      "[Epoch 2, Batch 9/100] Loss: 0.1471 | RMSE: 0.1471 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 112956.133\n",
      "[Epoch 2, Batch 10/100] Loss: 0.1493 | RMSE: 0.1493 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57053.445\n",
      "[Epoch 2, Batch 11/100] Loss: 0.1485 | RMSE: 0.1485 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71179.953\n",
      "[Epoch 2, Batch 12/100] Loss: 0.1452 | RMSE: 0.1452 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69824.766\n",
      "[Epoch 2, Batch 13/100] Loss: 0.1449 | RMSE: 0.1449 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80489.094\n",
      "[Epoch 2, Batch 14/100] Loss: 0.1371 | RMSE: 0.1371 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65164.395\n",
      "[Epoch 2, Batch 15/100] Loss: 0.1490 | RMSE: 0.1490 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57951.859\n",
      "[Epoch 2, Batch 16/100] Loss: 0.1419 | RMSE: 0.1419 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70105.500\n",
      "[Epoch 2, Batch 17/100] Loss: 0.1497 | RMSE: 0.1497 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 127179.320\n",
      "[Epoch 2, Batch 18/100] Loss: 0.1447 | RMSE: 0.1447 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66096.711\n",
      "[Epoch 2, Batch 19/100] Loss: 0.1362 | RMSE: 0.1362 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66138.766\n",
      "[Epoch 2, Batch 20/100] Loss: 0.1416 | RMSE: 0.1416 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69717.875\n",
      "[Epoch 2, Batch 21/100] Loss: 0.1467 | RMSE: 0.1467 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 116770.500\n",
      "[Epoch 2, Batch 22/100] Loss: 0.1488 | RMSE: 0.1488 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85927.594\n",
      "[Epoch 2, Batch 23/100] Loss: 0.1481 | RMSE: 0.1481 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 138382.578\n",
      "[Epoch 2, Batch 24/100] Loss: 0.1425 | RMSE: 0.1425 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62326.012\n",
      "[Epoch 2, Batch 25/100] Loss: 0.1494 | RMSE: 0.1494 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58885.062\n",
      "[Epoch 2, Batch 26/100] Loss: 0.1445 | RMSE: 0.1445 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47058.078\n",
      "[Epoch 2, Batch 27/100] Loss: 0.1516 | RMSE: 0.1516 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 107022.305\n",
      "[Epoch 2, Batch 28/100] Loss: 0.1429 | RMSE: 0.1429 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102567.086\n",
      "[Epoch 2, Batch 29/100] Loss: 0.1496 | RMSE: 0.1496 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 136674.703\n",
      "[Epoch 2, Batch 30/100] Loss: 0.1613 | RMSE: 0.1613 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 174317.656\n",
      "[Epoch 2, Batch 31/100] Loss: 0.1557 | RMSE: 0.1557 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91230.016\n",
      "[Epoch 2, Batch 32/100] Loss: 0.1404 | RMSE: 0.1404 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45326.320\n",
      "[Epoch 2, Batch 33/100] Loss: 0.1359 | RMSE: 0.1359 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74461.734\n",
      "[Epoch 2, Batch 34/100] Loss: 0.1441 | RMSE: 0.1441 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60119.902\n",
      "[Epoch 2, Batch 35/100] Loss: 0.1421 | RMSE: 0.1421 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92426.422\n",
      "[Epoch 2, Batch 36/100] Loss: 0.1511 | RMSE: 0.1511 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 130401.305\n",
      "[Epoch 2, Batch 37/100] Loss: 0.1528 | RMSE: 0.1528 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 147875.125\n",
      "[Epoch 2, Batch 38/100] Loss: 0.1559 | RMSE: 0.1559 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 144129.859\n",
      "[Epoch 2, Batch 39/100] Loss: 0.1404 | RMSE: 0.1404 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 113773.914\n",
      "[Epoch 2, Batch 40/100] Loss: 0.1550 | RMSE: 0.1550 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 130724.227\n",
      "[Epoch 2, Batch 41/100] Loss: 0.1534 | RMSE: 0.1534 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 120077.797\n",
      "[Epoch 2, Batch 42/100] Loss: 0.1432 | RMSE: 0.1432 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67639.703\n",
      "[Epoch 2, Batch 43/100] Loss: 0.1430 | RMSE: 0.1430 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92946.156\n",
      "[Epoch 2, Batch 44/100] Loss: 0.1435 | RMSE: 0.1435 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85501.000\n",
      "[Epoch 2, Batch 45/100] Loss: 0.1418 | RMSE: 0.1418 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51552.629\n",
      "[Epoch 2, Batch 46/100] Loss: 0.1452 | RMSE: 0.1452 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 99408.914\n",
      "[Epoch 2, Batch 47/100] Loss: 0.1544 | RMSE: 0.1544 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 135906.297\n",
      "[Epoch 2, Batch 48/100] Loss: 0.1593 | RMSE: 0.1593 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 113170.852\n",
      "[Epoch 2, Batch 49/100] Loss: 0.1556 | RMSE: 0.1556 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117066.648\n",
      "[Epoch 2, Batch 50/100] Loss: 0.1545 | RMSE: 0.1545 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82426.922\n",
      "[Epoch 2, Batch 51/100] Loss: 0.1478 | RMSE: 0.1478 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81440.320\n",
      "[Epoch 2, Batch 52/100] Loss: 0.1433 | RMSE: 0.1433 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 89695.906\n",
      "[Epoch 2, Batch 53/100] Loss: 0.1468 | RMSE: 0.1468 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45495.855\n",
      "[Epoch 2, Batch 54/100] Loss: 0.1440 | RMSE: 0.1440 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70615.406\n",
      "[Epoch 2, Batch 55/100] Loss: 0.1609 | RMSE: 0.1609 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 138025.547\n",
      "[Epoch 2, Batch 56/100] Loss: 0.1563 | RMSE: 0.1563 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 137419.531\n",
      "[Epoch 2, Batch 57/100] Loss: 0.1570 | RMSE: 0.1570 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 139143.891\n",
      "[Epoch 2, Batch 58/100] Loss: 0.1562 | RMSE: 0.1562 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 122483.883\n",
      "[Epoch 2, Batch 59/100] Loss: 0.1348 | RMSE: 0.1348 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 124695.477\n",
      "[Epoch 2, Batch 60/100] Loss: 0.1420 | RMSE: 0.1420 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78586.547\n",
      "[Epoch 2, Batch 61/100] Loss: 0.1396 | RMSE: 0.1396 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81326.852\n",
      "[Epoch 2, Batch 62/100] Loss: 0.1468 | RMSE: 0.1468 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 132296.547\n",
      "[Epoch 2, Batch 63/100] Loss: 0.1432 | RMSE: 0.1432 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 110754.562\n",
      "[Epoch 2, Batch 64/100] Loss: 0.1452 | RMSE: 0.1452 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69302.688\n",
      "[Epoch 2, Batch 65/100] Loss: 0.1432 | RMSE: 0.1432 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62392.121\n",
      "[Epoch 2, Batch 66/100] Loss: 0.1579 | RMSE: 0.1579 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 107891.258\n",
      "[Epoch 2, Batch 67/100] Loss: 0.1480 | RMSE: 0.1480 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 143686.766\n",
      "[Epoch 2, Batch 68/100] Loss: 0.1546 | RMSE: 0.1546 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 130608.234\n",
      "[Epoch 2, Batch 69/100] Loss: 0.1488 | RMSE: 0.1488 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 134909.047\n",
      "[Epoch 2, Batch 70/100] Loss: 0.1555 | RMSE: 0.1555 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 131469.438\n",
      "[Epoch 2, Batch 71/100] Loss: 0.1518 | RMSE: 0.1518 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 137592.469\n",
      "[Epoch 2, Batch 72/100] Loss: 0.1501 | RMSE: 0.1501 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 101773.109\n",
      "[Epoch 2, Batch 73/100] Loss: 0.1501 | RMSE: 0.1501 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 104952.859\n",
      "[Epoch 2, Batch 74/100] Loss: 0.1397 | RMSE: 0.1397 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91687.320\n",
      "[Epoch 2, Batch 75/100] Loss: 0.1357 | RMSE: 0.1357 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 111760.922\n",
      "[Epoch 2, Batch 76/100] Loss: 0.1423 | RMSE: 0.1423 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51849.574\n",
      "[Epoch 2, Batch 77/100] Loss: 0.1418 | RMSE: 0.1418 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66878.148\n",
      "[Epoch 2, Batch 78/100] Loss: 0.1460 | RMSE: 0.1460 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59593.395\n",
      "[Epoch 2, Batch 79/100] Loss: 0.1512 | RMSE: 0.1512 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 110478.023\n",
      "[Epoch 2, Batch 80/100] Loss: 0.1471 | RMSE: 0.1471 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 130328.148\n",
      "[Epoch 2, Batch 81/100] Loss: 0.1470 | RMSE: 0.1470 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76080.852\n",
      "[Epoch 2, Batch 82/100] Loss: 0.1419 | RMSE: 0.1419 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79663.430\n",
      "[Epoch 2, Batch 83/100] Loss: 0.1445 | RMSE: 0.1445 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80861.992\n",
      "[Epoch 2, Batch 84/100] Loss: 0.1372 | RMSE: 0.1372 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77211.680\n",
      "[Epoch 2, Batch 85/100] Loss: 0.1414 | RMSE: 0.1414 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83164.367\n",
      "[Epoch 2, Batch 86/100] Loss: 0.1469 | RMSE: 0.1469 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 144370.344\n",
      "[Epoch 2, Batch 87/100] Loss: 0.1397 | RMSE: 0.1397 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 124894.602\n",
      "[Epoch 2, Batch 88/100] Loss: 0.1416 | RMSE: 0.1416 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92540.977\n",
      "[Epoch 2, Batch 89/100] Loss: 0.1489 | RMSE: 0.1489 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100232.141\n",
      "[Epoch 2, Batch 90/100] Loss: 0.1427 | RMSE: 0.1427 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74358.102\n",
      "[Epoch 2, Batch 91/100] Loss: 0.1535 | RMSE: 0.1535 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 108017.188\n",
      "[Epoch 2, Batch 92/100] Loss: 0.1472 | RMSE: 0.1472 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 131138.359\n",
      "[Epoch 2, Batch 93/100] Loss: 0.1486 | RMSE: 0.1486 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 151856.312\n",
      "[Epoch 2, Batch 94/100] Loss: 0.1509 | RMSE: 0.1509 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 149809.234\n",
      "[Epoch 2, Batch 95/100] Loss: 0.1427 | RMSE: 0.1427 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77903.234\n",
      "[Epoch 2, Batch 96/100] Loss: 0.1375 | RMSE: 0.1375 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64612.906\n",
      "[Epoch 2, Batch 97/100] Loss: 0.1446 | RMSE: 0.1446 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 155456.469\n",
      "[Epoch 2, Batch 98/100] Loss: 0.1459 | RMSE: 0.1459 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 147329.500\n",
      "[Epoch 2, Batch 99/100] Loss: 0.1420 | RMSE: 0.1420 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105843.125\n",
      "[Epoch 2, Batch 100/100] Loss: 0.1357 | RMSE: 0.1357 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59345.918\n",
      "✅ Epoch 2 완료!\n",
      "   📈 평균 Loss: 0.146840\n",
      "   📊 평균 RMSE: 0.146840\n",
      "   ⚙️  학습률: 0.000181\n",
      "   🏆 NEW BEST! RMSE: 0.146840\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 3/80 시작...\n",
      "[Epoch 3, Batch 1/100] Loss: 0.1431 | RMSE: 0.1431 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57263.668\n",
      "[Epoch 3, Batch 2/100] Loss: 0.1333 | RMSE: 0.1333 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88999.758\n",
      "[Epoch 3, Batch 3/100] Loss: 0.1469 | RMSE: 0.1469 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 136627.609\n",
      "[Epoch 3, Batch 4/100] Loss: 0.1475 | RMSE: 0.1475 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 112368.211\n",
      "[Epoch 3, Batch 5/100] Loss: 0.1510 | RMSE: 0.1510 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 118997.578\n",
      "[Epoch 3, Batch 6/100] Loss: 0.1551 | RMSE: 0.1551 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 145962.953\n",
      "[Epoch 3, Batch 7/100] Loss: 0.1468 | RMSE: 0.1468 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 109469.109\n",
      "[Epoch 3, Batch 8/100] Loss: 0.1435 | RMSE: 0.1435 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 145708.000\n",
      "[Epoch 3, Batch 9/100] Loss: 0.1453 | RMSE: 0.1453 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 110390.055\n",
      "[Epoch 3, Batch 10/100] Loss: 0.1335 | RMSE: 0.1335 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82419.977\n",
      "[Epoch 3, Batch 11/100] Loss: 0.1314 | RMSE: 0.1314 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 89595.789\n",
      "[Epoch 3, Batch 12/100] Loss: 0.1425 | RMSE: 0.1425 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 98218.102\n",
      "[Epoch 3, Batch 13/100] Loss: 0.1404 | RMSE: 0.1404 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87667.688\n",
      "[Epoch 3, Batch 14/100] Loss: 0.1397 | RMSE: 0.1397 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92380.242\n",
      "[Epoch 3, Batch 15/100] Loss: 0.1523 | RMSE: 0.1523 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 150568.578\n",
      "[Epoch 3, Batch 16/100] Loss: 0.1354 | RMSE: 0.1354 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105582.633\n",
      "[Epoch 3, Batch 17/100] Loss: 0.1414 | RMSE: 0.1414 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81533.406\n",
      "[Epoch 3, Batch 18/100] Loss: 0.1398 | RMSE: 0.1398 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87633.547\n",
      "[Epoch 3, Batch 19/100] Loss: 0.1389 | RMSE: 0.1389 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 127590.383\n",
      "[Epoch 3, Batch 20/100] Loss: 0.1383 | RMSE: 0.1383 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 119093.188\n",
      "[Epoch 3, Batch 21/100] Loss: 0.1379 | RMSE: 0.1379 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 94612.727\n",
      "[Epoch 3, Batch 22/100] Loss: 0.1456 | RMSE: 0.1456 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 111324.172\n",
      "[Epoch 3, Batch 23/100] Loss: 0.1442 | RMSE: 0.1442 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 134493.031\n",
      "[Epoch 3, Batch 24/100] Loss: 0.1378 | RMSE: 0.1378 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 112460.297\n",
      "[Epoch 3, Batch 25/100] Loss: 0.1380 | RMSE: 0.1380 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 120127.469\n",
      "[Epoch 3, Batch 26/100] Loss: 0.1490 | RMSE: 0.1490 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92219.578\n",
      "[Epoch 3, Batch 27/100] Loss: 0.1491 | RMSE: 0.1491 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 164586.766\n",
      "[Epoch 3, Batch 28/100] Loss: 0.1522 | RMSE: 0.1522 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 129667.039\n",
      "[Epoch 3, Batch 29/100] Loss: 0.1458 | RMSE: 0.1458 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 119055.078\n",
      "[Epoch 3, Batch 30/100] Loss: 0.1448 | RMSE: 0.1448 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85898.930\n",
      "[Epoch 3, Batch 31/100] Loss: 0.1379 | RMSE: 0.1379 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 121843.875\n",
      "[Epoch 3, Batch 32/100] Loss: 0.1423 | RMSE: 0.1423 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 109273.312\n",
      "[Epoch 3, Batch 33/100] Loss: 0.1470 | RMSE: 0.1470 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 108049.102\n",
      "[Epoch 3, Batch 34/100] Loss: 0.1432 | RMSE: 0.1432 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 136317.312\n",
      "[Epoch 3, Batch 35/100] Loss: 0.1489 | RMSE: 0.1489 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 107368.742\n",
      "[Epoch 3, Batch 36/100] Loss: 0.1327 | RMSE: 0.1327 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92924.391\n",
      "[Epoch 3, Batch 37/100] Loss: 0.1334 | RMSE: 0.1334 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69621.109\n",
      "[Epoch 3, Batch 38/100] Loss: 0.1406 | RMSE: 0.1406 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80111.062\n",
      "[Epoch 3, Batch 39/100] Loss: 0.1351 | RMSE: 0.1351 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88671.219\n",
      "[Epoch 3, Batch 40/100] Loss: 0.1393 | RMSE: 0.1393 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88068.211\n",
      "[Epoch 3, Batch 41/100] Loss: 0.1359 | RMSE: 0.1359 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67377.305\n",
      "[Epoch 3, Batch 42/100] Loss: 0.1420 | RMSE: 0.1420 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62596.270\n",
      "[Epoch 3, Batch 43/100] Loss: 0.1480 | RMSE: 0.1480 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 129225.062\n",
      "[Epoch 3, Batch 44/100] Loss: 0.1460 | RMSE: 0.1460 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 108117.875\n",
      "[Epoch 3, Batch 45/100] Loss: 0.1435 | RMSE: 0.1435 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 111139.125\n",
      "[Epoch 3, Batch 46/100] Loss: 0.1472 | RMSE: 0.1472 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 122177.945\n",
      "[Epoch 3, Batch 47/100] Loss: 0.1434 | RMSE: 0.1434 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 109277.852\n",
      "[Epoch 3, Batch 48/100] Loss: 0.1365 | RMSE: 0.1365 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93807.617\n",
      "[Epoch 3, Batch 49/100] Loss: 0.1379 | RMSE: 0.1379 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100801.383\n",
      "[Epoch 3, Batch 50/100] Loss: 0.1557 | RMSE: 0.1557 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 182405.984\n",
      "[Epoch 3, Batch 51/100] Loss: 0.1352 | RMSE: 0.1352 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 125320.305\n",
      "[Epoch 3, Batch 52/100] Loss: 0.1413 | RMSE: 0.1413 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102600.516\n",
      "[Epoch 3, Batch 53/100] Loss: 0.1470 | RMSE: 0.1470 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78152.609\n",
      "[Epoch 3, Batch 54/100] Loss: 0.1355 | RMSE: 0.1355 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 94820.867\n",
      "[Epoch 3, Batch 55/100] Loss: 0.1391 | RMSE: 0.1391 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105082.273\n",
      "[Epoch 3, Batch 56/100] Loss: 0.1386 | RMSE: 0.1386 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72876.219\n",
      "[Epoch 3, Batch 57/100] Loss: 0.1428 | RMSE: 0.1428 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 108519.984\n",
      "[Epoch 3, Batch 58/100] Loss: 0.1473 | RMSE: 0.1473 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51446.234\n",
      "[Epoch 3, Batch 59/100] Loss: 0.1388 | RMSE: 0.1388 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72289.117\n",
      "[Epoch 3, Batch 60/100] Loss: 0.1497 | RMSE: 0.1497 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 144502.875\n",
      "[Epoch 3, Batch 61/100] Loss: 0.1424 | RMSE: 0.1424 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76224.664\n",
      "[Epoch 3, Batch 62/100] Loss: 0.1383 | RMSE: 0.1383 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 124646.461\n",
      "[Epoch 3, Batch 63/100] Loss: 0.1469 | RMSE: 0.1469 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92406.469\n",
      "[Epoch 3, Batch 64/100] Loss: 0.1281 | RMSE: 0.1281 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57709.715\n",
      "[Epoch 3, Batch 65/100] Loss: 0.1546 | RMSE: 0.1546 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 163150.812\n",
      "[Epoch 3, Batch 66/100] Loss: 0.1488 | RMSE: 0.1488 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 142265.922\n",
      "[Epoch 3, Batch 67/100] Loss: 0.1436 | RMSE: 0.1436 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 120647.102\n",
      "[Epoch 3, Batch 68/100] Loss: 0.1358 | RMSE: 0.1358 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100364.047\n",
      "[Epoch 3, Batch 69/100] Loss: 0.1420 | RMSE: 0.1420 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 120745.836\n",
      "[Epoch 3, Batch 70/100] Loss: 0.1467 | RMSE: 0.1467 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117552.680\n",
      "[Epoch 3, Batch 71/100] Loss: 0.1381 | RMSE: 0.1381 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 120482.141\n",
      "[Epoch 3, Batch 72/100] Loss: 0.1370 | RMSE: 0.1370 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90958.125\n",
      "[Epoch 3, Batch 73/100] Loss: 0.1341 | RMSE: 0.1341 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77384.852\n",
      "[Epoch 3, Batch 74/100] Loss: 0.1406 | RMSE: 0.1406 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100686.195\n",
      "[Epoch 3, Batch 75/100] Loss: 0.1399 | RMSE: 0.1399 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 135213.531\n",
      "[Epoch 3, Batch 76/100] Loss: 0.1445 | RMSE: 0.1445 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 136732.453\n",
      "[Epoch 3, Batch 77/100] Loss: 0.1377 | RMSE: 0.1377 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59292.145\n",
      "[Epoch 3, Batch 78/100] Loss: 0.1379 | RMSE: 0.1379 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 134107.516\n",
      "[Epoch 3, Batch 79/100] Loss: 0.1463 | RMSE: 0.1463 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 145514.578\n",
      "[Epoch 3, Batch 80/100] Loss: 0.1391 | RMSE: 0.1391 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 128175.156\n",
      "[Epoch 3, Batch 81/100] Loss: 0.1498 | RMSE: 0.1498 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 118475.883\n",
      "[Epoch 3, Batch 82/100] Loss: 0.1374 | RMSE: 0.1374 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 110299.188\n",
      "[Epoch 3, Batch 83/100] Loss: 0.1437 | RMSE: 0.1437 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 128762.609\n",
      "[Epoch 3, Batch 84/100] Loss: 0.1344 | RMSE: 0.1344 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 136626.141\n",
      "[Epoch 3, Batch 85/100] Loss: 0.1403 | RMSE: 0.1403 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 119737.188\n",
      "[Epoch 3, Batch 86/100] Loss: 0.1415 | RMSE: 0.1415 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 146116.016\n",
      "[Epoch 3, Batch 87/100] Loss: 0.1339 | RMSE: 0.1339 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76407.633\n",
      "[Epoch 3, Batch 88/100] Loss: 0.1422 | RMSE: 0.1422 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67840.195\n",
      "[Epoch 3, Batch 89/100] Loss: 0.1363 | RMSE: 0.1363 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 113389.156\n",
      "[Epoch 3, Batch 90/100] Loss: 0.1397 | RMSE: 0.1397 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 144560.359\n",
      "[Epoch 3, Batch 91/100] Loss: 0.1442 | RMSE: 0.1442 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 120012.047\n",
      "[Epoch 3, Batch 92/100] Loss: 0.1386 | RMSE: 0.1386 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 89268.391\n",
      "[Epoch 3, Batch 93/100] Loss: 0.1445 | RMSE: 0.1445 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117730.516\n",
      "[Epoch 3, Batch 94/100] Loss: 0.1440 | RMSE: 0.1440 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 148256.906\n",
      "[Epoch 3, Batch 95/100] Loss: 0.1477 | RMSE: 0.1477 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 170785.453\n",
      "[Epoch 3, Batch 96/100] Loss: 0.1467 | RMSE: 0.1467 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 106287.930\n",
      "[Epoch 3, Batch 97/100] Loss: 0.1352 | RMSE: 0.1352 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 95693.336\n",
      "[Epoch 3, Batch 98/100] Loss: 0.1445 | RMSE: 0.1445 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 135475.891\n",
      "[Epoch 3, Batch 99/100] Loss: 0.1451 | RMSE: 0.1451 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 171810.078\n",
      "[Epoch 3, Batch 100/100] Loss: 0.1468 | RMSE: 0.1468 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 160235.953\n",
      "✅ Epoch 3 완료!\n",
      "   📈 평균 Loss: 0.142015\n",
      "   📊 평균 RMSE: 0.142015\n",
      "   ⚙️  학습률: 0.000337\n",
      "   🏆 NEW BEST! RMSE: 0.142015\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 4/80 시작...\n",
      "[Epoch 4, Batch 1/100] Loss: 0.1381 | RMSE: 0.1381 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 131160.281\n",
      "[Epoch 4, Batch 2/100] Loss: 0.1377 | RMSE: 0.1377 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66615.281\n",
      "[Epoch 4, Batch 3/100] Loss: 0.1397 | RMSE: 0.1397 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 97680.883\n",
      "[Epoch 4, Batch 4/100] Loss: 0.1353 | RMSE: 0.1353 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 127716.656\n",
      "[Epoch 4, Batch 5/100] Loss: 0.1280 | RMSE: 0.1280 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48480.793\n",
      "[Epoch 4, Batch 6/100] Loss: 0.1370 | RMSE: 0.1370 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 113941.766\n",
      "[Epoch 4, Batch 7/100] Loss: 0.1355 | RMSE: 0.1355 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 107438.391\n",
      "[Epoch 4, Batch 8/100] Loss: 0.1342 | RMSE: 0.1342 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53898.918\n",
      "[Epoch 4, Batch 9/100] Loss: 0.1372 | RMSE: 0.1372 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70989.812\n",
      "[Epoch 4, Batch 10/100] Loss: 0.1378 | RMSE: 0.1378 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 98172.125\n",
      "[Epoch 4, Batch 11/100] Loss: 0.1417 | RMSE: 0.1417 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72975.516\n",
      "[Epoch 4, Batch 12/100] Loss: 0.1436 | RMSE: 0.1436 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79311.453\n",
      "[Epoch 4, Batch 13/100] Loss: 0.1380 | RMSE: 0.1380 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80306.445\n",
      "[Epoch 4, Batch 14/100] Loss: 0.1323 | RMSE: 0.1323 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73401.945\n",
      "[Epoch 4, Batch 15/100] Loss: 0.1423 | RMSE: 0.1423 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 123986.023\n",
      "[Epoch 4, Batch 16/100] Loss: 0.1431 | RMSE: 0.1431 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117433.336\n",
      "[Epoch 4, Batch 17/100] Loss: 0.1400 | RMSE: 0.1400 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 118980.852\n",
      "[Epoch 4, Batch 18/100] Loss: 0.1463 | RMSE: 0.1463 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 152458.031\n",
      "[Epoch 4, Batch 19/100] Loss: 0.1324 | RMSE: 0.1324 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 142040.281\n",
      "[Epoch 4, Batch 20/100] Loss: 0.1363 | RMSE: 0.1363 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 119774.555\n",
      "[Epoch 4, Batch 21/100] Loss: 0.1271 | RMSE: 0.1271 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72713.125\n",
      "[Epoch 4, Batch 22/100] Loss: 0.1325 | RMSE: 0.1325 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 106872.617\n",
      "[Epoch 4, Batch 23/100] Loss: 0.1280 | RMSE: 0.1280 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85908.469\n",
      "[Epoch 4, Batch 24/100] Loss: 0.1368 | RMSE: 0.1368 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74865.773\n",
      "[Epoch 4, Batch 25/100] Loss: 0.1307 | RMSE: 0.1307 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69549.844\n",
      "[Epoch 4, Batch 26/100] Loss: 0.1369 | RMSE: 0.1369 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87165.281\n",
      "[Epoch 4, Batch 27/100] Loss: 0.1320 | RMSE: 0.1320 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 94081.602\n",
      "[Epoch 4, Batch 28/100] Loss: 0.1329 | RMSE: 0.1329 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81257.500\n",
      "[Epoch 4, Batch 29/100] Loss: 0.1334 | RMSE: 0.1334 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 111440.859\n",
      "[Epoch 4, Batch 30/100] Loss: 0.1440 | RMSE: 0.1440 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 115644.828\n",
      "[Epoch 4, Batch 31/100] Loss: 0.1313 | RMSE: 0.1313 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77441.992\n",
      "[Epoch 4, Batch 32/100] Loss: 0.1401 | RMSE: 0.1401 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 107230.312\n",
      "[Epoch 4, Batch 33/100] Loss: 0.1404 | RMSE: 0.1404 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117447.125\n",
      "[Epoch 4, Batch 34/100] Loss: 0.1332 | RMSE: 0.1332 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65053.934\n",
      "[Epoch 4, Batch 35/100] Loss: 0.1345 | RMSE: 0.1345 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 116294.438\n",
      "[Epoch 4, Batch 36/100] Loss: 0.1464 | RMSE: 0.1464 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 142117.891\n",
      "[Epoch 4, Batch 37/100] Loss: 0.1396 | RMSE: 0.1396 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105051.742\n",
      "[Epoch 4, Batch 38/100] Loss: 0.1399 | RMSE: 0.1399 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 99875.438\n",
      "[Epoch 4, Batch 39/100] Loss: 0.1370 | RMSE: 0.1370 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 75689.609\n",
      "[Epoch 4, Batch 40/100] Loss: 0.1377 | RMSE: 0.1377 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 132661.688\n",
      "[Epoch 4, Batch 41/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105215.297\n",
      "[Epoch 4, Batch 42/100] Loss: 0.1406 | RMSE: 0.1406 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 118888.023\n",
      "[Epoch 4, Batch 43/100] Loss: 0.1491 | RMSE: 0.1491 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 145156.547\n",
      "[Epoch 4, Batch 44/100] Loss: 0.1360 | RMSE: 0.1360 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117559.414\n",
      "[Epoch 4, Batch 45/100] Loss: 0.1267 | RMSE: 0.1267 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 26607.146\n",
      "[Epoch 4, Batch 46/100] Loss: 0.1430 | RMSE: 0.1430 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 168658.062\n",
      "[Epoch 4, Batch 47/100] Loss: 0.1371 | RMSE: 0.1371 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 107567.688\n",
      "[Epoch 4, Batch 48/100] Loss: 0.1367 | RMSE: 0.1367 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74436.305\n",
      "[Epoch 4, Batch 49/100] Loss: 0.1373 | RMSE: 0.1373 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66072.375\n",
      "[Epoch 4, Batch 50/100] Loss: 0.1417 | RMSE: 0.1417 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51182.941\n",
      "[Epoch 4, Batch 51/100] Loss: 0.1299 | RMSE: 0.1299 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 56861.672\n",
      "[Epoch 4, Batch 52/100] Loss: 0.1344 | RMSE: 0.1344 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77865.570\n",
      "[Epoch 4, Batch 53/100] Loss: 0.1320 | RMSE: 0.1320 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86687.609\n",
      "[Epoch 4, Batch 54/100] Loss: 0.1379 | RMSE: 0.1379 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 116454.047\n",
      "[Epoch 4, Batch 55/100] Loss: 0.1385 | RMSE: 0.1385 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105686.570\n",
      "[Epoch 4, Batch 56/100] Loss: 0.1286 | RMSE: 0.1286 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83509.906\n",
      "[Epoch 4, Batch 57/100] Loss: 0.1299 | RMSE: 0.1299 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 56357.871\n",
      "[Epoch 4, Batch 58/100] Loss: 0.1398 | RMSE: 0.1398 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 99256.516\n",
      "[Epoch 4, Batch 59/100] Loss: 0.1333 | RMSE: 0.1333 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86735.000\n",
      "[Epoch 4, Batch 60/100] Loss: 0.1373 | RMSE: 0.1373 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79373.094\n",
      "[Epoch 4, Batch 61/100] Loss: 0.1422 | RMSE: 0.1422 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 131770.922\n",
      "[Epoch 4, Batch 62/100] Loss: 0.1408 | RMSE: 0.1408 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 146998.172\n",
      "[Epoch 4, Batch 63/100] Loss: 0.1329 | RMSE: 0.1329 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92507.219\n",
      "[Epoch 4, Batch 64/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88743.961\n",
      "[Epoch 4, Batch 65/100] Loss: 0.1284 | RMSE: 0.1284 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91956.750\n",
      "[Epoch 4, Batch 66/100] Loss: 0.1282 | RMSE: 0.1282 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100500.078\n",
      "[Epoch 4, Batch 67/100] Loss: 0.1407 | RMSE: 0.1407 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 142321.359\n",
      "[Epoch 4, Batch 68/100] Loss: 0.1373 | RMSE: 0.1373 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 138963.922\n",
      "[Epoch 4, Batch 69/100] Loss: 0.1388 | RMSE: 0.1388 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117085.562\n",
      "[Epoch 4, Batch 70/100] Loss: 0.1322 | RMSE: 0.1322 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87245.211\n",
      "[Epoch 4, Batch 71/100] Loss: 0.1429 | RMSE: 0.1429 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 138331.875\n",
      "[Epoch 4, Batch 72/100] Loss: 0.1422 | RMSE: 0.1422 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 142774.484\n",
      "[Epoch 4, Batch 73/100] Loss: 0.1360 | RMSE: 0.1360 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 133209.094\n",
      "[Epoch 4, Batch 74/100] Loss: 0.1389 | RMSE: 0.1389 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 123062.898\n",
      "[Epoch 4, Batch 75/100] Loss: 0.1325 | RMSE: 0.1325 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 108361.273\n",
      "[Epoch 4, Batch 76/100] Loss: 0.1353 | RMSE: 0.1353 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 104977.406\n",
      "[Epoch 4, Batch 77/100] Loss: 0.1374 | RMSE: 0.1374 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79049.977\n",
      "[Epoch 4, Batch 78/100] Loss: 0.1331 | RMSE: 0.1331 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 75076.359\n",
      "[Epoch 4, Batch 79/100] Loss: 0.1479 | RMSE: 0.1479 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 147067.766\n",
      "[Epoch 4, Batch 80/100] Loss: 0.1390 | RMSE: 0.1390 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 146361.906\n",
      "[Epoch 4, Batch 81/100] Loss: 0.1367 | RMSE: 0.1367 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91095.016\n",
      "[Epoch 4, Batch 82/100] Loss: 0.1310 | RMSE: 0.1310 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 96163.891\n",
      "[Epoch 4, Batch 83/100] Loss: 0.1270 | RMSE: 0.1270 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74629.039\n",
      "[Epoch 4, Batch 84/100] Loss: 0.1543 | RMSE: 0.1543 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 173335.516\n",
      "[Epoch 4, Batch 85/100] Loss: 0.1464 | RMSE: 0.1464 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 163320.641\n",
      "[Epoch 4, Batch 86/100] Loss: 0.1363 | RMSE: 0.1363 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117121.055\n",
      "[Epoch 4, Batch 87/100] Loss: 0.1297 | RMSE: 0.1297 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71562.602\n",
      "[Epoch 4, Batch 88/100] Loss: 0.1334 | RMSE: 0.1334 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 112152.047\n",
      "[Epoch 4, Batch 89/100] Loss: 0.1345 | RMSE: 0.1345 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77048.953\n",
      "[Epoch 4, Batch 90/100] Loss: 0.1291 | RMSE: 0.1291 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68083.797\n",
      "[Epoch 4, Batch 91/100] Loss: 0.1292 | RMSE: 0.1292 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73952.648\n",
      "[Epoch 4, Batch 92/100] Loss: 0.1279 | RMSE: 0.1279 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 43479.699\n",
      "[Epoch 4, Batch 93/100] Loss: 0.1295 | RMSE: 0.1295 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 49851.344\n",
      "[Epoch 4, Batch 94/100] Loss: 0.1365 | RMSE: 0.1365 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83497.625\n",
      "[Epoch 4, Batch 95/100] Loss: 0.1326 | RMSE: 0.1326 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 50458.348\n",
      "[Epoch 4, Batch 96/100] Loss: 0.1419 | RMSE: 0.1419 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 129812.602\n",
      "[Epoch 4, Batch 97/100] Loss: 0.1376 | RMSE: 0.1376 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 84198.305\n",
      "[Epoch 4, Batch 98/100] Loss: 0.1420 | RMSE: 0.1420 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 113549.766\n",
      "[Epoch 4, Batch 99/100] Loss: 0.1466 | RMSE: 0.1466 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 127354.008\n",
      "[Epoch 4, Batch 100/100] Loss: 0.1389 | RMSE: 0.1389 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105619.117\n",
      "✅ Epoch 4 완료!\n",
      "   📈 평균 Loss: 0.136470\n",
      "   📊 평균 RMSE: 0.136470\n",
      "   ⚙️  학습률: 0.000521\n",
      "   🏆 NEW BEST! RMSE: 0.136470\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 5/80 시작...\n",
      "[Epoch 5, Batch 1/100] Loss: 0.1478 | RMSE: 0.1478 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 134764.781\n",
      "[Epoch 5, Batch 2/100] Loss: 0.1549 | RMSE: 0.1549 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 160948.781\n",
      "[Epoch 5, Batch 3/100] Loss: 0.1372 | RMSE: 0.1372 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 123128.234\n",
      "[Epoch 5, Batch 4/100] Loss: 0.1358 | RMSE: 0.1358 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102116.977\n",
      "[Epoch 5, Batch 5/100] Loss: 0.1338 | RMSE: 0.1338 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105242.148\n",
      "[Epoch 5, Batch 6/100] Loss: 0.1266 | RMSE: 0.1266 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82901.750\n",
      "[Epoch 5, Batch 7/100] Loss: 0.1415 | RMSE: 0.1415 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 130086.312\n",
      "[Epoch 5, Batch 8/100] Loss: 0.1355 | RMSE: 0.1355 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 119952.062\n",
      "[Epoch 5, Batch 9/100] Loss: 0.1440 | RMSE: 0.1440 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 136531.938\n",
      "[Epoch 5, Batch 10/100] Loss: 0.1378 | RMSE: 0.1378 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 110516.258\n",
      "[Epoch 5, Batch 11/100] Loss: 0.1293 | RMSE: 0.1293 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54562.348\n",
      "[Epoch 5, Batch 12/100] Loss: 0.1413 | RMSE: 0.1413 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 110479.508\n",
      "[Epoch 5, Batch 13/100] Loss: 0.1299 | RMSE: 0.1299 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79086.133\n",
      "[Epoch 5, Batch 14/100] Loss: 0.1397 | RMSE: 0.1397 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90312.469\n",
      "[Epoch 5, Batch 15/100] Loss: 0.1405 | RMSE: 0.1405 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 109015.984\n",
      "[Epoch 5, Batch 16/100] Loss: 0.1323 | RMSE: 0.1323 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73070.758\n",
      "[Epoch 5, Batch 17/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58386.703\n",
      "[Epoch 5, Batch 18/100] Loss: 0.1325 | RMSE: 0.1325 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73544.781\n",
      "[Epoch 5, Batch 19/100] Loss: 0.1410 | RMSE: 0.1410 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 112475.047\n",
      "[Epoch 5, Batch 20/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53194.340\n",
      "[Epoch 5, Batch 21/100] Loss: 0.1248 | RMSE: 0.1248 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61143.367\n",
      "[Epoch 5, Batch 22/100] Loss: 0.1319 | RMSE: 0.1319 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69078.633\n",
      "[Epoch 5, Batch 23/100] Loss: 0.1290 | RMSE: 0.1290 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77649.039\n",
      "[Epoch 5, Batch 24/100] Loss: 0.1288 | RMSE: 0.1288 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71386.500\n",
      "[Epoch 5, Batch 25/100] Loss: 0.1599 | RMSE: 0.1599 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 167289.109\n",
      "[Epoch 5, Batch 26/100] Loss: 0.1409 | RMSE: 0.1409 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 138659.328\n",
      "[Epoch 5, Batch 27/100] Loss: 0.1496 | RMSE: 0.1496 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 114341.539\n",
      "[Epoch 5, Batch 28/100] Loss: 0.1326 | RMSE: 0.1326 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102882.445\n",
      "[Epoch 5, Batch 29/100] Loss: 0.1358 | RMSE: 0.1358 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105842.391\n",
      "[Epoch 5, Batch 30/100] Loss: 0.1376 | RMSE: 0.1376 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 123625.406\n",
      "[Epoch 5, Batch 31/100] Loss: 0.1383 | RMSE: 0.1383 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 103804.305\n",
      "[Epoch 5, Batch 32/100] Loss: 0.1292 | RMSE: 0.1292 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46584.477\n",
      "[Epoch 5, Batch 33/100] Loss: 0.1264 | RMSE: 0.1264 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 41502.645\n",
      "[Epoch 5, Batch 34/100] Loss: 0.1366 | RMSE: 0.1366 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69317.469\n",
      "[Epoch 5, Batch 35/100] Loss: 0.1276 | RMSE: 0.1276 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 52298.730\n",
      "[Epoch 5, Batch 36/100] Loss: 0.1293 | RMSE: 0.1293 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76966.594\n",
      "[Epoch 5, Batch 37/100] Loss: 0.1298 | RMSE: 0.1298 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81733.789\n",
      "[Epoch 5, Batch 38/100] Loss: 0.1285 | RMSE: 0.1285 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 43337.348\n",
      "[Epoch 5, Batch 39/100] Loss: 0.1285 | RMSE: 0.1285 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65050.020\n",
      "[Epoch 5, Batch 40/100] Loss: 0.1338 | RMSE: 0.1338 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82470.242\n",
      "[Epoch 5, Batch 41/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73674.602\n",
      "[Epoch 5, Batch 42/100] Loss: 0.1289 | RMSE: 0.1289 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77819.156\n",
      "[Epoch 5, Batch 43/100] Loss: 0.1293 | RMSE: 0.1293 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68272.141\n",
      "[Epoch 5, Batch 44/100] Loss: 0.1302 | RMSE: 0.1302 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87000.594\n",
      "[Epoch 5, Batch 45/100] Loss: 0.1246 | RMSE: 0.1246 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 49607.992\n",
      "[Epoch 5, Batch 46/100] Loss: 0.1245 | RMSE: 0.1245 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62361.934\n",
      "[Epoch 5, Batch 47/100] Loss: 0.1256 | RMSE: 0.1256 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46048.375\n",
      "[Epoch 5, Batch 48/100] Loss: 0.1270 | RMSE: 0.1270 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67483.188\n",
      "[Epoch 5, Batch 49/100] Loss: 0.1282 | RMSE: 0.1282 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68063.508\n",
      "[Epoch 5, Batch 50/100] Loss: 0.1295 | RMSE: 0.1295 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51442.289\n",
      "[Epoch 5, Batch 51/100] Loss: 0.1372 | RMSE: 0.1372 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100589.672\n",
      "[Epoch 5, Batch 52/100] Loss: 0.1380 | RMSE: 0.1380 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 98438.734\n",
      "[Epoch 5, Batch 53/100] Loss: 0.1299 | RMSE: 0.1299 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45584.613\n",
      "[Epoch 5, Batch 54/100] Loss: 0.1354 | RMSE: 0.1354 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100132.688\n",
      "[Epoch 5, Batch 55/100] Loss: 0.1406 | RMSE: 0.1406 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73433.812\n",
      "[Epoch 5, Batch 56/100] Loss: 0.1369 | RMSE: 0.1369 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78250.680\n",
      "[Epoch 5, Batch 57/100] Loss: 0.1356 | RMSE: 0.1356 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60661.262\n",
      "[Epoch 5, Batch 58/100] Loss: 0.1363 | RMSE: 0.1363 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80315.172\n",
      "[Epoch 5, Batch 59/100] Loss: 0.1380 | RMSE: 0.1380 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65359.090\n",
      "[Epoch 5, Batch 60/100] Loss: 0.1243 | RMSE: 0.1243 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54327.863\n",
      "[Epoch 5, Batch 61/100] Loss: 0.1310 | RMSE: 0.1310 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59090.129\n",
      "[Epoch 5, Batch 62/100] Loss: 0.1405 | RMSE: 0.1405 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 96919.555\n",
      "[Epoch 5, Batch 63/100] Loss: 0.1384 | RMSE: 0.1384 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68451.820\n",
      "[Epoch 5, Batch 64/100] Loss: 0.1311 | RMSE: 0.1311 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54595.520\n",
      "[Epoch 5, Batch 65/100] Loss: 0.1266 | RMSE: 0.1266 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 29953.434\n",
      "[Epoch 5, Batch 66/100] Loss: 0.1336 | RMSE: 0.1336 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 96225.258\n",
      "[Epoch 5, Batch 67/100] Loss: 0.1281 | RMSE: 0.1281 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46976.023\n",
      "[Epoch 5, Batch 68/100] Loss: 0.1235 | RMSE: 0.1235 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44716.715\n",
      "[Epoch 5, Batch 69/100] Loss: 0.1320 | RMSE: 0.1320 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60394.008\n",
      "[Epoch 5, Batch 70/100] Loss: 0.1297 | RMSE: 0.1297 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 63362.078\n",
      "[Epoch 5, Batch 71/100] Loss: 0.1285 | RMSE: 0.1285 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65218.418\n",
      "[Epoch 5, Batch 72/100] Loss: 0.1288 | RMSE: 0.1288 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59457.109\n",
      "[Epoch 5, Batch 73/100] Loss: 0.1321 | RMSE: 0.1321 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87083.172\n",
      "[Epoch 5, Batch 74/100] Loss: 0.1379 | RMSE: 0.1379 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 99323.297\n",
      "[Epoch 5, Batch 75/100] Loss: 0.1445 | RMSE: 0.1445 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 94609.328\n",
      "[Epoch 5, Batch 76/100] Loss: 0.1382 | RMSE: 0.1382 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 95788.680\n",
      "[Epoch 5, Batch 77/100] Loss: 0.1285 | RMSE: 0.1285 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 39883.641\n",
      "[Epoch 5, Batch 78/100] Loss: 0.1292 | RMSE: 0.1292 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 50058.734\n",
      "[Epoch 5, Batch 79/100] Loss: 0.1207 | RMSE: 0.1207 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 43481.156\n",
      "[Epoch 5, Batch 80/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 28736.998\n",
      "[Epoch 5, Batch 81/100] Loss: 0.1283 | RMSE: 0.1283 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68215.891\n",
      "[Epoch 5, Batch 82/100] Loss: 0.1326 | RMSE: 0.1326 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77655.211\n",
      "[Epoch 5, Batch 83/100] Loss: 0.1335 | RMSE: 0.1335 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 43900.414\n",
      "[Epoch 5, Batch 84/100] Loss: 0.1344 | RMSE: 0.1344 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 106093.688\n",
      "[Epoch 5, Batch 85/100] Loss: 0.1273 | RMSE: 0.1273 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72917.109\n",
      "[Epoch 5, Batch 86/100] Loss: 0.1292 | RMSE: 0.1292 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64137.355\n",
      "[Epoch 5, Batch 87/100] Loss: 0.1296 | RMSE: 0.1296 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54703.398\n",
      "[Epoch 5, Batch 88/100] Loss: 0.1225 | RMSE: 0.1225 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 37087.637\n",
      "[Epoch 5, Batch 89/100] Loss: 0.1206 | RMSE: 0.1206 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 50985.047\n",
      "[Epoch 5, Batch 90/100] Loss: 0.1278 | RMSE: 0.1278 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85349.453\n",
      "[Epoch 5, Batch 91/100] Loss: 0.1240 | RMSE: 0.1240 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 39752.488\n",
      "[Epoch 5, Batch 92/100] Loss: 0.1367 | RMSE: 0.1367 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91510.070\n",
      "[Epoch 5, Batch 93/100] Loss: 0.1225 | RMSE: 0.1225 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68097.312\n",
      "[Epoch 5, Batch 94/100] Loss: 0.1243 | RMSE: 0.1243 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 56325.789\n",
      "[Epoch 5, Batch 95/100] Loss: 0.1250 | RMSE: 0.1250 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55163.992\n",
      "[Epoch 5, Batch 96/100] Loss: 0.1308 | RMSE: 0.1308 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45811.480\n",
      "[Epoch 5, Batch 97/100] Loss: 0.1299 | RMSE: 0.1299 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78459.828\n",
      "[Epoch 5, Batch 98/100] Loss: 0.1248 | RMSE: 0.1248 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62174.359\n",
      "[Epoch 5, Batch 99/100] Loss: 0.1249 | RMSE: 0.1249 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69480.312\n",
      "[Epoch 5, Batch 100/100] Loss: 0.1332 | RMSE: 0.1332 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 34855.871\n",
      "✅ Epoch 5 완료!\n",
      "   📈 평균 Loss: 0.132362\n",
      "   📊 평균 RMSE: 0.132362\n",
      "   ⚙️  학습률: 0.000705\n",
      "   🏆 NEW BEST! RMSE: 0.132362\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 6/80 시작...\n",
      "[Epoch 6, Batch 1/100] Loss: 0.1447 | RMSE: 0.1447 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93955.273\n",
      "[Epoch 6, Batch 2/100] Loss: 0.1291 | RMSE: 0.1291 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68346.820\n",
      "[Epoch 6, Batch 3/100] Loss: 0.1341 | RMSE: 0.1341 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90285.680\n",
      "[Epoch 6, Batch 4/100] Loss: 0.1304 | RMSE: 0.1304 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69306.977\n",
      "[Epoch 6, Batch 5/100] Loss: 0.1378 | RMSE: 0.1378 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 75580.562\n",
      "[Epoch 6, Batch 6/100] Loss: 0.1257 | RMSE: 0.1257 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53032.070\n",
      "[Epoch 6, Batch 7/100] Loss: 0.1280 | RMSE: 0.1280 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55174.125\n",
      "[Epoch 6, Batch 8/100] Loss: 0.1233 | RMSE: 0.1233 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 52350.887\n",
      "[Epoch 6, Batch 9/100] Loss: 0.1289 | RMSE: 0.1289 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69173.969\n",
      "[Epoch 6, Batch 10/100] Loss: 0.1301 | RMSE: 0.1301 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 35640.414\n",
      "[Epoch 6, Batch 11/100] Loss: 0.1212 | RMSE: 0.1212 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 43750.906\n",
      "[Epoch 6, Batch 12/100] Loss: 0.1342 | RMSE: 0.1342 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93873.773\n",
      "[Epoch 6, Batch 13/100] Loss: 0.1342 | RMSE: 0.1342 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88946.859\n",
      "[Epoch 6, Batch 14/100] Loss: 0.1314 | RMSE: 0.1314 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 33636.617\n",
      "[Epoch 6, Batch 15/100] Loss: 0.1368 | RMSE: 0.1368 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71525.328\n",
      "[Epoch 6, Batch 16/100] Loss: 0.1274 | RMSE: 0.1274 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64810.957\n",
      "[Epoch 6, Batch 17/100] Loss: 0.1279 | RMSE: 0.1279 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 29499.812\n",
      "[Epoch 6, Batch 18/100] Loss: 0.1408 | RMSE: 0.1408 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 89265.688\n",
      "[Epoch 6, Batch 19/100] Loss: 0.1467 | RMSE: 0.1467 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 103730.219\n",
      "[Epoch 6, Batch 20/100] Loss: 0.1321 | RMSE: 0.1321 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55944.180\n",
      "[Epoch 6, Batch 21/100] Loss: 0.1337 | RMSE: 0.1337 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86577.992\n",
      "[Epoch 6, Batch 22/100] Loss: 0.1399 | RMSE: 0.1399 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105427.531\n",
      "[Epoch 6, Batch 23/100] Loss: 0.1330 | RMSE: 0.1330 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82387.688\n",
      "[Epoch 6, Batch 24/100] Loss: 0.1313 | RMSE: 0.1313 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76530.180\n",
      "[Epoch 6, Batch 25/100] Loss: 0.1242 | RMSE: 0.1242 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 23883.916\n",
      "[Epoch 6, Batch 26/100] Loss: 0.1375 | RMSE: 0.1375 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90711.086\n",
      "[Epoch 6, Batch 27/100] Loss: 0.1357 | RMSE: 0.1357 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88932.711\n",
      "[Epoch 6, Batch 28/100] Loss: 0.1346 | RMSE: 0.1346 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44870.621\n",
      "[Epoch 6, Batch 29/100] Loss: 0.1237 | RMSE: 0.1237 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46476.785\n",
      "[Epoch 6, Batch 30/100] Loss: 0.1401 | RMSE: 0.1401 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 104904.445\n",
      "[Epoch 6, Batch 31/100] Loss: 0.1304 | RMSE: 0.1304 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77424.375\n",
      "[Epoch 6, Batch 32/100] Loss: 0.1275 | RMSE: 0.1275 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 26167.764\n",
      "[Epoch 6, Batch 33/100] Loss: 0.1357 | RMSE: 0.1357 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54628.344\n",
      "[Epoch 6, Batch 34/100] Loss: 0.1430 | RMSE: 0.1430 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85501.125\n",
      "[Epoch 6, Batch 35/100] Loss: 0.1355 | RMSE: 0.1355 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60243.516\n",
      "[Epoch 6, Batch 36/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 75815.062\n",
      "[Epoch 6, Batch 37/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45964.543\n",
      "[Epoch 6, Batch 38/100] Loss: 0.1256 | RMSE: 0.1256 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42401.758\n",
      "[Epoch 6, Batch 39/100] Loss: 0.1483 | RMSE: 0.1483 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102643.250\n",
      "[Epoch 6, Batch 40/100] Loss: 0.1381 | RMSE: 0.1381 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 75641.805\n",
      "[Epoch 6, Batch 41/100] Loss: 0.1350 | RMSE: 0.1350 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65100.906\n",
      "[Epoch 6, Batch 42/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76082.828\n",
      "[Epoch 6, Batch 43/100] Loss: 0.1418 | RMSE: 0.1418 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 103247.930\n",
      "[Epoch 6, Batch 44/100] Loss: 0.1328 | RMSE: 0.1328 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58935.488\n",
      "[Epoch 6, Batch 45/100] Loss: 0.1358 | RMSE: 0.1358 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51275.141\n",
      "[Epoch 6, Batch 46/100] Loss: 0.1306 | RMSE: 0.1306 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70380.117\n",
      "[Epoch 6, Batch 47/100] Loss: 0.1234 | RMSE: 0.1234 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 40269.305\n",
      "[Epoch 6, Batch 48/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64965.559\n",
      "[Epoch 6, Batch 49/100] Loss: 0.1265 | RMSE: 0.1265 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 29797.521\n",
      "[Epoch 6, Batch 50/100] Loss: 0.1322 | RMSE: 0.1322 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78228.703\n",
      "[Epoch 6, Batch 51/100] Loss: 0.1394 | RMSE: 0.1394 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 106107.359\n",
      "[Epoch 6, Batch 52/100] Loss: 0.1348 | RMSE: 0.1348 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67014.414\n",
      "[Epoch 6, Batch 53/100] Loss: 0.1321 | RMSE: 0.1321 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64987.617\n",
      "[Epoch 6, Batch 54/100] Loss: 0.1407 | RMSE: 0.1407 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88541.867\n",
      "[Epoch 6, Batch 55/100] Loss: 0.1282 | RMSE: 0.1282 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58938.629\n",
      "[Epoch 6, Batch 56/100] Loss: 0.1194 | RMSE: 0.1194 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 34801.652\n",
      "[Epoch 6, Batch 57/100] Loss: 0.1235 | RMSE: 0.1235 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78933.656\n",
      "[Epoch 6, Batch 58/100] Loss: 0.1275 | RMSE: 0.1275 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44343.031\n",
      "[Epoch 6, Batch 59/100] Loss: 0.1408 | RMSE: 0.1408 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 84552.172\n",
      "[Epoch 6, Batch 60/100] Loss: 0.1338 | RMSE: 0.1338 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 106942.633\n",
      "[Epoch 6, Batch 61/100] Loss: 0.1280 | RMSE: 0.1280 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71195.438\n",
      "[Epoch 6, Batch 62/100] Loss: 0.1445 | RMSE: 0.1445 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 111468.820\n",
      "[Epoch 6, Batch 63/100] Loss: 0.1330 | RMSE: 0.1330 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58573.938\n",
      "[Epoch 6, Batch 64/100] Loss: 0.1292 | RMSE: 0.1292 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65122.348\n",
      "[Epoch 6, Batch 65/100] Loss: 0.1352 | RMSE: 0.1352 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 63980.984\n",
      "[Epoch 6, Batch 66/100] Loss: 0.1320 | RMSE: 0.1320 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83771.875\n",
      "[Epoch 6, Batch 67/100] Loss: 0.1226 | RMSE: 0.1226 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54209.711\n",
      "[Epoch 6, Batch 68/100] Loss: 0.1331 | RMSE: 0.1331 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 50913.344\n",
      "[Epoch 6, Batch 69/100] Loss: 0.1317 | RMSE: 0.1317 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81731.086\n",
      "[Epoch 6, Batch 70/100] Loss: 0.1226 | RMSE: 0.1226 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47895.309\n",
      "[Epoch 6, Batch 71/100] Loss: 0.1261 | RMSE: 0.1261 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74689.047\n",
      "[Epoch 6, Batch 72/100] Loss: 0.1444 | RMSE: 0.1444 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90390.555\n",
      "[Epoch 6, Batch 73/100] Loss: 0.1311 | RMSE: 0.1311 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74653.391\n",
      "[Epoch 6, Batch 74/100] Loss: 0.1283 | RMSE: 0.1283 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80200.438\n",
      "[Epoch 6, Batch 75/100] Loss: 0.1318 | RMSE: 0.1318 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65844.992\n",
      "[Epoch 6, Batch 76/100] Loss: 0.1345 | RMSE: 0.1345 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68139.656\n",
      "[Epoch 6, Batch 77/100] Loss: 0.1215 | RMSE: 0.1215 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 36207.953\n",
      "[Epoch 6, Batch 78/100] Loss: 0.1228 | RMSE: 0.1228 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51786.660\n",
      "[Epoch 6, Batch 79/100] Loss: 0.1320 | RMSE: 0.1320 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83543.289\n",
      "[Epoch 6, Batch 80/100] Loss: 0.1275 | RMSE: 0.1275 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47684.555\n",
      "[Epoch 6, Batch 81/100] Loss: 0.1203 | RMSE: 0.1203 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 20243.996\n",
      "[Epoch 6, Batch 82/100] Loss: 0.1367 | RMSE: 0.1367 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93477.227\n",
      "[Epoch 6, Batch 83/100] Loss: 0.1335 | RMSE: 0.1335 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 50236.398\n",
      "[Epoch 6, Batch 84/100] Loss: 0.1366 | RMSE: 0.1366 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 97945.234\n",
      "[Epoch 6, Batch 85/100] Loss: 0.1305 | RMSE: 0.1305 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51194.320\n",
      "[Epoch 6, Batch 86/100] Loss: 0.1292 | RMSE: 0.1292 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78298.297\n",
      "[Epoch 6, Batch 87/100] Loss: 0.1246 | RMSE: 0.1246 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42029.727\n",
      "[Epoch 6, Batch 88/100] Loss: 0.1203 | RMSE: 0.1203 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 29796.100\n",
      "[Epoch 6, Batch 89/100] Loss: 0.1279 | RMSE: 0.1279 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48395.145\n",
      "[Epoch 6, Batch 90/100] Loss: 0.1313 | RMSE: 0.1313 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77923.992\n",
      "[Epoch 6, Batch 91/100] Loss: 0.1275 | RMSE: 0.1275 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64878.090\n",
      "[Epoch 6, Batch 92/100] Loss: 0.1269 | RMSE: 0.1269 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54768.926\n",
      "[Epoch 6, Batch 93/100] Loss: 0.1280 | RMSE: 0.1280 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62722.023\n",
      "[Epoch 6, Batch 94/100] Loss: 0.1274 | RMSE: 0.1274 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48819.820\n",
      "[Epoch 6, Batch 95/100] Loss: 0.1301 | RMSE: 0.1301 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 49864.461\n",
      "[Epoch 6, Batch 96/100] Loss: 0.1259 | RMSE: 0.1259 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53747.672\n",
      "[Epoch 6, Batch 97/100] Loss: 0.1261 | RMSE: 0.1261 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48922.871\n",
      "[Epoch 6, Batch 98/100] Loss: 0.1200 | RMSE: 0.1200 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54999.199\n",
      "[Epoch 6, Batch 99/100] Loss: 0.1214 | RMSE: 0.1214 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 34063.355\n",
      "[Epoch 6, Batch 100/100] Loss: 0.1294 | RMSE: 0.1294 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48183.723\n",
      "✅ Epoch 6 완료!\n",
      "   📈 평균 Loss: 0.131280\n",
      "   📊 평균 RMSE: 0.131280\n",
      "   ⚙️  학습률: 0.000860\n",
      "   🏆 NEW BEST! RMSE: 0.131280\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 7/80 시작...\n",
      "[Epoch 7, Batch 1/100] Loss: 0.1265 | RMSE: 0.1265 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67484.727\n",
      "[Epoch 7, Batch 2/100] Loss: 0.1213 | RMSE: 0.1213 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 24608.375\n",
      "[Epoch 7, Batch 3/100] Loss: 0.1220 | RMSE: 0.1220 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 25682.334\n",
      "[Epoch 7, Batch 4/100] Loss: 0.1249 | RMSE: 0.1249 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66847.203\n",
      "[Epoch 7, Batch 5/100] Loss: 0.1254 | RMSE: 0.1254 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55504.004\n",
      "[Epoch 7, Batch 6/100] Loss: 0.1283 | RMSE: 0.1283 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86583.008\n",
      "[Epoch 7, Batch 7/100] Loss: 0.1298 | RMSE: 0.1298 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93660.977\n",
      "[Epoch 7, Batch 8/100] Loss: 0.1331 | RMSE: 0.1331 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 41115.988\n",
      "[Epoch 7, Batch 9/100] Loss: 0.1258 | RMSE: 0.1258 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59248.824\n",
      "[Epoch 7, Batch 10/100] Loss: 0.1267 | RMSE: 0.1267 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77112.078\n",
      "[Epoch 7, Batch 11/100] Loss: 0.1250 | RMSE: 0.1250 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61832.371\n",
      "[Epoch 7, Batch 12/100] Loss: 0.1257 | RMSE: 0.1257 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 27573.996\n",
      "[Epoch 7, Batch 13/100] Loss: 0.1263 | RMSE: 0.1263 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 34838.117\n",
      "[Epoch 7, Batch 14/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44018.996\n",
      "[Epoch 7, Batch 15/100] Loss: 0.1282 | RMSE: 0.1282 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77709.047\n",
      "[Epoch 7, Batch 16/100] Loss: 0.1200 | RMSE: 0.1200 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 13753.675\n",
      "[Epoch 7, Batch 17/100] Loss: 0.1219 | RMSE: 0.1219 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46932.793\n",
      "[Epoch 7, Batch 18/100] Loss: 0.1246 | RMSE: 0.1246 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60970.684\n",
      "[Epoch 7, Batch 19/100] Loss: 0.1292 | RMSE: 0.1292 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68540.445\n",
      "[Epoch 7, Batch 20/100] Loss: 0.1397 | RMSE: 0.1397 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 122379.859\n",
      "[Epoch 7, Batch 21/100] Loss: 0.1258 | RMSE: 0.1258 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54045.184\n",
      "[Epoch 7, Batch 22/100] Loss: 0.1275 | RMSE: 0.1275 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65829.508\n",
      "[Epoch 7, Batch 23/100] Loss: 0.1352 | RMSE: 0.1352 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 104820.242\n",
      "[Epoch 7, Batch 24/100] Loss: 0.1240 | RMSE: 0.1240 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61121.059\n",
      "[Epoch 7, Batch 25/100] Loss: 0.1290 | RMSE: 0.1290 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51234.848\n",
      "[Epoch 7, Batch 26/100] Loss: 0.1335 | RMSE: 0.1335 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58255.504\n",
      "[Epoch 7, Batch 27/100] Loss: 0.1267 | RMSE: 0.1267 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 75487.203\n",
      "[Epoch 7, Batch 28/100] Loss: 0.1362 | RMSE: 0.1362 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85509.023\n",
      "[Epoch 7, Batch 29/100] Loss: 0.1232 | RMSE: 0.1232 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 19812.100\n",
      "[Epoch 7, Batch 30/100] Loss: 0.1315 | RMSE: 0.1315 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87964.000\n",
      "[Epoch 7, Batch 31/100] Loss: 0.1336 | RMSE: 0.1336 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46364.164\n",
      "[Epoch 7, Batch 32/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48334.543\n",
      "[Epoch 7, Batch 33/100] Loss: 0.1316 | RMSE: 0.1316 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45936.953\n",
      "[Epoch 7, Batch 34/100] Loss: 0.1238 | RMSE: 0.1238 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64305.062\n",
      "[Epoch 7, Batch 35/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62129.551\n",
      "[Epoch 7, Batch 36/100] Loss: 0.1234 | RMSE: 0.1234 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 30891.590\n",
      "[Epoch 7, Batch 37/100] Loss: 0.1249 | RMSE: 0.1249 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46285.840\n",
      "[Epoch 7, Batch 38/100] Loss: 0.1247 | RMSE: 0.1247 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61499.867\n",
      "[Epoch 7, Batch 39/100] Loss: 0.1291 | RMSE: 0.1291 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92086.789\n",
      "[Epoch 7, Batch 40/100] Loss: 0.1247 | RMSE: 0.1247 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58675.453\n",
      "[Epoch 7, Batch 41/100] Loss: 0.1245 | RMSE: 0.1245 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61401.852\n",
      "[Epoch 7, Batch 42/100] Loss: 0.1336 | RMSE: 0.1336 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70998.211\n",
      "[Epoch 7, Batch 43/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57056.305\n",
      "[Epoch 7, Batch 44/100] Loss: 0.1359 | RMSE: 0.1359 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 122356.383\n",
      "[Epoch 7, Batch 45/100] Loss: 0.1438 | RMSE: 0.1438 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 121601.281\n",
      "[Epoch 7, Batch 46/100] Loss: 0.1208 | RMSE: 0.1208 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 49165.797\n",
      "[Epoch 7, Batch 47/100] Loss: 0.1289 | RMSE: 0.1289 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68227.672\n",
      "[Epoch 7, Batch 48/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66132.430\n",
      "[Epoch 7, Batch 49/100] Loss: 0.1329 | RMSE: 0.1329 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74051.398\n",
      "[Epoch 7, Batch 50/100] Loss: 0.1270 | RMSE: 0.1270 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70961.141\n",
      "[Epoch 7, Batch 51/100] Loss: 0.1279 | RMSE: 0.1279 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 84413.234\n",
      "[Epoch 7, Batch 52/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73844.008\n",
      "[Epoch 7, Batch 53/100] Loss: 0.1269 | RMSE: 0.1269 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60808.223\n",
      "[Epoch 7, Batch 54/100] Loss: 0.1235 | RMSE: 0.1235 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51762.695\n",
      "[Epoch 7, Batch 55/100] Loss: 0.1307 | RMSE: 0.1307 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69299.031\n",
      "[Epoch 7, Batch 56/100] Loss: 0.1384 | RMSE: 0.1384 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 101747.305\n",
      "[Epoch 7, Batch 57/100] Loss: 0.1379 | RMSE: 0.1379 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 109905.477\n",
      "[Epoch 7, Batch 58/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 35927.492\n",
      "[Epoch 7, Batch 59/100] Loss: 0.1217 | RMSE: 0.1217 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 34591.555\n",
      "[Epoch 7, Batch 60/100] Loss: 0.1293 | RMSE: 0.1293 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74859.781\n",
      "[Epoch 7, Batch 61/100] Loss: 0.1412 | RMSE: 0.1412 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 111839.062\n",
      "[Epoch 7, Batch 62/100] Loss: 0.1261 | RMSE: 0.1261 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80055.852\n",
      "[Epoch 7, Batch 63/100] Loss: 0.1272 | RMSE: 0.1272 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57643.273\n",
      "[Epoch 7, Batch 64/100] Loss: 0.1226 | RMSE: 0.1226 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55978.879\n",
      "[Epoch 7, Batch 65/100] Loss: 0.1227 | RMSE: 0.1227 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58861.727\n",
      "[Epoch 7, Batch 66/100] Loss: 0.1279 | RMSE: 0.1279 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 50661.426\n",
      "[Epoch 7, Batch 67/100] Loss: 0.1211 | RMSE: 0.1211 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42355.648\n",
      "[Epoch 7, Batch 68/100] Loss: 0.1397 | RMSE: 0.1397 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 104801.664\n",
      "[Epoch 7, Batch 69/100] Loss: 0.1433 | RMSE: 0.1433 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83640.344\n",
      "[Epoch 7, Batch 70/100] Loss: 0.1356 | RMSE: 0.1356 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 101973.148\n",
      "[Epoch 7, Batch 71/100] Loss: 0.1218 | RMSE: 0.1218 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 43626.938\n",
      "[Epoch 7, Batch 72/100] Loss: 0.1307 | RMSE: 0.1307 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61477.797\n",
      "[Epoch 7, Batch 73/100] Loss: 0.1332 | RMSE: 0.1332 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 84435.133\n",
      "[Epoch 7, Batch 74/100] Loss: 0.1295 | RMSE: 0.1295 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54983.969\n",
      "[Epoch 7, Batch 75/100] Loss: 0.1300 | RMSE: 0.1300 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82465.086\n",
      "[Epoch 7, Batch 76/100] Loss: 0.1358 | RMSE: 0.1358 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 95756.969\n",
      "[Epoch 7, Batch 77/100] Loss: 0.1216 | RMSE: 0.1216 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 41834.688\n",
      "[Epoch 7, Batch 78/100] Loss: 0.1380 | RMSE: 0.1380 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88249.117\n",
      "[Epoch 7, Batch 79/100] Loss: 0.1404 | RMSE: 0.1404 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93431.328\n",
      "[Epoch 7, Batch 80/100] Loss: 0.1220 | RMSE: 0.1220 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58462.820\n",
      "[Epoch 7, Batch 81/100] Loss: 0.1234 | RMSE: 0.1234 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 31266.197\n",
      "[Epoch 7, Batch 82/100] Loss: 0.1289 | RMSE: 0.1289 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79336.711\n",
      "[Epoch 7, Batch 83/100] Loss: 0.1322 | RMSE: 0.1322 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91963.133\n",
      "[Epoch 7, Batch 84/100] Loss: 0.1310 | RMSE: 0.1310 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80014.945\n",
      "[Epoch 7, Batch 85/100] Loss: 0.1287 | RMSE: 0.1287 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70686.328\n",
      "[Epoch 7, Batch 86/100] Loss: 0.1298 | RMSE: 0.1298 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55371.363\n",
      "[Epoch 7, Batch 87/100] Loss: 0.1215 | RMSE: 0.1215 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48220.789\n",
      "[Epoch 7, Batch 88/100] Loss: 0.1306 | RMSE: 0.1306 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87125.180\n",
      "[Epoch 7, Batch 89/100] Loss: 0.1348 | RMSE: 0.1348 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86310.367\n",
      "[Epoch 7, Batch 90/100] Loss: 0.1194 | RMSE: 0.1194 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 24483.107\n",
      "[Epoch 7, Batch 91/100] Loss: 0.1290 | RMSE: 0.1290 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57603.641\n",
      "[Epoch 7, Batch 92/100] Loss: 0.1362 | RMSE: 0.1362 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 84350.148\n",
      "[Epoch 7, Batch 93/100] Loss: 0.1250 | RMSE: 0.1250 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64609.875\n",
      "[Epoch 7, Batch 94/100] Loss: 0.1225 | RMSE: 0.1225 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42051.613\n",
      "[Epoch 7, Batch 95/100] Loss: 0.1308 | RMSE: 0.1308 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 29483.498\n",
      "[Epoch 7, Batch 96/100] Loss: 0.1243 | RMSE: 0.1243 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53707.875\n",
      "[Epoch 7, Batch 97/100] Loss: 0.1211 | RMSE: 0.1211 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 29010.244\n",
      "[Epoch 7, Batch 98/100] Loss: 0.1259 | RMSE: 0.1259 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69691.805\n",
      "[Epoch 7, Batch 99/100] Loss: 0.1240 | RMSE: 0.1240 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70200.453\n",
      "[Epoch 7, Batch 100/100] Loss: 0.1253 | RMSE: 0.1253 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59062.910\n",
      "✅ Epoch 7 완료!\n",
      "   📈 평균 Loss: 0.128541\n",
      "   📊 평균 RMSE: 0.128541\n",
      "   ⚙️  학습률: 0.000964\n",
      "   🏆 NEW BEST! RMSE: 0.128541\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 8/80 시작...\n",
      "[Epoch 8, Batch 1/100] Loss: 0.1310 | RMSE: 0.1310 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57483.121\n",
      "[Epoch 8, Batch 2/100] Loss: 0.1328 | RMSE: 0.1328 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105752.688\n",
      "[Epoch 8, Batch 3/100] Loss: 0.1295 | RMSE: 0.1295 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61178.520\n",
      "[Epoch 8, Batch 4/100] Loss: 0.1340 | RMSE: 0.1340 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 95720.391\n",
      "[Epoch 8, Batch 5/100] Loss: 0.1255 | RMSE: 0.1255 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76437.344\n",
      "[Epoch 8, Batch 6/100] Loss: 0.1243 | RMSE: 0.1243 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48897.469\n",
      "[Epoch 8, Batch 7/100] Loss: 0.1212 | RMSE: 0.1212 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54800.227\n",
      "[Epoch 8, Batch 8/100] Loss: 0.1257 | RMSE: 0.1257 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 38649.867\n",
      "[Epoch 8, Batch 9/100] Loss: 0.1238 | RMSE: 0.1238 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77204.859\n",
      "[Epoch 8, Batch 10/100] Loss: 0.1326 | RMSE: 0.1326 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 84378.453\n",
      "[Epoch 8, Batch 11/100] Loss: 0.1226 | RMSE: 0.1226 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65732.578\n",
      "[Epoch 8, Batch 12/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68393.195\n",
      "[Epoch 8, Batch 13/100] Loss: 0.1326 | RMSE: 0.1326 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76366.344\n",
      "[Epoch 8, Batch 14/100] Loss: 0.1338 | RMSE: 0.1338 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 49603.309\n",
      "[Epoch 8, Batch 15/100] Loss: 0.1298 | RMSE: 0.1298 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65155.027\n",
      "[Epoch 8, Batch 16/100] Loss: 0.1359 | RMSE: 0.1359 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 97791.922\n",
      "[Epoch 8, Batch 17/100] Loss: 0.1232 | RMSE: 0.1232 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67746.719\n",
      "[Epoch 8, Batch 18/100] Loss: 0.1273 | RMSE: 0.1273 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 89661.609\n",
      "[Epoch 8, Batch 19/100] Loss: 0.1322 | RMSE: 0.1322 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74114.898\n",
      "[Epoch 8, Batch 20/100] Loss: 0.1254 | RMSE: 0.1254 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 39810.535\n",
      "[Epoch 8, Batch 21/100] Loss: 0.1259 | RMSE: 0.1259 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67305.883\n",
      "[Epoch 8, Batch 22/100] Loss: 0.1262 | RMSE: 0.1262 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68620.422\n",
      "[Epoch 8, Batch 23/100] Loss: 0.1315 | RMSE: 0.1315 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90683.086\n",
      "[Epoch 8, Batch 24/100] Loss: 0.1194 | RMSE: 0.1194 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 12554.457\n",
      "[Epoch 8, Batch 25/100] Loss: 0.1410 | RMSE: 0.1410 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 96990.438\n",
      "[Epoch 8, Batch 26/100] Loss: 0.1404 | RMSE: 0.1404 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53683.875\n",
      "[Epoch 8, Batch 27/100] Loss: 0.1384 | RMSE: 0.1384 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62747.586\n",
      "[Epoch 8, Batch 28/100] Loss: 0.1295 | RMSE: 0.1295 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53198.875\n",
      "[Epoch 8, Batch 29/100] Loss: 0.1359 | RMSE: 0.1359 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80816.688\n",
      "[Epoch 8, Batch 30/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71034.430\n",
      "[Epoch 8, Batch 31/100] Loss: 0.1247 | RMSE: 0.1247 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 37219.406\n",
      "[Epoch 8, Batch 32/100] Loss: 0.1279 | RMSE: 0.1279 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55373.945\n",
      "[Epoch 8, Batch 33/100] Loss: 0.1328 | RMSE: 0.1328 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82560.828\n",
      "[Epoch 8, Batch 34/100] Loss: 0.1339 | RMSE: 0.1339 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 75717.945\n",
      "[Epoch 8, Batch 35/100] Loss: 0.1289 | RMSE: 0.1289 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 52483.059\n",
      "[Epoch 8, Batch 36/100] Loss: 0.1290 | RMSE: 0.1290 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70001.125\n",
      "[Epoch 8, Batch 37/100] Loss: 0.1283 | RMSE: 0.1283 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 18770.598\n",
      "[Epoch 8, Batch 38/100] Loss: 0.1293 | RMSE: 0.1293 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47504.551\n",
      "[Epoch 8, Batch 39/100] Loss: 0.1290 | RMSE: 0.1290 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 52553.801\n",
      "[Epoch 8, Batch 40/100] Loss: 0.1250 | RMSE: 0.1250 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72483.195\n",
      "[Epoch 8, Batch 41/100] Loss: 0.1276 | RMSE: 0.1276 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42366.113\n",
      "[Epoch 8, Batch 42/100] Loss: 0.1402 | RMSE: 0.1402 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 103584.133\n",
      "[Epoch 8, Batch 43/100] Loss: 0.1379 | RMSE: 0.1379 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 108801.984\n",
      "[Epoch 8, Batch 44/100] Loss: 0.1322 | RMSE: 0.1322 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71609.523\n",
      "[Epoch 8, Batch 45/100] Loss: 0.1242 | RMSE: 0.1242 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 30842.713\n",
      "[Epoch 8, Batch 46/100] Loss: 0.1243 | RMSE: 0.1243 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88578.820\n",
      "[Epoch 8, Batch 47/100] Loss: 0.1303 | RMSE: 0.1303 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80718.023\n",
      "[Epoch 8, Batch 48/100] Loss: 0.1303 | RMSE: 0.1303 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61462.461\n",
      "[Epoch 8, Batch 49/100] Loss: 0.1269 | RMSE: 0.1269 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54760.969\n",
      "[Epoch 8, Batch 50/100] Loss: 0.1214 | RMSE: 0.1214 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 33474.609\n",
      "[Epoch 8, Batch 51/100] Loss: 0.1350 | RMSE: 0.1350 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100311.789\n",
      "[Epoch 8, Batch 52/100] Loss: 0.1305 | RMSE: 0.1305 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 94798.109\n",
      "[Epoch 8, Batch 53/100] Loss: 0.1221 | RMSE: 0.1221 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47005.047\n",
      "[Epoch 8, Batch 54/100] Loss: 0.1263 | RMSE: 0.1263 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79287.289\n",
      "[Epoch 8, Batch 55/100] Loss: 0.1323 | RMSE: 0.1323 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 96480.609\n",
      "[Epoch 8, Batch 56/100] Loss: 0.1290 | RMSE: 0.1290 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74530.164\n",
      "[Epoch 8, Batch 57/100] Loss: 0.1255 | RMSE: 0.1255 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47079.836\n",
      "[Epoch 8, Batch 58/100] Loss: 0.1283 | RMSE: 0.1283 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67348.984\n",
      "[Epoch 8, Batch 59/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73697.766\n",
      "[Epoch 8, Batch 60/100] Loss: 0.1281 | RMSE: 0.1281 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85798.070\n",
      "[Epoch 8, Batch 61/100] Loss: 0.1269 | RMSE: 0.1269 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60913.059\n",
      "[Epoch 8, Batch 62/100] Loss: 0.1258 | RMSE: 0.1258 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71186.305\n",
      "[Epoch 8, Batch 63/100] Loss: 0.1322 | RMSE: 0.1322 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82690.336\n",
      "[Epoch 8, Batch 64/100] Loss: 0.1253 | RMSE: 0.1253 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58412.938\n",
      "[Epoch 8, Batch 65/100] Loss: 0.1261 | RMSE: 0.1261 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72721.164\n",
      "[Epoch 8, Batch 66/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82535.695\n",
      "[Epoch 8, Batch 67/100] Loss: 0.1198 | RMSE: 0.1198 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47075.012\n",
      "[Epoch 8, Batch 68/100] Loss: 0.1247 | RMSE: 0.1247 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78335.352\n",
      "[Epoch 8, Batch 69/100] Loss: 0.1216 | RMSE: 0.1216 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62934.797\n",
      "[Epoch 8, Batch 70/100] Loss: 0.1245 | RMSE: 0.1245 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53005.574\n",
      "[Epoch 8, Batch 71/100] Loss: 0.1194 | RMSE: 0.1194 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42147.105\n",
      "[Epoch 8, Batch 72/100] Loss: 0.1204 | RMSE: 0.1204 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45971.352\n",
      "[Epoch 8, Batch 73/100] Loss: 0.1231 | RMSE: 0.1231 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 38504.238\n",
      "[Epoch 8, Batch 74/100] Loss: 0.1303 | RMSE: 0.1303 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53067.828\n",
      "[Epoch 8, Batch 75/100] Loss: 0.1233 | RMSE: 0.1233 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 15040.443\n",
      "[Epoch 8, Batch 76/100] Loss: 0.1275 | RMSE: 0.1275 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67014.844\n",
      "[Epoch 8, Batch 77/100] Loss: 0.1302 | RMSE: 0.1302 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77655.789\n",
      "[Epoch 8, Batch 78/100] Loss: 0.1223 | RMSE: 0.1223 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42675.117\n",
      "[Epoch 8, Batch 79/100] Loss: 0.1380 | RMSE: 0.1380 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88433.203\n",
      "[Epoch 8, Batch 80/100] Loss: 0.1240 | RMSE: 0.1240 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57139.734\n",
      "[Epoch 8, Batch 81/100] Loss: 0.1247 | RMSE: 0.1247 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54397.609\n",
      "[Epoch 8, Batch 82/100] Loss: 0.1286 | RMSE: 0.1286 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80793.438\n",
      "[Epoch 8, Batch 83/100] Loss: 0.1248 | RMSE: 0.1248 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72722.531\n",
      "[Epoch 8, Batch 84/100] Loss: 0.1264 | RMSE: 0.1264 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 30363.516\n",
      "[Epoch 8, Batch 85/100] Loss: 0.1274 | RMSE: 0.1274 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 49062.918\n",
      "[Epoch 8, Batch 86/100] Loss: 0.1284 | RMSE: 0.1284 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51183.164\n",
      "[Epoch 8, Batch 87/100] Loss: 0.1189 | RMSE: 0.1189 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 35443.379\n",
      "[Epoch 8, Batch 88/100] Loss: 0.1239 | RMSE: 0.1239 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67517.570\n",
      "[Epoch 8, Batch 89/100] Loss: 0.1232 | RMSE: 0.1232 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62304.508\n",
      "[Epoch 8, Batch 90/100] Loss: 0.1219 | RMSE: 0.1219 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55008.551\n",
      "[Epoch 8, Batch 91/100] Loss: 0.1336 | RMSE: 0.1336 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76661.812\n",
      "[Epoch 8, Batch 92/100] Loss: 0.1342 | RMSE: 0.1342 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65865.062\n",
      "[Epoch 8, Batch 93/100] Loss: 0.1267 | RMSE: 0.1267 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88874.172\n",
      "[Epoch 8, Batch 94/100] Loss: 0.1376 | RMSE: 0.1376 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 120553.695\n",
      "[Epoch 8, Batch 95/100] Loss: 0.1252 | RMSE: 0.1252 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66750.484\n",
      "[Epoch 8, Batch 96/100] Loss: 0.1227 | RMSE: 0.1227 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 28165.615\n",
      "[Epoch 8, Batch 97/100] Loss: 0.1283 | RMSE: 0.1283 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69421.492\n",
      "[Epoch 8, Batch 98/100] Loss: 0.1230 | RMSE: 0.1230 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54762.156\n",
      "[Epoch 8, Batch 99/100] Loss: 0.1321 | RMSE: 0.1321 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58001.129\n",
      "[Epoch 8, Batch 100/100] Loss: 0.1241 | RMSE: 0.1241 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62560.359\n",
      "✅ Epoch 8 완료!\n",
      "   📈 평균 Loss: 0.128045\n",
      "   📊 평균 RMSE: 0.128045\n",
      "   ⚙️  학습률: 0.001000\n",
      "   🏆 NEW BEST! RMSE: 0.128045\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 9/80 시작...\n",
      "[Epoch 9, Batch 1/100] Loss: 0.1276 | RMSE: 0.1276 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46281.465\n",
      "[Epoch 9, Batch 2/100] Loss: 0.1314 | RMSE: 0.1314 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83804.562\n",
      "[Epoch 9, Batch 3/100] Loss: 0.1256 | RMSE: 0.1256 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70773.398\n",
      "[Epoch 9, Batch 4/100] Loss: 0.1208 | RMSE: 0.1208 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 19997.016\n",
      "[Epoch 9, Batch 5/100] Loss: 0.1321 | RMSE: 0.1321 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 105069.547\n",
      "[Epoch 9, Batch 6/100] Loss: 0.1315 | RMSE: 0.1315 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60231.699\n",
      "[Epoch 9, Batch 7/100] Loss: 0.1278 | RMSE: 0.1278 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66621.258\n",
      "[Epoch 9, Batch 8/100] Loss: 0.1308 | RMSE: 0.1308 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80721.688\n",
      "[Epoch 9, Batch 9/100] Loss: 0.1257 | RMSE: 0.1257 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 63407.543\n",
      "[Epoch 9, Batch 10/100] Loss: 0.1515 | RMSE: 0.1515 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 137568.469\n",
      "[Epoch 9, Batch 11/100] Loss: 0.1395 | RMSE: 0.1395 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 120740.133\n",
      "[Epoch 9, Batch 12/100] Loss: 0.1259 | RMSE: 0.1259 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48293.793\n",
      "[Epoch 9, Batch 13/100] Loss: 0.1439 | RMSE: 0.1439 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 117596.781\n",
      "[Epoch 9, Batch 14/100] Loss: 0.1469 | RMSE: 0.1469 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 111576.188\n",
      "[Epoch 9, Batch 15/100] Loss: 0.1242 | RMSE: 0.1242 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 26593.568\n",
      "[Epoch 9, Batch 16/100] Loss: 0.1316 | RMSE: 0.1316 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 94503.023\n",
      "[Epoch 9, Batch 17/100] Loss: 0.1309 | RMSE: 0.1309 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86295.984\n",
      "[Epoch 9, Batch 18/100] Loss: 0.1220 | RMSE: 0.1220 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 24276.055\n",
      "[Epoch 9, Batch 19/100] Loss: 0.1371 | RMSE: 0.1371 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 101031.289\n",
      "[Epoch 9, Batch 20/100] Loss: 0.1348 | RMSE: 0.1348 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93737.609\n",
      "[Epoch 9, Batch 21/100] Loss: 0.1357 | RMSE: 0.1357 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 98723.688\n",
      "[Epoch 9, Batch 22/100] Loss: 0.1269 | RMSE: 0.1269 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 56544.160\n",
      "[Epoch 9, Batch 23/100] Loss: 0.1281 | RMSE: 0.1281 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60371.344\n",
      "[Epoch 9, Batch 24/100] Loss: 0.1257 | RMSE: 0.1257 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44837.305\n",
      "[Epoch 9, Batch 25/100] Loss: 0.1230 | RMSE: 0.1230 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51017.742\n",
      "[Epoch 9, Batch 26/100] Loss: 0.1270 | RMSE: 0.1270 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73778.797\n",
      "[Epoch 9, Batch 27/100] Loss: 0.1275 | RMSE: 0.1275 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77863.555\n",
      "[Epoch 9, Batch 28/100] Loss: 0.1284 | RMSE: 0.1284 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 27043.551\n",
      "[Epoch 9, Batch 29/100] Loss: 0.1209 | RMSE: 0.1209 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 41151.816\n",
      "[Epoch 9, Batch 30/100] Loss: 0.1264 | RMSE: 0.1264 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 96517.109\n",
      "[Epoch 9, Batch 31/100] Loss: 0.1279 | RMSE: 0.1279 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58349.672\n",
      "[Epoch 9, Batch 32/100] Loss: 0.1181 | RMSE: 0.1181 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 38569.766\n",
      "[Epoch 9, Batch 33/100] Loss: 0.1235 | RMSE: 0.1235 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54944.410\n",
      "[Epoch 9, Batch 34/100] Loss: 0.1243 | RMSE: 0.1243 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42446.250\n",
      "[Epoch 9, Batch 35/100] Loss: 0.1431 | RMSE: 0.1431 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 124029.625\n",
      "[Epoch 9, Batch 36/100] Loss: 0.1298 | RMSE: 0.1298 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81302.305\n",
      "[Epoch 9, Batch 37/100] Loss: 0.1260 | RMSE: 0.1260 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 49211.297\n",
      "[Epoch 9, Batch 38/100] Loss: 0.1297 | RMSE: 0.1297 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87864.195\n",
      "[Epoch 9, Batch 39/100] Loss: 0.1256 | RMSE: 0.1256 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57381.680\n",
      "[Epoch 9, Batch 40/100] Loss: 0.1256 | RMSE: 0.1256 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62459.855\n",
      "[Epoch 9, Batch 41/100] Loss: 0.1324 | RMSE: 0.1324 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92502.312\n",
      "[Epoch 9, Batch 42/100] Loss: 0.1179 | RMSE: 0.1179 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 28227.268\n",
      "[Epoch 9, Batch 43/100] Loss: 0.1281 | RMSE: 0.1281 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100290.289\n",
      "[Epoch 9, Batch 44/100] Loss: 0.1256 | RMSE: 0.1256 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 85892.641\n",
      "[Epoch 9, Batch 45/100] Loss: 0.1187 | RMSE: 0.1187 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 48503.457\n",
      "[Epoch 9, Batch 46/100] Loss: 0.1216 | RMSE: 0.1216 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 16441.367\n",
      "[Epoch 9, Batch 47/100] Loss: 0.1301 | RMSE: 0.1301 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54075.574\n",
      "[Epoch 9, Batch 48/100] Loss: 0.1266 | RMSE: 0.1266 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 56886.777\n",
      "[Epoch 9, Batch 49/100] Loss: 0.1281 | RMSE: 0.1281 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 38683.609\n",
      "[Epoch 9, Batch 50/100] Loss: 0.1204 | RMSE: 0.1204 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46490.605\n",
      "[Epoch 9, Batch 51/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 95597.258\n",
      "[Epoch 9, Batch 52/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 36435.848\n",
      "[Epoch 9, Batch 53/100] Loss: 0.1236 | RMSE: 0.1236 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 40353.305\n",
      "[Epoch 9, Batch 54/100] Loss: 0.1256 | RMSE: 0.1256 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58600.141\n",
      "[Epoch 9, Batch 55/100] Loss: 0.1251 | RMSE: 0.1251 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60240.410\n",
      "[Epoch 9, Batch 56/100] Loss: 0.1220 | RMSE: 0.1220 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 34110.289\n",
      "[Epoch 9, Batch 57/100] Loss: 0.1273 | RMSE: 0.1273 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66555.367\n",
      "[Epoch 9, Batch 58/100] Loss: 0.1293 | RMSE: 0.1293 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 94510.469\n",
      "[Epoch 9, Batch 59/100] Loss: 0.1274 | RMSE: 0.1274 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 42889.770\n",
      "[Epoch 9, Batch 60/100] Loss: 0.1257 | RMSE: 0.1257 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72890.422\n",
      "[Epoch 9, Batch 61/100] Loss: 0.1235 | RMSE: 0.1235 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55240.152\n",
      "[Epoch 9, Batch 62/100] Loss: 0.1389 | RMSE: 0.1389 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 115630.164\n",
      "[Epoch 9, Batch 63/100] Loss: 0.1273 | RMSE: 0.1273 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88440.234\n",
      "[Epoch 9, Batch 64/100] Loss: 0.1193 | RMSE: 0.1193 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45471.582\n",
      "[Epoch 9, Batch 65/100] Loss: 0.1300 | RMSE: 0.1300 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61741.551\n",
      "[Epoch 9, Batch 66/100] Loss: 0.1291 | RMSE: 0.1291 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69317.039\n",
      "[Epoch 9, Batch 67/100] Loss: 0.1283 | RMSE: 0.1283 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83449.227\n",
      "[Epoch 9, Batch 68/100] Loss: 0.1244 | RMSE: 0.1244 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78480.820\n",
      "[Epoch 9, Batch 69/100] Loss: 0.1269 | RMSE: 0.1269 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78937.961\n",
      "[Epoch 9, Batch 70/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 33318.578\n",
      "[Epoch 9, Batch 71/100] Loss: 0.1321 | RMSE: 0.1321 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 93706.828\n",
      "[Epoch 9, Batch 72/100] Loss: 0.1408 | RMSE: 0.1408 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92820.117\n",
      "[Epoch 9, Batch 73/100] Loss: 0.1259 | RMSE: 0.1259 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66805.133\n",
      "[Epoch 9, Batch 74/100] Loss: 0.1280 | RMSE: 0.1280 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71714.797\n",
      "[Epoch 9, Batch 75/100] Loss: 0.1303 | RMSE: 0.1303 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 94701.695\n",
      "[Epoch 9, Batch 76/100] Loss: 0.1262 | RMSE: 0.1262 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57876.637\n",
      "[Epoch 9, Batch 77/100] Loss: 0.1271 | RMSE: 0.1271 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81972.398\n",
      "[Epoch 9, Batch 78/100] Loss: 0.1313 | RMSE: 0.1313 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 107112.266\n",
      "[Epoch 9, Batch 79/100] Loss: 0.1186 | RMSE: 0.1186 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 29148.035\n",
      "[Epoch 9, Batch 80/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 100831.398\n",
      "[Epoch 9, Batch 81/100] Loss: 0.1285 | RMSE: 0.1285 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 88064.047\n",
      "[Epoch 9, Batch 82/100] Loss: 0.1222 | RMSE: 0.1222 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71016.891\n",
      "[Epoch 9, Batch 83/100] Loss: 0.1255 | RMSE: 0.1255 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59513.309\n",
      "[Epoch 9, Batch 84/100] Loss: 0.1232 | RMSE: 0.1232 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 72314.641\n",
      "[Epoch 9, Batch 85/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70587.789\n",
      "[Epoch 9, Batch 86/100] Loss: 0.1234 | RMSE: 0.1234 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 30778.523\n",
      "[Epoch 9, Batch 87/100] Loss: 0.1239 | RMSE: 0.1239 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90327.453\n",
      "[Epoch 9, Batch 88/100] Loss: 0.1219 | RMSE: 0.1219 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 36224.066\n",
      "[Epoch 9, Batch 89/100] Loss: 0.1179 | RMSE: 0.1179 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 24501.787\n",
      "[Epoch 9, Batch 90/100] Loss: 0.1247 | RMSE: 0.1247 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70729.719\n",
      "[Epoch 9, Batch 91/100] Loss: 0.1257 | RMSE: 0.1257 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66351.133\n",
      "[Epoch 9, Batch 92/100] Loss: 0.1242 | RMSE: 0.1242 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73638.414\n",
      "[Epoch 9, Batch 93/100] Loss: 0.1299 | RMSE: 0.1299 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70188.797\n",
      "[Epoch 9, Batch 94/100] Loss: 0.1270 | RMSE: 0.1270 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 46508.711\n",
      "[Epoch 9, Batch 95/100] Loss: 0.1195 | RMSE: 0.1195 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 56614.391\n",
      "[Epoch 9, Batch 96/100] Loss: 0.1248 | RMSE: 0.1248 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70922.117\n",
      "[Epoch 9, Batch 97/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71280.328\n",
      "[Epoch 9, Batch 98/100] Loss: 0.1189 | RMSE: 0.1189 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 28921.508\n",
      "[Epoch 9, Batch 99/100] Loss: 0.1263 | RMSE: 0.1263 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58570.867\n",
      "[Epoch 9, Batch 100/100] Loss: 0.1272 | RMSE: 0.1272 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87891.141\n",
      "✅ Epoch 9 완료!\n",
      "   📈 평균 Loss: 0.127542\n",
      "   📊 평균 RMSE: 0.127542\n",
      "   ⚙️  학습률: 0.001000\n",
      "   🏆 NEW BEST! RMSE: 0.127542\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 10/80 시작...\n",
      "[Epoch 10, Batch 1/100] Loss: 0.1277 | RMSE: 0.1277 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59457.965\n",
      "[Epoch 10, Batch 2/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65897.617\n",
      "[Epoch 10, Batch 3/100] Loss: 0.1303 | RMSE: 0.1303 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 83405.836\n",
      "[Epoch 10, Batch 4/100] Loss: 0.1193 | RMSE: 0.1193 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 18109.270\n",
      "[Epoch 10, Batch 5/100] Loss: 0.1208 | RMSE: 0.1208 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 30494.092\n",
      "[Epoch 10, Batch 6/100] Loss: 0.1259 | RMSE: 0.1259 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 39406.145\n",
      "[Epoch 10, Batch 7/100] Loss: 0.1163 | RMSE: 0.1163 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 31491.736\n",
      "[Epoch 10, Batch 8/100] Loss: 0.1301 | RMSE: 0.1301 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74110.258\n",
      "[Epoch 10, Batch 9/100] Loss: 0.1227 | RMSE: 0.1227 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 59302.043\n",
      "[Epoch 10, Batch 10/100] Loss: 0.1255 | RMSE: 0.1255 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61701.125\n",
      "[Epoch 10, Batch 11/100] Loss: 0.1232 | RMSE: 0.1232 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 82273.688\n",
      "[Epoch 10, Batch 12/100] Loss: 0.1169 | RMSE: 0.1169 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 34059.211\n",
      "[Epoch 10, Batch 13/100] Loss: 0.1322 | RMSE: 0.1322 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81672.375\n",
      "[Epoch 10, Batch 14/100] Loss: 0.1256 | RMSE: 0.1256 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64640.566\n",
      "[Epoch 10, Batch 15/100] Loss: 0.1210 | RMSE: 0.1210 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 33906.906\n",
      "[Epoch 10, Batch 16/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86482.484\n",
      "[Epoch 10, Batch 17/100] Loss: 0.1285 | RMSE: 0.1285 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 95745.977\n",
      "[Epoch 10, Batch 18/100] Loss: 0.1206 | RMSE: 0.1206 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 40060.465\n",
      "[Epoch 10, Batch 19/100] Loss: 0.1213 | RMSE: 0.1213 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44194.352\n",
      "[Epoch 10, Batch 20/100] Loss: 0.1368 | RMSE: 0.1368 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 84323.062\n",
      "[Epoch 10, Batch 21/100] Loss: 0.1318 | RMSE: 0.1318 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91474.547\n",
      "[Epoch 10, Batch 22/100] Loss: 0.1261 | RMSE: 0.1261 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 16441.434\n",
      "[Epoch 10, Batch 23/100] Loss: 0.1298 | RMSE: 0.1298 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 37544.938\n",
      "[Epoch 10, Batch 24/100] Loss: 0.1264 | RMSE: 0.1264 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 63017.629\n",
      "[Epoch 10, Batch 25/100] Loss: 0.1210 | RMSE: 0.1210 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 47657.402\n",
      "[Epoch 10, Batch 26/100] Loss: 0.1190 | RMSE: 0.1190 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57018.250\n",
      "[Epoch 10, Batch 27/100] Loss: 0.1302 | RMSE: 0.1302 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 57400.930\n",
      "[Epoch 10, Batch 28/100] Loss: 0.1308 | RMSE: 0.1308 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44115.840\n",
      "[Epoch 10, Batch 29/100] Loss: 0.1192 | RMSE: 0.1192 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 30805.428\n",
      "[Epoch 10, Batch 30/100] Loss: 0.1241 | RMSE: 0.1241 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60573.691\n",
      "[Epoch 10, Batch 31/100] Loss: 0.1234 | RMSE: 0.1234 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62774.953\n",
      "[Epoch 10, Batch 32/100] Loss: 0.1317 | RMSE: 0.1317 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79114.578\n",
      "[Epoch 10, Batch 33/100] Loss: 0.1184 | RMSE: 0.1184 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 28323.561\n",
      "[Epoch 10, Batch 34/100] Loss: 0.1236 | RMSE: 0.1236 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60826.555\n",
      "[Epoch 10, Batch 35/100] Loss: 0.1368 | RMSE: 0.1368 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 92460.633\n",
      "[Epoch 10, Batch 36/100] Loss: 0.1210 | RMSE: 0.1210 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 45017.051\n",
      "[Epoch 10, Batch 37/100] Loss: 0.1189 | RMSE: 0.1189 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 56135.238\n",
      "[Epoch 10, Batch 38/100] Loss: 0.1302 | RMSE: 0.1302 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 90566.266\n",
      "[Epoch 10, Batch 39/100] Loss: 0.1247 | RMSE: 0.1247 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60991.109\n",
      "[Epoch 10, Batch 40/100] Loss: 0.1286 | RMSE: 0.1286 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60640.273\n",
      "[Epoch 10, Batch 41/100] Loss: 0.1275 | RMSE: 0.1275 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 60830.789\n",
      "[Epoch 10, Batch 42/100] Loss: 0.1229 | RMSE: 0.1229 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 52072.168\n",
      "[Epoch 10, Batch 43/100] Loss: 0.1335 | RMSE: 0.1335 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 69610.398\n",
      "[Epoch 10, Batch 44/100] Loss: 0.1250 | RMSE: 0.1250 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61292.398\n",
      "[Epoch 10, Batch 45/100] Loss: 0.1259 | RMSE: 0.1259 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 65560.539\n",
      "[Epoch 10, Batch 46/100] Loss: 0.1170 | RMSE: 0.1170 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 41857.832\n",
      "[Epoch 10, Batch 47/100] Loss: 0.1197 | RMSE: 0.1197 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53074.566\n",
      "[Epoch 10, Batch 48/100] Loss: 0.1238 | RMSE: 0.1238 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 62177.246\n",
      "[Epoch 10, Batch 49/100] Loss: 0.1245 | RMSE: 0.1245 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 76991.703\n",
      "[Epoch 10, Batch 50/100] Loss: 0.1247 | RMSE: 0.1247 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 73347.656\n",
      "[Epoch 10, Batch 51/100] Loss: 0.1234 | RMSE: 0.1234 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 61744.430\n",
      "[Epoch 10, Batch 52/100] Loss: 0.1254 | RMSE: 0.1254 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 63326.875\n",
      "[Epoch 10, Batch 53/100] Loss: 0.1237 | RMSE: 0.1237 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53911.461\n",
      "[Epoch 10, Batch 54/100] Loss: 0.1243 | RMSE: 0.1243 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86366.594\n",
      "[Epoch 10, Batch 55/100] Loss: 0.1203 | RMSE: 0.1203 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 40775.289\n",
      "[Epoch 10, Batch 56/100] Loss: 0.1262 | RMSE: 0.1262 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 97994.070\n",
      "[Epoch 10, Batch 57/100] Loss: 0.1230 | RMSE: 0.1230 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74067.836\n",
      "[Epoch 10, Batch 58/100] Loss: 0.1209 | RMSE: 0.1209 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53138.836\n",
      "[Epoch 10, Batch 59/100] Loss: 0.1262 | RMSE: 0.1262 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77374.977\n",
      "[Epoch 10, Batch 60/100] Loss: 0.1203 | RMSE: 0.1203 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67793.383\n",
      "[Epoch 10, Batch 61/100] Loss: 0.1239 | RMSE: 0.1239 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 95874.219\n",
      "[Epoch 10, Batch 62/100] Loss: 0.1228 | RMSE: 0.1228 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 80683.734\n",
      "[Epoch 10, Batch 63/100] Loss: 0.1254 | RMSE: 0.1254 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 53364.648\n",
      "[Epoch 10, Batch 64/100] Loss: 0.1217 | RMSE: 0.1217 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 70761.367\n",
      "[Epoch 10, Batch 65/100] Loss: 0.1280 | RMSE: 0.1280 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91797.789\n",
      "[Epoch 10, Batch 66/100] Loss: 0.1237 | RMSE: 0.1237 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 51682.094\n",
      "[Epoch 10, Batch 67/100] Loss: 0.1231 | RMSE: 0.1231 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 91847.180\n",
      "[Epoch 10, Batch 68/100] Loss: 0.1240 | RMSE: 0.1240 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 55937.098\n",
      "[Epoch 10, Batch 69/100] Loss: 0.1314 | RMSE: 0.1314 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 125221.828\n",
      "[Epoch 10, Batch 70/100] Loss: 0.1258 | RMSE: 0.1258 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 79004.898\n",
      "[Epoch 10, Batch 71/100] Loss: 0.1176 | RMSE: 0.1176 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 49762.141\n",
      "[Epoch 10, Batch 72/100] Loss: 0.1219 | RMSE: 0.1219 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44497.797\n",
      "[Epoch 10, Batch 73/100] Loss: 0.1208 | RMSE: 0.1208 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 20826.525\n",
      "[Epoch 10, Batch 74/100] Loss: 0.1408 | RMSE: 0.1408 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 102463.828\n",
      "[Epoch 10, Batch 75/100] Loss: 0.1381 | RMSE: 0.1381 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 74044.719\n",
      "[Epoch 10, Batch 76/100] Loss: 0.1288 | RMSE: 0.1288 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77118.523\n",
      "[Epoch 10, Batch 77/100] Loss: 0.1221 | RMSE: 0.1221 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68780.883\n",
      "[Epoch 10, Batch 78/100] Loss: 0.1314 | RMSE: 0.1314 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 95525.055\n",
      "[Epoch 10, Batch 79/100] Loss: 0.1312 | RMSE: 0.1312 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 81609.180\n",
      "[Epoch 10, Batch 80/100] Loss: 0.1258 | RMSE: 0.1258 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 58884.137\n",
      "[Epoch 10, Batch 81/100] Loss: 0.1273 | RMSE: 0.1273 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 77993.805\n",
      "[Epoch 10, Batch 82/100] Loss: 0.1371 | RMSE: 0.1371 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 118946.469\n",
      "[Epoch 10, Batch 83/100] Loss: 0.1279 | RMSE: 0.1279 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 52165.363\n",
      "[Epoch 10, Batch 84/100] Loss: 0.1260 | RMSE: 0.1260 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 64997.328\n",
      "[Epoch 10, Batch 85/100] Loss: 0.1317 | RMSE: 0.1317 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 109112.758\n",
      "[Epoch 10, Batch 86/100] Loss: 0.1363 | RMSE: 0.1363 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 103213.242\n",
      "[Epoch 10, Batch 87/100] Loss: 0.1261 | RMSE: 0.1261 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 87422.148\n",
      "[Epoch 10, Batch 88/100] Loss: 0.1252 | RMSE: 0.1252 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 78565.242\n",
      "[Epoch 10, Batch 89/100] Loss: 0.1238 | RMSE: 0.1238 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 86090.719\n",
      "[Epoch 10, Batch 90/100] Loss: 0.1210 | RMSE: 0.1210 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 67630.156\n",
      "[Epoch 10, Batch 91/100] Loss: 0.1210 | RMSE: 0.1210 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 66138.055\n",
      "[Epoch 10, Batch 92/100] Loss: 0.1268 | RMSE: 0.1268 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 71471.602\n",
      "[Epoch 10, Batch 93/100] Loss: 0.1197 | RMSE: 0.1197 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54220.973\n",
      "[Epoch 10, Batch 94/100] Loss: 0.1227 | RMSE: 0.1227 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 68685.195\n",
      "[Epoch 10, Batch 95/100] Loss: 0.1219 | RMSE: 0.1219 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 44797.012\n",
      "[Epoch 10, Batch 96/100] Loss: 0.1148 | RMSE: 0.1148 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 17718.049\n",
      "[Epoch 10, Batch 97/100] Loss: 0.1205 | RMSE: 0.1205 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 54883.297\n",
      "[Epoch 10, Batch 98/100] Loss: 0.1223 | RMSE: 0.1223 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 63833.867\n",
      "[Epoch 10, Batch 99/100] Loss: 0.1165 | RMSE: 0.1165 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 37372.035\n",
      "[Epoch 10, Batch 100/100] Loss: 0.1164 | RMSE: 0.1164 | Consistency: 0.0000 | SSIM: 0.0000 | Grad: 38337.500\n",
      "✅ Epoch 10 완료!\n",
      "   📈 평균 Loss: 0.125097\n",
      "   📊 평균 RMSE: 0.125097\n",
      "   ⚙️  학습률: 0.000998\n",
      "   🏆 NEW BEST! RMSE: 0.125097\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 11/80 시작...\n",
      "[Epoch 11, Batch 1/100] Loss: 0.2136 | RMSE: 0.1268 | Consistency: 0.0764 | SSIM: 0.8604 | Grad: 82351.062\n",
      "[Epoch 11, Batch 2/100] Loss: 0.2030 | RMSE: 0.1165 | Consistency: 0.0604 | SSIM: 0.8593 | Grad: 30054.881\n",
      "[Epoch 11, Batch 3/100] Loss: 0.2040 | RMSE: 0.1175 | Consistency: 0.0614 | SSIM: 0.8583 | Grad: 59876.980\n",
      "[Epoch 11, Batch 4/100] Loss: 0.2123 | RMSE: 0.1254 | Consistency: 0.0699 | SSIM: 0.8617 | Grad: 70518.438\n",
      "[Epoch 11, Batch 5/100] Loss: 0.2085 | RMSE: 0.1220 | Consistency: 0.0646 | SSIM: 0.8586 | Grad: 57255.953\n",
      "[Epoch 11, Batch 6/100] Loss: 0.2047 | RMSE: 0.1179 | Consistency: 0.0615 | SSIM: 0.8618 | Grad: 21681.531\n",
      "[Epoch 11, Batch 7/100] Loss: 0.2109 | RMSE: 0.1240 | Consistency: 0.0680 | SSIM: 0.8620 | Grad: 48092.855\n",
      "[Epoch 11, Batch 8/100] Loss: 0.2126 | RMSE: 0.1258 | Consistency: 0.0672 | SSIM: 0.8613 | Grad: 79109.156\n",
      "[Epoch 11, Batch 9/100] Loss: 0.2125 | RMSE: 0.1256 | Consistency: 0.0655 | SSIM: 0.8625 | Grad: 72548.930\n",
      "[Epoch 11, Batch 10/100] Loss: 0.2157 | RMSE: 0.1287 | Consistency: 0.0732 | SSIM: 0.8631 | Grad: 76090.039\n",
      "[Epoch 11, Batch 11/100] Loss: 0.2094 | RMSE: 0.1224 | Consistency: 0.0646 | SSIM: 0.8628 | Grad: 67312.195\n",
      "[Epoch 11, Batch 12/100] Loss: 0.2056 | RMSE: 0.1188 | Consistency: 0.0616 | SSIM: 0.8613 | Grad: 44919.516\n",
      "[Epoch 11, Batch 13/100] Loss: 0.2060 | RMSE: 0.1192 | Consistency: 0.0610 | SSIM: 0.8621 | Grad: 49676.734\n",
      "[Epoch 11, Batch 14/100] Loss: 0.2079 | RMSE: 0.1210 | Consistency: 0.0637 | SSIM: 0.8620 | Grad: 44675.645\n",
      "[Epoch 11, Batch 15/100] Loss: 0.2058 | RMSE: 0.1190 | Consistency: 0.0630 | SSIM: 0.8615 | Grad: 38483.633\n",
      "[Epoch 11, Batch 16/100] Loss: 0.2110 | RMSE: 0.1240 | Consistency: 0.0656 | SSIM: 0.8639 | Grad: 40325.887\n",
      "[Epoch 11, Batch 17/100] Loss: 0.2158 | RMSE: 0.1288 | Consistency: 0.0713 | SSIM: 0.8633 | Grad: 55172.527\n",
      "[Epoch 11, Batch 18/100] Loss: 0.2095 | RMSE: 0.1225 | Consistency: 0.0621 | SSIM: 0.8636 | Grad: 29304.590\n",
      "[Epoch 11, Batch 19/100] Loss: 0.2098 | RMSE: 0.1230 | Consistency: 0.0673 | SSIM: 0.8612 | Grad: 54888.672\n",
      "[Epoch 11, Batch 20/100] Loss: 0.2095 | RMSE: 0.1226 | Consistency: 0.0638 | SSIM: 0.8633 | Grad: 52647.918\n",
      "[Epoch 11, Batch 21/100] Loss: 0.2156 | RMSE: 0.1285 | Consistency: 0.0700 | SSIM: 0.8638 | Grad: 15484.418\n",
      "[Epoch 11, Batch 22/100] Loss: 0.2097 | RMSE: 0.1229 | Consistency: 0.0657 | SSIM: 0.8617 | Grad: 70237.617\n",
      "[Epoch 11, Batch 23/100] Loss: 0.2115 | RMSE: 0.1246 | Consistency: 0.0646 | SSIM: 0.8631 | Grad: 65784.742\n",
      "[Epoch 11, Batch 24/100] Loss: 0.2120 | RMSE: 0.1250 | Consistency: 0.0683 | SSIM: 0.8634 | Grad: 49329.559\n",
      "[Epoch 11, Batch 25/100] Loss: 0.2101 | RMSE: 0.1230 | Consistency: 0.0685 | SSIM: 0.8635 | Grad: 62652.574\n",
      "[Epoch 11, Batch 26/100] Loss: 0.2108 | RMSE: 0.1239 | Consistency: 0.0625 | SSIM: 0.8632 | Grad: 75924.180\n",
      "[Epoch 11, Batch 27/100] Loss: 0.2135 | RMSE: 0.1265 | Consistency: 0.0694 | SSIM: 0.8628 | Grad: 40894.879\n",
      "[Epoch 11, Batch 28/100] Loss: 0.2045 | RMSE: 0.1177 | Consistency: 0.0584 | SSIM: 0.8621 | Grad: 35866.551\n",
      "[Epoch 11, Batch 29/100] Loss: 0.2096 | RMSE: 0.1228 | Consistency: 0.0694 | SSIM: 0.8607 | Grad: 41723.695\n",
      "[Epoch 11, Batch 30/100] Loss: 0.2117 | RMSE: 0.1249 | Consistency: 0.0713 | SSIM: 0.8615 | Grad: 68313.859\n",
      "[Epoch 11, Batch 31/100] Loss: 0.2097 | RMSE: 0.1229 | Consistency: 0.0650 | SSIM: 0.8613 | Grad: 57824.070\n",
      "[Epoch 11, Batch 32/100] Loss: 0.2080 | RMSE: 0.1214 | Consistency: 0.0663 | SSIM: 0.8590 | Grad: 74669.164\n",
      "[Epoch 11, Batch 33/100] Loss: 0.2076 | RMSE: 0.1209 | Consistency: 0.0670 | SSIM: 0.8602 | Grad: 60591.461\n",
      "[Epoch 11, Batch 34/100] Loss: 0.2062 | RMSE: 0.1193 | Consistency: 0.0620 | SSIM: 0.8632 | Grad: 63777.219\n",
      "[Epoch 11, Batch 35/100] Loss: 0.2160 | RMSE: 0.1289 | Consistency: 0.0783 | SSIM: 0.8631 | Grad: 54750.410\n",
      "[Epoch 11, Batch 36/100] Loss: 0.2174 | RMSE: 0.1304 | Consistency: 0.0693 | SSIM: 0.8634 | Grad: 88489.117\n",
      "[Epoch 11, Batch 37/100] Loss: 0.2126 | RMSE: 0.1256 | Consistency: 0.0681 | SSIM: 0.8629 | Grad: 61772.406\n",
      "[Epoch 11, Batch 38/100] Loss: 0.2081 | RMSE: 0.1213 | Consistency: 0.0656 | SSIM: 0.8614 | Grad: 60022.488\n",
      "[Epoch 11, Batch 39/100] Loss: 0.2046 | RMSE: 0.1180 | Consistency: 0.0630 | SSIM: 0.8601 | Grad: 81615.070\n",
      "[Epoch 11, Batch 40/100] Loss: 0.2106 | RMSE: 0.1237 | Consistency: 0.0721 | SSIM: 0.8618 | Grad: 98929.891\n",
      "[Epoch 11, Batch 41/100] Loss: 0.2078 | RMSE: 0.1210 | Consistency: 0.0664 | SSIM: 0.8609 | Grad: 54194.520\n",
      "[Epoch 11, Batch 42/100] Loss: 0.2051 | RMSE: 0.1183 | Consistency: 0.0635 | SSIM: 0.8617 | Grad: 37157.035\n",
      "[Epoch 11, Batch 43/100] Loss: 0.2042 | RMSE: 0.1174 | Consistency: 0.0598 | SSIM: 0.8620 | Grad: 51864.207\n",
      "[Epoch 11, Batch 44/100] Loss: 0.2070 | RMSE: 0.1204 | Consistency: 0.0602 | SSIM: 0.8606 | Grad: 60380.953\n",
      "[Epoch 11, Batch 45/100] Loss: 0.2086 | RMSE: 0.1220 | Consistency: 0.0681 | SSIM: 0.8593 | Grad: 53387.695\n",
      "[Epoch 11, Batch 46/100] Loss: 0.2081 | RMSE: 0.1214 | Consistency: 0.0609 | SSIM: 0.8606 | Grad: 72001.250\n",
      "[Epoch 11, Batch 47/100] Loss: 0.2069 | RMSE: 0.1202 | Consistency: 0.0663 | SSIM: 0.8604 | Grad: 63651.871\n",
      "[Epoch 11, Batch 48/100] Loss: 0.2067 | RMSE: 0.1200 | Consistency: 0.0612 | SSIM: 0.8607 | Grad: 41210.910\n",
      "[Epoch 11, Batch 49/100] Loss: 0.2011 | RMSE: 0.1145 | Consistency: 0.0598 | SSIM: 0.8597 | Grad: 51830.719\n",
      "[Epoch 11, Batch 50/100] Loss: 0.2042 | RMSE: 0.1176 | Consistency: 0.0626 | SSIM: 0.8603 | Grad: 38597.828\n",
      "[Epoch 11, Batch 51/100] Loss: 0.2116 | RMSE: 0.1246 | Consistency: 0.0740 | SSIM: 0.8621 | Grad: 86043.414\n",
      "[Epoch 11, Batch 52/100] Loss: 0.2102 | RMSE: 0.1235 | Consistency: 0.0719 | SSIM: 0.8596 | Grad: 84865.883\n",
      "[Epoch 11, Batch 53/100] Loss: 0.2033 | RMSE: 0.1168 | Consistency: 0.0606 | SSIM: 0.8589 | Grad: 51487.270\n",
      "[Epoch 11, Batch 54/100] Loss: 0.2072 | RMSE: 0.1203 | Consistency: 0.0609 | SSIM: 0.8625 | Grad: 60844.043\n",
      "[Epoch 11, Batch 55/100] Loss: 0.2102 | RMSE: 0.1233 | Consistency: 0.0682 | SSIM: 0.8619 | Grad: 59390.223\n",
      "[Epoch 11, Batch 56/100] Loss: 0.2103 | RMSE: 0.1234 | Consistency: 0.0640 | SSIM: 0.8622 | Grad: 51216.121\n",
      "[Epoch 11, Batch 57/100] Loss: 0.2103 | RMSE: 0.1235 | Consistency: 0.0665 | SSIM: 0.8615 | Grad: 47874.664\n",
      "[Epoch 11, Batch 58/100] Loss: 0.2079 | RMSE: 0.1210 | Consistency: 0.0636 | SSIM: 0.8628 | Grad: 52379.488\n",
      "[Epoch 11, Batch 59/100] Loss: 0.2152 | RMSE: 0.1282 | Consistency: 0.0701 | SSIM: 0.8627 | Grad: 70480.031\n",
      "[Epoch 11, Batch 60/100] Loss: 0.2131 | RMSE: 0.1261 | Consistency: 0.0700 | SSIM: 0.8627 | Grad: 56695.633\n",
      "[Epoch 11, Batch 61/100] Loss: 0.2103 | RMSE: 0.1237 | Consistency: 0.0637 | SSIM: 0.8599 | Grad: 47506.988\n",
      "[Epoch 11, Batch 62/100] Loss: 0.2091 | RMSE: 0.1221 | Consistency: 0.0653 | SSIM: 0.8630 | Grad: 50208.051\n",
      "[Epoch 11, Batch 63/100] Loss: 0.2071 | RMSE: 0.1202 | Consistency: 0.0647 | SSIM: 0.8618 | Grad: 21816.248\n",
      "[Epoch 11, Batch 64/100] Loss: 0.2133 | RMSE: 0.1264 | Consistency: 0.0642 | SSIM: 0.8628 | Grad: 92493.844\n",
      "[Epoch 11, Batch 65/100] Loss: 0.2079 | RMSE: 0.1211 | Consistency: 0.0588 | SSIM: 0.8617 | Grad: 78302.414\n",
      "[Epoch 11, Batch 66/100] Loss: 0.2053 | RMSE: 0.1185 | Consistency: 0.0616 | SSIM: 0.8613 | Grad: 51551.680\n",
      "[Epoch 11, Batch 67/100] Loss: 0.2180 | RMSE: 0.1309 | Consistency: 0.0696 | SSIM: 0.8638 | Grad: 71699.508\n",
      "[Epoch 11, Batch 68/100] Loss: 0.2151 | RMSE: 0.1282 | Consistency: 0.0663 | SSIM: 0.8631 | Grad: 100830.062\n",
      "[Epoch 11, Batch 69/100] Loss: 0.2076 | RMSE: 0.1208 | Consistency: 0.0650 | SSIM: 0.8613 | Grad: 24439.273\n",
      "[Epoch 11, Batch 70/100] Loss: 0.2096 | RMSE: 0.1225 | Consistency: 0.0673 | SSIM: 0.8640 | Grad: 67036.094\n",
      "[Epoch 11, Batch 71/100] Loss: 0.2107 | RMSE: 0.1240 | Consistency: 0.0603 | SSIM: 0.8610 | Grad: 82709.203\n",
      "[Epoch 11, Batch 72/100] Loss: 0.2103 | RMSE: 0.1234 | Consistency: 0.0633 | SSIM: 0.8632 | Grad: 56998.762\n",
      "[Epoch 11, Batch 73/100] Loss: 0.2069 | RMSE: 0.1203 | Consistency: 0.0602 | SSIM: 0.8601 | Grad: 72412.109\n",
      "[Epoch 11, Batch 74/100] Loss: 0.2223 | RMSE: 0.1354 | Consistency: 0.0624 | SSIM: 0.8633 | Grad: 105668.609\n",
      "[Epoch 11, Batch 75/100] Loss: 0.2100 | RMSE: 0.1232 | Consistency: 0.0611 | SSIM: 0.8625 | Grad: 60214.066\n",
      "[Epoch 11, Batch 76/100] Loss: 0.2078 | RMSE: 0.1210 | Consistency: 0.0605 | SSIM: 0.8621 | Grad: 76416.867\n",
      "[Epoch 11, Batch 77/100] Loss: 0.2261 | RMSE: 0.1391 | Consistency: 0.0695 | SSIM: 0.8632 | Grad: 101901.117\n",
      "[Epoch 11, Batch 78/100] Loss: 0.2141 | RMSE: 0.1271 | Consistency: 0.0749 | SSIM: 0.8629 | Grad: 66101.008\n",
      "[Epoch 11, Batch 79/100] Loss: 0.2055 | RMSE: 0.1187 | Consistency: 0.0620 | SSIM: 0.8622 | Grad: 26838.490\n",
      "[Epoch 11, Batch 80/100] Loss: 0.2060 | RMSE: 0.1193 | Consistency: 0.0626 | SSIM: 0.8601 | Grad: 54588.566\n",
      "[Epoch 11, Batch 81/100] Loss: 0.2063 | RMSE: 0.1195 | Consistency: 0.0620 | SSIM: 0.8619 | Grad: 55036.559\n",
      "[Epoch 11, Batch 82/100] Loss: 0.2099 | RMSE: 0.1230 | Consistency: 0.0621 | SSIM: 0.8622 | Grad: 55735.219\n",
      "[Epoch 11, Batch 83/100] Loss: 0.2130 | RMSE: 0.1261 | Consistency: 0.0694 | SSIM: 0.8613 | Grad: 64196.125\n",
      "[Epoch 11, Batch 84/100] Loss: 0.2057 | RMSE: 0.1189 | Consistency: 0.0627 | SSIM: 0.8616 | Grad: 34645.180\n",
      "[Epoch 11, Batch 85/100] Loss: 0.2113 | RMSE: 0.1242 | Consistency: 0.0766 | SSIM: 0.8634 | Grad: 77649.906\n",
      "[Epoch 11, Batch 86/100] Loss: 0.2110 | RMSE: 0.1241 | Consistency: 0.0670 | SSIM: 0.8622 | Grad: 82381.266\n",
      "[Epoch 11, Batch 87/100] Loss: 0.2065 | RMSE: 0.1199 | Consistency: 0.0626 | SSIM: 0.8603 | Grad: 50308.953\n",
      "[Epoch 11, Batch 88/100] Loss: 0.2099 | RMSE: 0.1232 | Consistency: 0.0636 | SSIM: 0.8608 | Grad: 21781.537\n",
      "[Epoch 11, Batch 89/100] Loss: 0.2042 | RMSE: 0.1174 | Consistency: 0.0603 | SSIM: 0.8619 | Grad: 35812.086\n",
      "[Epoch 11, Batch 90/100] Loss: 0.2054 | RMSE: 0.1188 | Consistency: 0.0616 | SSIM: 0.8596 | Grad: 41929.094\n",
      "[Epoch 11, Batch 91/100] Loss: 0.2084 | RMSE: 0.1216 | Consistency: 0.0639 | SSIM: 0.8618 | Grad: 70636.328\n",
      "[Epoch 11, Batch 92/100] Loss: 0.2163 | RMSE: 0.1294 | Consistency: 0.0657 | SSIM: 0.8625 | Grad: 91730.859\n",
      "[Epoch 11, Batch 93/100] Loss: 0.2091 | RMSE: 0.1223 | Consistency: 0.0633 | SSIM: 0.8624 | Grad: 69954.695\n",
      "[Epoch 11, Batch 94/100] Loss: 0.2146 | RMSE: 0.1277 | Consistency: 0.0670 | SSIM: 0.8623 | Grad: 74895.617\n",
      "[Epoch 11, Batch 95/100] Loss: 0.2135 | RMSE: 0.1268 | Consistency: 0.0691 | SSIM: 0.8599 | Grad: 72602.844\n",
      "[Epoch 11, Batch 96/100] Loss: 0.2116 | RMSE: 0.1247 | Consistency: 0.0647 | SSIM: 0.8625 | Grad: 85948.820\n",
      "[Epoch 11, Batch 97/100] Loss: 0.2119 | RMSE: 0.1252 | Consistency: 0.0634 | SSIM: 0.8611 | Grad: 84188.141\n",
      "[Epoch 11, Batch 98/100] Loss: 0.2136 | RMSE: 0.1267 | Consistency: 0.0653 | SSIM: 0.8619 | Grad: 76392.102\n",
      "[Epoch 11, Batch 99/100] Loss: 0.2040 | RMSE: 0.1173 | Consistency: 0.0599 | SSIM: 0.8608 | Grad: 59325.141\n",
      "[Epoch 11, Batch 100/100] Loss: 0.2127 | RMSE: 0.1261 | Consistency: 0.0670 | SSIM: 0.8593 | Grad: 83025.820\n",
      "✅ Epoch 11 완료!\n",
      "   📈 평균 Loss: 0.209785\n",
      "   📊 평균 RMSE: 0.122959\n",
      "   ⚙️  학습률: 0.000996\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 12/80 시작...\n",
      "[Epoch 12, Batch 1/100] Loss: 0.2098 | RMSE: 0.1228 | Consistency: 0.0614 | SSIM: 0.8633 | Grad: 69061.516\n",
      "[Epoch 12, Batch 2/100] Loss: 0.2207 | RMSE: 0.1334 | Consistency: 0.0897 | SSIM: 0.8640 | Grad: 109472.266\n",
      "[Epoch 12, Batch 3/100] Loss: 0.2174 | RMSE: 0.1306 | Consistency: 0.0643 | SSIM: 0.8618 | Grad: 114577.781\n",
      "[Epoch 12, Batch 4/100] Loss: 0.2096 | RMSE: 0.1227 | Consistency: 0.0625 | SSIM: 0.8626 | Grad: 60596.645\n",
      "[Epoch 12, Batch 5/100] Loss: 0.2058 | RMSE: 0.1191 | Consistency: 0.0632 | SSIM: 0.8610 | Grad: 30527.438\n",
      "[Epoch 12, Batch 6/100] Loss: 0.2194 | RMSE: 0.1323 | Consistency: 0.0755 | SSIM: 0.8640 | Grad: 85994.984\n",
      "[Epoch 12, Batch 7/100] Loss: 0.2150 | RMSE: 0.1282 | Consistency: 0.0660 | SSIM: 0.8611 | Grad: 94709.828\n",
      "[Epoch 12, Batch 8/100] Loss: 0.2127 | RMSE: 0.1257 | Consistency: 0.0695 | SSIM: 0.8625 | Grad: 75802.938\n",
      "[Epoch 12, Batch 9/100] Loss: 0.2096 | RMSE: 0.1228 | Consistency: 0.0688 | SSIM: 0.8613 | Grad: 52665.785\n",
      "[Epoch 12, Batch 10/100] Loss: 0.2119 | RMSE: 0.1251 | Consistency: 0.0637 | SSIM: 0.8616 | Grad: 96000.625\n",
      "[Epoch 12, Batch 11/100] Loss: 0.2164 | RMSE: 0.1293 | Consistency: 0.0773 | SSIM: 0.8635 | Grad: 103860.719\n",
      "[Epoch 12, Batch 12/100] Loss: 0.2089 | RMSE: 0.1219 | Consistency: 0.0681 | SSIM: 0.8631 | Grad: 51983.258\n",
      "[Epoch 12, Batch 13/100] Loss: 0.2134 | RMSE: 0.1264 | Consistency: 0.0625 | SSIM: 0.8634 | Grad: 92019.203\n",
      "[Epoch 12, Batch 14/100] Loss: 0.2273 | RMSE: 0.1399 | Consistency: 0.0870 | SSIM: 0.8655 | Grad: 113226.555\n",
      "[Epoch 12, Batch 15/100] Loss: 0.2189 | RMSE: 0.1317 | Consistency: 0.0772 | SSIM: 0.8640 | Grad: 83897.977\n",
      "[Epoch 12, Batch 16/100] Loss: 0.2157 | RMSE: 0.1287 | Consistency: 0.0723 | SSIM: 0.8625 | Grad: 54188.516\n",
      "[Epoch 12, Batch 17/100] Loss: 0.2080 | RMSE: 0.1213 | Consistency: 0.0591 | SSIM: 0.8607 | Grad: 61948.641\n",
      "[Epoch 12, Batch 18/100] Loss: 0.2116 | RMSE: 0.1246 | Consistency: 0.0635 | SSIM: 0.8631 | Grad: 77706.203\n",
      "[Epoch 12, Batch 19/100] Loss: 0.2081 | RMSE: 0.1212 | Consistency: 0.0598 | SSIM: 0.8631 | Grad: 53389.160\n",
      "[Epoch 12, Batch 20/100] Loss: 0.2108 | RMSE: 0.1239 | Consistency: 0.0655 | SSIM: 0.8622 | Grad: 82493.750\n",
      "[Epoch 12, Batch 21/100] Loss: 0.2135 | RMSE: 0.1266 | Consistency: 0.0639 | SSIM: 0.8628 | Grad: 42588.770\n",
      "[Epoch 12, Batch 22/100] Loss: 0.2105 | RMSE: 0.1236 | Consistency: 0.0613 | SSIM: 0.8630 | Grad: 68555.258\n",
      "[Epoch 12, Batch 23/100] Loss: 0.2049 | RMSE: 0.1182 | Consistency: 0.0609 | SSIM: 0.8605 | Grad: 35620.781\n",
      "[Epoch 12, Batch 24/100] Loss: 0.2225 | RMSE: 0.1354 | Consistency: 0.0717 | SSIM: 0.8634 | Grad: 129424.008\n",
      "[Epoch 12, Batch 25/100] Loss: 0.2280 | RMSE: 0.1410 | Consistency: 0.0856 | SSIM: 0.8615 | Grad: 150870.125\n",
      "[Epoch 12, Batch 26/100] Loss: 0.2184 | RMSE: 0.1313 | Consistency: 0.0774 | SSIM: 0.8625 | Grad: 98427.867\n",
      "[Epoch 12, Batch 27/100] Loss: 0.2079 | RMSE: 0.1210 | Consistency: 0.0600 | SSIM: 0.8630 | Grad: 57468.586\n",
      "[Epoch 12, Batch 28/100] Loss: 0.2234 | RMSE: 0.1362 | Consistency: 0.0813 | SSIM: 0.8642 | Grad: 100872.555\n",
      "[Epoch 12, Batch 29/100] Loss: 0.2198 | RMSE: 0.1326 | Consistency: 0.0733 | SSIM: 0.8645 | Grad: 102066.750\n",
      "[Epoch 12, Batch 30/100] Loss: 0.2087 | RMSE: 0.1218 | Consistency: 0.0621 | SSIM: 0.8623 | Grad: 68430.320\n",
      "[Epoch 12, Batch 31/100] Loss: 0.2136 | RMSE: 0.1265 | Consistency: 0.0660 | SSIM: 0.8641 | Grad: 62608.668\n",
      "[Epoch 12, Batch 32/100] Loss: 0.2172 | RMSE: 0.1301 | Consistency: 0.0640 | SSIM: 0.8643 | Grad: 96505.383\n",
      "[Epoch 12, Batch 33/100] Loss: 0.2100 | RMSE: 0.1230 | Consistency: 0.0703 | SSIM: 0.8625 | Grad: 76633.078\n",
      "[Epoch 12, Batch 34/100] Loss: 0.2081 | RMSE: 0.1215 | Consistency: 0.0692 | SSIM: 0.8597 | Grad: 60399.902\n",
      "[Epoch 12, Batch 35/100] Loss: 0.2053 | RMSE: 0.1186 | Consistency: 0.0637 | SSIM: 0.8606 | Grad: 63256.062\n",
      "[Epoch 12, Batch 36/100] Loss: 0.2068 | RMSE: 0.1202 | Consistency: 0.0704 | SSIM: 0.8590 | Grad: 78211.578\n",
      "[Epoch 12, Batch 37/100] Loss: 0.2075 | RMSE: 0.1208 | Consistency: 0.0625 | SSIM: 0.8601 | Grad: 54319.191\n",
      "[Epoch 12, Batch 38/100] Loss: 0.2069 | RMSE: 0.1201 | Consistency: 0.0623 | SSIM: 0.8617 | Grad: 29053.650\n",
      "[Epoch 12, Batch 39/100] Loss: 0.2033 | RMSE: 0.1166 | Consistency: 0.0595 | SSIM: 0.8604 | Grad: 29091.570\n",
      "[Epoch 12, Batch 40/100] Loss: 0.2176 | RMSE: 0.1303 | Consistency: 0.0888 | SSIM: 0.8640 | Grad: 110161.695\n",
      "[Epoch 12, Batch 41/100] Loss: 0.2207 | RMSE: 0.1334 | Consistency: 0.0874 | SSIM: 0.8640 | Grad: 101643.094\n",
      "[Epoch 12, Batch 42/100] Loss: 0.2064 | RMSE: 0.1197 | Consistency: 0.0605 | SSIM: 0.8615 | Grad: 17343.678\n",
      "[Epoch 12, Batch 43/100] Loss: 0.2148 | RMSE: 0.1281 | Consistency: 0.0659 | SSIM: 0.8606 | Grad: 102979.070\n",
      "[Epoch 12, Batch 44/100] Loss: 0.2134 | RMSE: 0.1268 | Consistency: 0.0626 | SSIM: 0.8598 | Grad: 92812.273\n",
      "[Epoch 12, Batch 45/100] Loss: 0.2104 | RMSE: 0.1236 | Consistency: 0.0623 | SSIM: 0.8619 | Grad: 36507.359\n",
      "[Epoch 12, Batch 46/100] Loss: 0.2098 | RMSE: 0.1229 | Consistency: 0.0634 | SSIM: 0.8623 | Grad: 23259.672\n",
      "[Epoch 12, Batch 47/100] Loss: 0.2089 | RMSE: 0.1219 | Consistency: 0.0638 | SSIM: 0.8635 | Grad: 54931.508\n",
      "[Epoch 12, Batch 48/100] Loss: 0.2065 | RMSE: 0.1197 | Consistency: 0.0645 | SSIM: 0.8612 | Grad: 49639.773\n",
      "[Epoch 12, Batch 49/100] Loss: 0.2062 | RMSE: 0.1196 | Consistency: 0.0617 | SSIM: 0.8603 | Grad: 34931.824\n",
      "[Epoch 12, Batch 50/100] Loss: 0.2159 | RMSE: 0.1290 | Consistency: 0.0682 | SSIM: 0.8626 | Grad: 84258.742\n",
      "[Epoch 12, Batch 51/100] Loss: 0.2173 | RMSE: 0.1304 | Consistency: 0.0698 | SSIM: 0.8622 | Grad: 85552.297\n",
      "[Epoch 12, Batch 52/100] Loss: 0.2066 | RMSE: 0.1199 | Consistency: 0.0596 | SSIM: 0.8614 | Grad: 43087.816\n",
      "[Epoch 12, Batch 53/100] Loss: 0.2139 | RMSE: 0.1272 | Consistency: 0.0748 | SSIM: 0.8603 | Grad: 109813.562\n",
      "[Epoch 12, Batch 54/100] Loss: 0.2134 | RMSE: 0.1263 | Consistency: 0.0704 | SSIM: 0.8637 | Grad: 108246.539\n",
      "[Epoch 12, Batch 55/100] Loss: 0.2106 | RMSE: 0.1236 | Consistency: 0.0620 | SSIM: 0.8630 | Grad: 66950.008\n",
      "[Epoch 12, Batch 56/100] Loss: 0.2135 | RMSE: 0.1264 | Consistency: 0.0716 | SSIM: 0.8644 | Grad: 38377.223\n",
      "[Epoch 12, Batch 57/100] Loss: 0.2134 | RMSE: 0.1264 | Consistency: 0.0677 | SSIM: 0.8637 | Grad: 75670.297\n",
      "[Epoch 12, Batch 58/100] Loss: 0.2080 | RMSE: 0.1212 | Consistency: 0.0647 | SSIM: 0.8611 | Grad: 28121.932\n",
      "[Epoch 12, Batch 59/100] Loss: 0.2037 | RMSE: 0.1171 | Consistency: 0.0621 | SSIM: 0.8601 | Grad: 61632.980\n",
      "[Epoch 12, Batch 60/100] Loss: 0.2115 | RMSE: 0.1247 | Consistency: 0.0720 | SSIM: 0.8604 | Grad: 91359.219\n",
      "[Epoch 12, Batch 61/100] Loss: 0.2096 | RMSE: 0.1227 | Consistency: 0.0586 | SSIM: 0.8633 | Grad: 38553.234\n",
      "[Epoch 12, Batch 62/100] Loss: 0.2292 | RMSE: 0.1418 | Consistency: 0.0814 | SSIM: 0.8655 | Grad: 109997.828\n",
      "[Epoch 12, Batch 63/100] Loss: 0.2262 | RMSE: 0.1389 | Consistency: 0.0776 | SSIM: 0.8656 | Grad: 90070.078\n",
      "[Epoch 12, Batch 64/100] Loss: 0.2157 | RMSE: 0.1288 | Consistency: 0.0707 | SSIM: 0.8619 | Grad: 88799.219\n",
      "[Epoch 12, Batch 65/100] Loss: 0.2084 | RMSE: 0.1217 | Consistency: 0.0647 | SSIM: 0.8610 | Grad: 53151.891\n",
      "[Epoch 12, Batch 66/100] Loss: 0.2150 | RMSE: 0.1281 | Consistency: 0.0656 | SSIM: 0.8621 | Grad: 82421.703\n",
      "[Epoch 12, Batch 67/100] Loss: 0.2148 | RMSE: 0.1280 | Consistency: 0.0779 | SSIM: 0.8603 | Grad: 114585.781\n",
      "[Epoch 12, Batch 68/100] Loss: 0.2059 | RMSE: 0.1192 | Consistency: 0.0643 | SSIM: 0.8605 | Grad: 49211.938\n",
      "[Epoch 12, Batch 69/100] Loss: 0.2031 | RMSE: 0.1164 | Consistency: 0.0589 | SSIM: 0.8608 | Grad: 51697.918\n",
      "[Epoch 12, Batch 70/100] Loss: 0.2065 | RMSE: 0.1197 | Consistency: 0.0656 | SSIM: 0.8618 | Grad: 64834.133\n",
      "[Epoch 12, Batch 71/100] Loss: 0.2068 | RMSE: 0.1202 | Consistency: 0.0638 | SSIM: 0.8594 | Grad: 39698.418\n",
      "[Epoch 12, Batch 72/100] Loss: 0.2089 | RMSE: 0.1222 | Consistency: 0.0727 | SSIM: 0.8606 | Grad: 94442.891\n",
      "[Epoch 12, Batch 73/100] Loss: 0.2093 | RMSE: 0.1224 | Consistency: 0.0684 | SSIM: 0.8615 | Grad: 94527.047\n",
      "[Epoch 12, Batch 74/100] Loss: 0.2017 | RMSE: 0.1153 | Consistency: 0.0574 | SSIM: 0.8586 | Grad: 13967.591\n",
      "[Epoch 12, Batch 75/100] Loss: 0.2068 | RMSE: 0.1200 | Consistency: 0.0675 | SSIM: 0.8607 | Grad: 62270.441\n",
      "[Epoch 12, Batch 76/100] Loss: 0.2078 | RMSE: 0.1211 | Consistency: 0.0570 | SSIM: 0.8612 | Grad: 69027.492\n",
      "[Epoch 12, Batch 77/100] Loss: 0.2096 | RMSE: 0.1229 | Consistency: 0.0630 | SSIM: 0.8604 | Grad: 49722.910\n",
      "[Epoch 12, Batch 78/100] Loss: 0.2081 | RMSE: 0.1216 | Consistency: 0.0677 | SSIM: 0.8584 | Grad: 76488.023\n",
      "[Epoch 12, Batch 79/100] Loss: 0.2168 | RMSE: 0.1300 | Consistency: 0.0646 | SSIM: 0.8618 | Grad: 90888.484\n",
      "[Epoch 12, Batch 80/100] Loss: 0.2160 | RMSE: 0.1290 | Consistency: 0.0629 | SSIM: 0.8640 | Grad: 77475.586\n",
      "[Epoch 12, Batch 81/100] Loss: 0.2039 | RMSE: 0.1170 | Consistency: 0.0600 | SSIM: 0.8628 | Grad: 47553.723\n",
      "[Epoch 12, Batch 82/100] Loss: 0.2138 | RMSE: 0.1267 | Consistency: 0.0706 | SSIM: 0.8636 | Grad: 69224.227\n",
      "[Epoch 12, Batch 83/100] Loss: 0.2139 | RMSE: 0.1270 | Consistency: 0.0689 | SSIM: 0.8622 | Grad: 78178.461\n",
      "[Epoch 12, Batch 84/100] Loss: 0.2068 | RMSE: 0.1201 | Consistency: 0.0630 | SSIM: 0.8607 | Grad: 35699.957\n",
      "[Epoch 12, Batch 85/100] Loss: 0.2148 | RMSE: 0.1279 | Consistency: 0.0688 | SSIM: 0.8620 | Grad: 90012.656\n",
      "[Epoch 12, Batch 86/100] Loss: 0.2080 | RMSE: 0.1213 | Consistency: 0.0579 | SSIM: 0.8613 | Grad: 67639.500\n",
      "[Epoch 12, Batch 87/100] Loss: 0.2071 | RMSE: 0.1203 | Consistency: 0.0583 | SSIM: 0.8620 | Grad: 36681.801\n",
      "[Epoch 12, Batch 88/100] Loss: 0.2115 | RMSE: 0.1246 | Consistency: 0.0729 | SSIM: 0.8614 | Grad: 74071.719\n",
      "[Epoch 12, Batch 89/100] Loss: 0.2123 | RMSE: 0.1254 | Consistency: 0.0635 | SSIM: 0.8628 | Grad: 67291.195\n",
      "[Epoch 12, Batch 90/100] Loss: 0.2066 | RMSE: 0.1200 | Consistency: 0.0628 | SSIM: 0.8602 | Grad: 43831.188\n",
      "[Epoch 12, Batch 91/100] Loss: 0.2075 | RMSE: 0.1209 | Consistency: 0.0624 | SSIM: 0.8600 | Grad: 42589.352\n",
      "[Epoch 12, Batch 92/100] Loss: 0.2164 | RMSE: 0.1296 | Consistency: 0.0721 | SSIM: 0.8612 | Grad: 82195.383\n",
      "[Epoch 12, Batch 93/100] Loss: 0.2096 | RMSE: 0.1230 | Consistency: 0.0691 | SSIM: 0.8593 | Grad: 50466.766\n",
      "[Epoch 12, Batch 94/100] Loss: 0.2093 | RMSE: 0.1224 | Consistency: 0.0635 | SSIM: 0.8627 | Grad: 34333.898\n",
      "[Epoch 12, Batch 95/100] Loss: 0.2092 | RMSE: 0.1223 | Consistency: 0.0645 | SSIM: 0.8621 | Grad: 35632.734\n",
      "[Epoch 12, Batch 96/100] Loss: 0.2056 | RMSE: 0.1190 | Consistency: 0.0638 | SSIM: 0.8599 | Grad: 88023.750\n",
      "[Epoch 12, Batch 97/100] Loss: 0.2114 | RMSE: 0.1245 | Consistency: 0.0681 | SSIM: 0.8620 | Grad: 83351.352\n",
      "[Epoch 12, Batch 98/100] Loss: 0.2046 | RMSE: 0.1178 | Consistency: 0.0601 | SSIM: 0.8622 | Grad: 54838.211\n",
      "[Epoch 12, Batch 99/100] Loss: 0.2143 | RMSE: 0.1272 | Consistency: 0.0707 | SSIM: 0.8640 | Grad: 70510.102\n",
      "[Epoch 12, Batch 100/100] Loss: 0.2076 | RMSE: 0.1208 | Consistency: 0.0591 | SSIM: 0.8618 | Grad: 64702.590\n",
      "✅ Epoch 12 완료!\n",
      "   📈 평균 Loss: 0.211702\n",
      "   📊 평균 RMSE: 0.124831\n",
      "   ⚙️  학습률: 0.000992\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 13/80 시작...\n",
      "[Epoch 13, Batch 1/100] Loss: 0.2110 | RMSE: 0.1243 | Consistency: 0.0663 | SSIM: 0.8604 | Grad: 76290.633\n",
      "[Epoch 13, Batch 2/100] Loss: 0.2112 | RMSE: 0.1244 | Consistency: 0.0693 | SSIM: 0.8608 | Grad: 73046.266\n",
      "[Epoch 13, Batch 3/100] Loss: 0.2052 | RMSE: 0.1186 | Consistency: 0.0586 | SSIM: 0.8602 | Grad: 35003.738\n",
      "[Epoch 13, Batch 4/100] Loss: 0.2125 | RMSE: 0.1258 | Consistency: 0.0644 | SSIM: 0.8604 | Grad: 87625.680\n",
      "[Epoch 13, Batch 5/100] Loss: 0.2040 | RMSE: 0.1175 | Consistency: 0.0592 | SSIM: 0.8593 | Grad: 48039.910\n",
      "[Epoch 13, Batch 6/100] Loss: 0.2058 | RMSE: 0.1191 | Consistency: 0.0622 | SSIM: 0.8608 | Grad: 48588.242\n",
      "[Epoch 13, Batch 7/100] Loss: 0.2085 | RMSE: 0.1218 | Consistency: 0.0580 | SSIM: 0.8614 | Grad: 68954.266\n",
      "[Epoch 13, Batch 8/100] Loss: 0.2075 | RMSE: 0.1207 | Consistency: 0.0635 | SSIM: 0.8624 | Grad: 73605.023\n",
      "[Epoch 13, Batch 9/100] Loss: 0.2036 | RMSE: 0.1171 | Consistency: 0.0586 | SSIM: 0.8598 | Grad: 53038.449\n",
      "[Epoch 13, Batch 10/100] Loss: 0.2047 | RMSE: 0.1180 | Consistency: 0.0633 | SSIM: 0.8607 | Grad: 63172.391\n",
      "[Epoch 13, Batch 11/100] Loss: 0.2031 | RMSE: 0.1164 | Consistency: 0.0617 | SSIM: 0.8603 | Grad: 52825.664\n",
      "[Epoch 13, Batch 12/100] Loss: 0.2073 | RMSE: 0.1205 | Consistency: 0.0611 | SSIM: 0.8616 | Grad: 21414.988\n",
      "[Epoch 13, Batch 13/100] Loss: 0.2028 | RMSE: 0.1163 | Consistency: 0.0624 | SSIM: 0.8585 | Grad: 56737.477\n",
      "[Epoch 13, Batch 14/100] Loss: 0.2097 | RMSE: 0.1227 | Consistency: 0.0654 | SSIM: 0.8629 | Grad: 73470.781\n",
      "[Epoch 13, Batch 15/100] Loss: 0.2064 | RMSE: 0.1197 | Consistency: 0.0597 | SSIM: 0.8619 | Grad: 32899.160\n",
      "[Epoch 13, Batch 16/100] Loss: 0.2100 | RMSE: 0.1230 | Consistency: 0.0600 | SSIM: 0.8635 | Grad: 66715.211\n",
      "[Epoch 13, Batch 17/100] Loss: 0.2075 | RMSE: 0.1208 | Consistency: 0.0612 | SSIM: 0.8616 | Grad: 70697.180\n",
      "[Epoch 13, Batch 18/100] Loss: 0.2093 | RMSE: 0.1224 | Consistency: 0.0626 | SSIM: 0.8633 | Grad: 41891.129\n",
      "[Epoch 13, Batch 19/100] Loss: 0.2082 | RMSE: 0.1214 | Consistency: 0.0617 | SSIM: 0.8619 | Grad: 51027.426\n",
      "[Epoch 13, Batch 20/100] Loss: 0.2083 | RMSE: 0.1216 | Consistency: 0.0670 | SSIM: 0.8600 | Grad: 75509.328\n",
      "[Epoch 13, Batch 21/100] Loss: 0.2122 | RMSE: 0.1252 | Consistency: 0.0669 | SSIM: 0.8634 | Grad: 32175.264\n",
      "[Epoch 13, Batch 22/100] Loss: 0.2089 | RMSE: 0.1223 | Consistency: 0.0700 | SSIM: 0.8593 | Grad: 80105.906\n",
      "[Epoch 13, Batch 23/100] Loss: 0.2091 | RMSE: 0.1224 | Consistency: 0.0652 | SSIM: 0.8607 | Grad: 59974.688\n",
      "[Epoch 13, Batch 24/100] Loss: 0.2042 | RMSE: 0.1173 | Consistency: 0.0624 | SSIM: 0.8621 | Grad: 30349.908\n",
      "[Epoch 13, Batch 25/100] Loss: 0.2071 | RMSE: 0.1203 | Consistency: 0.0673 | SSIM: 0.8614 | Grad: 59627.578\n",
      "[Epoch 13, Batch 26/100] Loss: 0.2073 | RMSE: 0.1203 | Consistency: 0.0669 | SSIM: 0.8638 | Grad: 56297.633\n",
      "[Epoch 13, Batch 27/100] Loss: 0.2143 | RMSE: 0.1274 | Consistency: 0.0633 | SSIM: 0.8635 | Grad: 69775.484\n",
      "[Epoch 13, Batch 28/100] Loss: 0.2093 | RMSE: 0.1222 | Consistency: 0.0712 | SSIM: 0.8639 | Grad: 82106.500\n",
      "[Epoch 13, Batch 29/100] Loss: 0.2091 | RMSE: 0.1222 | Consistency: 0.0667 | SSIM: 0.8616 | Grad: 45271.594\n",
      "[Epoch 13, Batch 30/100] Loss: 0.2053 | RMSE: 0.1185 | Consistency: 0.0599 | SSIM: 0.8623 | Grad: 25876.834\n",
      "[Epoch 13, Batch 31/100] Loss: 0.2191 | RMSE: 0.1322 | Consistency: 0.0749 | SSIM: 0.8617 | Grad: 114461.508\n",
      "[Epoch 13, Batch 32/100] Loss: 0.2126 | RMSE: 0.1257 | Consistency: 0.0655 | SSIM: 0.8626 | Grad: 76557.539\n",
      "[Epoch 13, Batch 33/100] Loss: 0.2109 | RMSE: 0.1239 | Consistency: 0.0658 | SSIM: 0.8625 | Grad: 45753.223\n",
      "[Epoch 13, Batch 34/100] Loss: 0.2260 | RMSE: 0.1391 | Consistency: 0.0719 | SSIM: 0.8620 | Grad: 138136.797\n",
      "[Epoch 13, Batch 35/100] Loss: 0.2196 | RMSE: 0.1328 | Consistency: 0.0716 | SSIM: 0.8610 | Grad: 132241.859\n",
      "[Epoch 13, Batch 36/100] Loss: 0.2090 | RMSE: 0.1224 | Consistency: 0.0616 | SSIM: 0.8602 | Grad: 105723.719\n",
      "[Epoch 13, Batch 37/100] Loss: 0.2046 | RMSE: 0.1180 | Consistency: 0.0603 | SSIM: 0.8602 | Grad: 73740.141\n",
      "[Epoch 13, Batch 38/100] Loss: 0.2095 | RMSE: 0.1228 | Consistency: 0.0621 | SSIM: 0.8607 | Grad: 88804.312\n",
      "[Epoch 13, Batch 39/100] Loss: 0.2038 | RMSE: 0.1172 | Consistency: 0.0598 | SSIM: 0.8604 | Grad: 44480.387\n",
      "[Epoch 13, Batch 40/100] Loss: 0.2104 | RMSE: 0.1237 | Consistency: 0.0733 | SSIM: 0.8599 | Grad: 66871.430\n",
      "[Epoch 13, Batch 41/100] Loss: 0.2031 | RMSE: 0.1165 | Consistency: 0.0605 | SSIM: 0.8599 | Grad: 9018.177\n",
      "[Epoch 13, Batch 42/100] Loss: 0.2068 | RMSE: 0.1201 | Consistency: 0.0614 | SSIM: 0.8611 | Grad: 53622.516\n",
      "[Epoch 13, Batch 43/100] Loss: 0.2089 | RMSE: 0.1220 | Consistency: 0.0697 | SSIM: 0.8615 | Grad: 42862.707\n",
      "[Epoch 13, Batch 44/100] Loss: 0.2076 | RMSE: 0.1208 | Consistency: 0.0645 | SSIM: 0.8610 | Grad: 44818.961\n",
      "[Epoch 13, Batch 45/100] Loss: 0.2111 | RMSE: 0.1241 | Consistency: 0.0602 | SSIM: 0.8635 | Grad: 62383.852\n",
      "[Epoch 13, Batch 46/100] Loss: 0.2108 | RMSE: 0.1239 | Consistency: 0.0633 | SSIM: 0.8626 | Grad: 67163.906\n",
      "[Epoch 13, Batch 47/100] Loss: 0.2078 | RMSE: 0.1211 | Consistency: 0.0598 | SSIM: 0.8619 | Grad: 42272.836\n",
      "[Epoch 13, Batch 48/100] Loss: 0.2091 | RMSE: 0.1222 | Consistency: 0.0611 | SSIM: 0.8626 | Grad: 36613.297\n",
      "[Epoch 13, Batch 49/100] Loss: 0.2107 | RMSE: 0.1237 | Consistency: 0.0697 | SSIM: 0.8632 | Grad: 73311.102\n",
      "[Epoch 13, Batch 50/100] Loss: 0.2221 | RMSE: 0.1351 | Consistency: 0.0699 | SSIM: 0.8635 | Grad: 114869.906\n",
      "[Epoch 13, Batch 51/100] Loss: 0.2048 | RMSE: 0.1182 | Consistency: 0.0609 | SSIM: 0.8600 | Grad: 54015.969\n",
      "[Epoch 13, Batch 52/100] Loss: 0.2019 | RMSE: 0.1155 | Consistency: 0.0580 | SSIM: 0.8587 | Grad: 25277.580\n",
      "[Epoch 13, Batch 53/100] Loss: 0.2134 | RMSE: 0.1267 | Consistency: 0.0642 | SSIM: 0.8603 | Grad: 94874.586\n",
      "[Epoch 13, Batch 54/100] Loss: 0.2181 | RMSE: 0.1311 | Consistency: 0.0712 | SSIM: 0.8627 | Grad: 95559.617\n",
      "[Epoch 13, Batch 55/100] Loss: 0.2136 | RMSE: 0.1267 | Consistency: 0.0694 | SSIM: 0.8622 | Grad: 75800.523\n",
      "[Epoch 13, Batch 56/100] Loss: 0.2136 | RMSE: 0.1267 | Consistency: 0.0685 | SSIM: 0.8623 | Grad: 67885.453\n",
      "[Epoch 13, Batch 57/100] Loss: 0.2147 | RMSE: 0.1279 | Consistency: 0.0726 | SSIM: 0.8611 | Grad: 115057.102\n",
      "[Epoch 13, Batch 58/100] Loss: 0.2141 | RMSE: 0.1272 | Consistency: 0.0626 | SSIM: 0.8629 | Grad: 80420.773\n",
      "[Epoch 13, Batch 59/100] Loss: 0.2065 | RMSE: 0.1197 | Consistency: 0.0601 | SSIM: 0.8618 | Grad: 54882.254\n",
      "[Epoch 13, Batch 60/100] Loss: 0.2047 | RMSE: 0.1178 | Consistency: 0.0593 | SSIM: 0.8632 | Grad: 34523.176\n",
      "[Epoch 13, Batch 61/100] Loss: 0.2102 | RMSE: 0.1232 | Consistency: 0.0688 | SSIM: 0.8625 | Grad: 57263.641\n",
      "[Epoch 13, Batch 62/100] Loss: 0.2038 | RMSE: 0.1171 | Consistency: 0.0578 | SSIM: 0.8608 | Grad: 36281.812\n",
      "[Epoch 13, Batch 63/100] Loss: 0.2184 | RMSE: 0.1313 | Consistency: 0.0766 | SSIM: 0.8638 | Grad: 51510.348\n",
      "[Epoch 13, Batch 64/100] Loss: 0.2089 | RMSE: 0.1220 | Consistency: 0.0706 | SSIM: 0.8614 | Grad: 81299.352\n",
      "[Epoch 13, Batch 65/100] Loss: 0.2124 | RMSE: 0.1254 | Consistency: 0.0667 | SSIM: 0.8636 | Grad: 88939.266\n",
      "[Epoch 13, Batch 66/100] Loss: 0.2054 | RMSE: 0.1185 | Consistency: 0.0601 | SSIM: 0.8625 | Grad: 18621.873\n",
      "[Epoch 13, Batch 67/100] Loss: 0.2072 | RMSE: 0.1206 | Consistency: 0.0659 | SSIM: 0.8600 | Grad: 64987.395\n",
      "[Epoch 13, Batch 68/100] Loss: 0.2110 | RMSE: 0.1243 | Consistency: 0.0659 | SSIM: 0.8612 | Grad: 48568.371\n",
      "[Epoch 13, Batch 69/100] Loss: 0.2059 | RMSE: 0.1192 | Consistency: 0.0587 | SSIM: 0.8615 | Grad: 65648.297\n",
      "[Epoch 13, Batch 70/100] Loss: 0.2096 | RMSE: 0.1228 | Consistency: 0.0675 | SSIM: 0.8616 | Grad: 70294.344\n",
      "[Epoch 13, Batch 71/100] Loss: 0.2084 | RMSE: 0.1215 | Consistency: 0.0668 | SSIM: 0.8628 | Grad: 46884.043\n",
      "[Epoch 13, Batch 72/100] Loss: 0.2063 | RMSE: 0.1194 | Consistency: 0.0630 | SSIM: 0.8623 | Grad: 55662.762\n",
      "[Epoch 13, Batch 73/100] Loss: 0.2035 | RMSE: 0.1168 | Consistency: 0.0570 | SSIM: 0.8611 | Grad: 28497.906\n",
      "[Epoch 13, Batch 74/100] Loss: 0.2111 | RMSE: 0.1242 | Consistency: 0.0636 | SSIM: 0.8618 | Grad: 84976.586\n",
      "[Epoch 13, Batch 75/100] Loss: 0.2154 | RMSE: 0.1284 | Consistency: 0.0683 | SSIM: 0.8640 | Grad: 69280.469\n",
      "[Epoch 13, Batch 76/100] Loss: 0.2063 | RMSE: 0.1198 | Consistency: 0.0610 | SSIM: 0.8588 | Grad: 17376.445\n",
      "[Epoch 13, Batch 77/100] Loss: 0.2162 | RMSE: 0.1294 | Consistency: 0.0684 | SSIM: 0.8610 | Grad: 82502.023\n",
      "[Epoch 13, Batch 78/100] Loss: 0.2090 | RMSE: 0.1224 | Consistency: 0.0635 | SSIM: 0.8591 | Grad: 71218.852\n",
      "[Epoch 13, Batch 79/100] Loss: 0.2069 | RMSE: 0.1203 | Consistency: 0.0636 | SSIM: 0.8600 | Grad: 44492.359\n",
      "[Epoch 13, Batch 80/100] Loss: 0.2103 | RMSE: 0.1235 | Consistency: 0.0665 | SSIM: 0.8614 | Grad: 75950.195\n",
      "[Epoch 13, Batch 81/100] Loss: 0.2150 | RMSE: 0.1281 | Consistency: 0.0680 | SSIM: 0.8620 | Grad: 89750.125\n",
      "[Epoch 13, Batch 82/100] Loss: 0.2074 | RMSE: 0.1205 | Consistency: 0.0635 | SSIM: 0.8636 | Grad: 53161.723\n",
      "[Epoch 13, Batch 83/100] Loss: 0.2063 | RMSE: 0.1195 | Consistency: 0.0543 | SSIM: 0.8630 | Grad: 64104.520\n",
      "[Epoch 13, Batch 84/100] Loss: 0.2212 | RMSE: 0.1339 | Consistency: 0.0871 | SSIM: 0.8642 | Grad: 93316.633\n",
      "[Epoch 13, Batch 85/100] Loss: 0.2122 | RMSE: 0.1253 | Consistency: 0.0699 | SSIM: 0.8625 | Grad: 78287.039\n",
      "[Epoch 13, Batch 86/100] Loss: 0.2119 | RMSE: 0.1251 | Consistency: 0.0593 | SSIM: 0.8617 | Grad: 61988.621\n",
      "[Epoch 13, Batch 87/100] Loss: 0.2166 | RMSE: 0.1298 | Consistency: 0.0877 | SSIM: 0.8600 | Grad: 106814.062\n",
      "[Epoch 13, Batch 88/100] Loss: 0.2124 | RMSE: 0.1257 | Consistency: 0.0739 | SSIM: 0.8595 | Grad: 91336.430\n",
      "[Epoch 13, Batch 89/100] Loss: 0.2068 | RMSE: 0.1202 | Consistency: 0.0591 | SSIM: 0.8607 | Grad: 81070.250\n",
      "[Epoch 13, Batch 90/100] Loss: 0.2089 | RMSE: 0.1220 | Consistency: 0.0623 | SSIM: 0.8623 | Grad: 70400.203\n",
      "[Epoch 13, Batch 91/100] Loss: 0.2095 | RMSE: 0.1225 | Consistency: 0.0710 | SSIM: 0.8624 | Grad: 75984.516\n",
      "[Epoch 13, Batch 92/100] Loss: 0.2055 | RMSE: 0.1186 | Consistency: 0.0646 | SSIM: 0.8624 | Grad: 32204.273\n",
      "[Epoch 13, Batch 93/100] Loss: 0.2063 | RMSE: 0.1194 | Consistency: 0.0640 | SSIM: 0.8625 | Grad: 58113.500\n",
      "[Epoch 13, Batch 94/100] Loss: 0.2073 | RMSE: 0.1206 | Consistency: 0.0624 | SSIM: 0.8608 | Grad: 48299.992\n",
      "[Epoch 13, Batch 95/100] Loss: 0.2081 | RMSE: 0.1214 | Consistency: 0.0602 | SSIM: 0.8610 | Grad: 56740.477\n",
      "[Epoch 13, Batch 96/100] Loss: 0.2151 | RMSE: 0.1281 | Consistency: 0.0666 | SSIM: 0.8631 | Grad: 46665.152\n",
      "[Epoch 13, Batch 97/100] Loss: 0.2132 | RMSE: 0.1262 | Consistency: 0.0653 | SSIM: 0.8636 | Grad: 70336.500\n",
      "[Epoch 13, Batch 98/100] Loss: 0.2118 | RMSE: 0.1251 | Consistency: 0.0598 | SSIM: 0.8616 | Grad: 73108.844\n",
      "[Epoch 13, Batch 99/100] Loss: 0.2067 | RMSE: 0.1201 | Consistency: 0.0605 | SSIM: 0.8604 | Grad: 71164.367\n",
      "[Epoch 13, Batch 100/100] Loss: 0.2168 | RMSE: 0.1298 | Consistency: 0.0688 | SSIM: 0.8629 | Grad: 98336.039\n",
      "✅ Epoch 13 완료!\n",
      "   📈 평균 Loss: 0.209680\n",
      "   📊 평균 RMSE: 0.122868\n",
      "   ⚙️  학습률: 0.000988\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 14/80 시작...\n",
      "[Epoch 14, Batch 1/100] Loss: 0.2125 | RMSE: 0.1258 | Consistency: 0.0586 | SSIM: 0.8614 | Grad: 78905.539\n",
      "[Epoch 14, Batch 2/100] Loss: 0.2106 | RMSE: 0.1241 | Consistency: 0.0642 | SSIM: 0.8593 | Grad: 98052.430\n",
      "[Epoch 14, Batch 3/100] Loss: 0.2070 | RMSE: 0.1203 | Consistency: 0.0620 | SSIM: 0.8608 | Grad: 43482.930\n",
      "[Epoch 14, Batch 4/100] Loss: 0.2116 | RMSE: 0.1248 | Consistency: 0.0674 | SSIM: 0.8610 | Grad: 50768.660\n",
      "[Epoch 14, Batch 5/100] Loss: 0.2059 | RMSE: 0.1193 | Consistency: 0.0585 | SSIM: 0.8607 | Grad: 55576.949\n",
      "[Epoch 14, Batch 6/100] Loss: 0.2131 | RMSE: 0.1261 | Consistency: 0.0660 | SSIM: 0.8633 | Grad: 74046.656\n",
      "[Epoch 14, Batch 7/100] Loss: 0.2066 | RMSE: 0.1200 | Consistency: 0.0622 | SSIM: 0.8596 | Grad: 58433.965\n",
      "[Epoch 14, Batch 8/100] Loss: 0.2057 | RMSE: 0.1189 | Consistency: 0.0580 | SSIM: 0.8614 | Grad: 42095.664\n",
      "[Epoch 14, Batch 9/100] Loss: 0.2031 | RMSE: 0.1165 | Consistency: 0.0559 | SSIM: 0.8603 | Grad: 38795.211\n",
      "[Epoch 14, Batch 10/100] Loss: 0.2042 | RMSE: 0.1174 | Consistency: 0.0609 | SSIM: 0.8620 | Grad: 50131.977\n",
      "[Epoch 14, Batch 11/100] Loss: 0.2018 | RMSE: 0.1152 | Consistency: 0.0590 | SSIM: 0.8605 | Grad: 32775.453\n",
      "[Epoch 14, Batch 12/100] Loss: 0.2010 | RMSE: 0.1142 | Consistency: 0.0563 | SSIM: 0.8620 | Grad: 15276.798\n",
      "[Epoch 14, Batch 13/100] Loss: 0.2131 | RMSE: 0.1261 | Consistency: 0.0648 | SSIM: 0.8632 | Grad: 79886.945\n",
      "[Epoch 14, Batch 14/100] Loss: 0.2117 | RMSE: 0.1249 | Consistency: 0.0588 | SSIM: 0.8615 | Grad: 64974.875\n",
      "[Epoch 14, Batch 15/100] Loss: 0.2069 | RMSE: 0.1203 | Consistency: 0.0623 | SSIM: 0.8597 | Grad: 68981.578\n",
      "[Epoch 14, Batch 16/100] Loss: 0.2083 | RMSE: 0.1216 | Consistency: 0.0644 | SSIM: 0.8603 | Grad: 58955.211\n",
      "[Epoch 14, Batch 17/100] Loss: 0.2083 | RMSE: 0.1215 | Consistency: 0.0596 | SSIM: 0.8616 | Grad: 62435.230\n",
      "[Epoch 14, Batch 18/100] Loss: 0.2045 | RMSE: 0.1181 | Consistency: 0.0599 | SSIM: 0.8579 | Grad: 59747.109\n",
      "[Epoch 14, Batch 19/100] Loss: 0.2047 | RMSE: 0.1182 | Consistency: 0.0602 | SSIM: 0.8588 | Grad: 51294.320\n",
      "[Epoch 14, Batch 20/100] Loss: 0.2135 | RMSE: 0.1265 | Consistency: 0.0669 | SSIM: 0.8626 | Grad: 66087.836\n",
      "[Epoch 14, Batch 21/100] Loss: 0.2127 | RMSE: 0.1258 | Consistency: 0.0674 | SSIM: 0.8625 | Grad: 84456.680\n",
      "[Epoch 14, Batch 22/100] Loss: 0.2080 | RMSE: 0.1212 | Consistency: 0.0598 | SSIM: 0.8612 | Grad: 56234.613\n",
      "[Epoch 14, Batch 23/100] Loss: 0.2124 | RMSE: 0.1254 | Consistency: 0.0674 | SSIM: 0.8625 | Grad: 96039.828\n",
      "[Epoch 14, Batch 24/100] Loss: 0.2080 | RMSE: 0.1214 | Consistency: 0.0700 | SSIM: 0.8594 | Grad: 94691.586\n",
      "[Epoch 14, Batch 25/100] Loss: 0.2056 | RMSE: 0.1189 | Consistency: 0.0596 | SSIM: 0.8606 | Grad: 78142.984\n",
      "[Epoch 14, Batch 26/100] Loss: 0.2190 | RMSE: 0.1319 | Consistency: 0.0678 | SSIM: 0.8649 | Grad: 91336.086\n",
      "[Epoch 14, Batch 27/100] Loss: 0.2147 | RMSE: 0.1279 | Consistency: 0.0707 | SSIM: 0.8610 | Grad: 76031.875\n",
      "[Epoch 14, Batch 28/100] Loss: 0.2161 | RMSE: 0.1292 | Consistency: 0.0640 | SSIM: 0.8626 | Grad: 95645.469\n",
      "[Epoch 14, Batch 29/100] Loss: 0.2065 | RMSE: 0.1200 | Consistency: 0.0605 | SSIM: 0.8593 | Grad: 46037.559\n",
      "[Epoch 14, Batch 30/100] Loss: 0.2066 | RMSE: 0.1198 | Consistency: 0.0636 | SSIM: 0.8610 | Grad: 34573.891\n",
      "[Epoch 14, Batch 31/100] Loss: 0.2152 | RMSE: 0.1284 | Consistency: 0.0597 | SSIM: 0.8620 | Grad: 115185.641\n",
      "[Epoch 14, Batch 32/100] Loss: 0.2095 | RMSE: 0.1230 | Consistency: 0.0620 | SSIM: 0.8591 | Grad: 96588.000\n",
      "[Epoch 14, Batch 33/100] Loss: 0.2153 | RMSE: 0.1281 | Consistency: 0.0754 | SSIM: 0.8641 | Grad: 72096.992\n",
      "[Epoch 14, Batch 34/100] Loss: 0.2146 | RMSE: 0.1274 | Consistency: 0.0720 | SSIM: 0.8655 | Grad: 71258.180\n",
      "[Epoch 14, Batch 35/100] Loss: 0.2141 | RMSE: 0.1271 | Consistency: 0.0692 | SSIM: 0.8629 | Grad: 97634.258\n",
      "[Epoch 14, Batch 36/100] Loss: 0.2141 | RMSE: 0.1272 | Consistency: 0.0697 | SSIM: 0.8623 | Grad: 74097.047\n",
      "[Epoch 14, Batch 37/100] Loss: 0.2148 | RMSE: 0.1277 | Consistency: 0.0719 | SSIM: 0.8636 | Grad: 112477.219\n",
      "[Epoch 14, Batch 38/100] Loss: 0.2139 | RMSE: 0.1267 | Consistency: 0.0631 | SSIM: 0.8651 | Grad: 86869.250\n",
      "[Epoch 14, Batch 39/100] Loss: 0.2086 | RMSE: 0.1217 | Consistency: 0.0648 | SSIM: 0.8621 | Grad: 68987.945\n",
      "[Epoch 14, Batch 40/100] Loss: 0.2084 | RMSE: 0.1218 | Consistency: 0.0619 | SSIM: 0.8598 | Grad: 69238.969\n",
      "[Epoch 14, Batch 41/100] Loss: 0.2105 | RMSE: 0.1239 | Consistency: 0.0643 | SSIM: 0.8592 | Grad: 70962.609\n",
      "[Epoch 14, Batch 42/100] Loss: 0.2092 | RMSE: 0.1224 | Consistency: 0.0646 | SSIM: 0.8613 | Grad: 66616.609\n",
      "[Epoch 14, Batch 43/100] Loss: 0.2083 | RMSE: 0.1216 | Consistency: 0.0651 | SSIM: 0.8604 | Grad: 67882.648\n",
      "[Epoch 14, Batch 44/100] Loss: 0.2065 | RMSE: 0.1201 | Consistency: 0.0652 | SSIM: 0.8583 | Grad: 61867.867\n",
      "[Epoch 14, Batch 45/100] Loss: 0.2121 | RMSE: 0.1253 | Consistency: 0.0624 | SSIM: 0.8614 | Grad: 88915.297\n",
      "[Epoch 14, Batch 46/100] Loss: 0.2095 | RMSE: 0.1226 | Consistency: 0.0651 | SSIM: 0.8623 | Grad: 76567.805\n",
      "[Epoch 14, Batch 47/100] Loss: 0.2093 | RMSE: 0.1224 | Consistency: 0.0644 | SSIM: 0.8620 | Grad: 80555.711\n",
      "[Epoch 14, Batch 48/100] Loss: 0.2105 | RMSE: 0.1235 | Consistency: 0.0630 | SSIM: 0.8635 | Grad: 73370.609\n",
      "[Epoch 14, Batch 49/100] Loss: 0.2067 | RMSE: 0.1200 | Consistency: 0.0618 | SSIM: 0.8609 | Grad: 58255.977\n",
      "[Epoch 14, Batch 50/100] Loss: 0.2107 | RMSE: 0.1241 | Consistency: 0.0651 | SSIM: 0.8600 | Grad: 79691.703\n",
      "[Epoch 14, Batch 51/100] Loss: 0.2050 | RMSE: 0.1185 | Consistency: 0.0607 | SSIM: 0.8581 | Grad: 46423.242\n",
      "[Epoch 14, Batch 52/100] Loss: 0.2071 | RMSE: 0.1204 | Consistency: 0.0636 | SSIM: 0.8611 | Grad: 46312.500\n",
      "[Epoch 14, Batch 53/100] Loss: 0.2094 | RMSE: 0.1226 | Consistency: 0.0663 | SSIM: 0.8616 | Grad: 31481.479\n",
      "[Epoch 14, Batch 54/100] Loss: 0.2030 | RMSE: 0.1164 | Consistency: 0.0567 | SSIM: 0.8609 | Grad: 57987.145\n",
      "[Epoch 14, Batch 55/100] Loss: 0.2111 | RMSE: 0.1241 | Consistency: 0.0697 | SSIM: 0.8631 | Grad: 94327.453\n",
      "[Epoch 14, Batch 56/100] Loss: 0.2199 | RMSE: 0.1328 | Consistency: 0.0679 | SSIM: 0.8644 | Grad: 125362.891\n",
      "[Epoch 14, Batch 57/100] Loss: 0.2160 | RMSE: 0.1291 | Consistency: 0.0622 | SSIM: 0.8626 | Grad: 81066.953\n",
      "[Epoch 14, Batch 58/100] Loss: 0.2092 | RMSE: 0.1224 | Consistency: 0.0633 | SSIM: 0.8624 | Grad: 64711.371\n",
      "[Epoch 14, Batch 59/100] Loss: 0.2020 | RMSE: 0.1154 | Consistency: 0.0580 | SSIM: 0.8600 | Grad: 52730.434\n",
      "[Epoch 14, Batch 60/100] Loss: 0.2074 | RMSE: 0.1205 | Consistency: 0.0596 | SSIM: 0.8622 | Grad: 32319.881\n",
      "[Epoch 14, Batch 61/100] Loss: 0.2036 | RMSE: 0.1169 | Consistency: 0.0603 | SSIM: 0.8606 | Grad: 54804.621\n",
      "[Epoch 14, Batch 62/100] Loss: 0.2066 | RMSE: 0.1200 | Consistency: 0.0597 | SSIM: 0.8598 | Grad: 76909.812\n",
      "[Epoch 14, Batch 63/100] Loss: 0.2017 | RMSE: 0.1151 | Consistency: 0.0575 | SSIM: 0.8601 | Grad: 27310.510\n",
      "[Epoch 14, Batch 64/100] Loss: 0.2091 | RMSE: 0.1223 | Consistency: 0.0617 | SSIM: 0.8624 | Grad: 37705.941\n",
      "[Epoch 14, Batch 65/100] Loss: 0.2232 | RMSE: 0.1360 | Consistency: 0.0711 | SSIM: 0.8644 | Grad: 101718.148\n",
      "[Epoch 14, Batch 66/100] Loss: 0.2191 | RMSE: 0.1322 | Consistency: 0.0651 | SSIM: 0.8624 | Grad: 110316.109\n",
      "[Epoch 14, Batch 67/100] Loss: 0.2104 | RMSE: 0.1237 | Consistency: 0.0695 | SSIM: 0.8604 | Grad: 75199.445\n",
      "[Epoch 14, Batch 68/100] Loss: 0.2069 | RMSE: 0.1203 | Consistency: 0.0658 | SSIM: 0.8597 | Grad: 56152.164\n",
      "[Epoch 14, Batch 69/100] Loss: 0.2150 | RMSE: 0.1282 | Consistency: 0.0614 | SSIM: 0.8624 | Grad: 113316.078\n",
      "[Epoch 14, Batch 70/100] Loss: 0.2253 | RMSE: 0.1382 | Consistency: 0.0660 | SSIM: 0.8645 | Grad: 137392.562\n",
      "[Epoch 14, Batch 71/100] Loss: 0.2058 | RMSE: 0.1191 | Consistency: 0.0583 | SSIM: 0.8614 | Grad: 51478.766\n",
      "[Epoch 14, Batch 72/100] Loss: 0.2118 | RMSE: 0.1249 | Consistency: 0.0632 | SSIM: 0.8620 | Grad: 52688.691\n",
      "[Epoch 14, Batch 73/100] Loss: 0.2031 | RMSE: 0.1166 | Consistency: 0.0568 | SSIM: 0.8600 | Grad: 60634.590\n",
      "[Epoch 14, Batch 74/100] Loss: 0.2044 | RMSE: 0.1178 | Consistency: 0.0618 | SSIM: 0.8600 | Grad: 52005.348\n",
      "[Epoch 14, Batch 75/100] Loss: 0.2097 | RMSE: 0.1229 | Consistency: 0.0637 | SSIM: 0.8615 | Grad: 60830.547\n",
      "[Epoch 14, Batch 76/100] Loss: 0.2043 | RMSE: 0.1181 | Consistency: 0.0604 | SSIM: 0.8556 | Grad: 65283.227\n",
      "[Epoch 14, Batch 77/100] Loss: 0.2039 | RMSE: 0.1173 | Consistency: 0.0580 | SSIM: 0.8601 | Grad: 11465.627\n",
      "[Epoch 14, Batch 78/100] Loss: 0.2019 | RMSE: 0.1155 | Consistency: 0.0563 | SSIM: 0.8588 | Grad: 38638.742\n",
      "[Epoch 14, Batch 79/100] Loss: 0.2091 | RMSE: 0.1224 | Consistency: 0.0652 | SSIM: 0.8608 | Grad: 58432.051\n",
      "[Epoch 14, Batch 80/100] Loss: 0.2040 | RMSE: 0.1173 | Consistency: 0.0622 | SSIM: 0.8615 | Grad: 48383.418\n",
      "[Epoch 14, Batch 81/100] Loss: 0.2060 | RMSE: 0.1194 | Consistency: 0.0600 | SSIM: 0.8602 | Grad: 43416.562\n",
      "[Epoch 14, Batch 82/100] Loss: 0.2089 | RMSE: 0.1221 | Consistency: 0.0699 | SSIM: 0.8607 | Grad: 69422.602\n",
      "[Epoch 14, Batch 83/100] Loss: 0.2074 | RMSE: 0.1206 | Consistency: 0.0655 | SSIM: 0.8607 | Grad: 56630.203\n",
      "[Epoch 14, Batch 84/100] Loss: 0.2059 | RMSE: 0.1190 | Consistency: 0.0635 | SSIM: 0.8618 | Grad: 46891.539\n",
      "[Epoch 14, Batch 85/100] Loss: 0.2083 | RMSE: 0.1217 | Consistency: 0.0615 | SSIM: 0.8608 | Grad: 61330.352\n",
      "[Epoch 14, Batch 86/100] Loss: 0.2041 | RMSE: 0.1174 | Consistency: 0.0626 | SSIM: 0.8608 | Grad: 36673.840\n",
      "[Epoch 14, Batch 87/100] Loss: 0.2031 | RMSE: 0.1169 | Consistency: 0.0569 | SSIM: 0.8569 | Grad: 66019.562\n",
      "[Epoch 14, Batch 88/100] Loss: 0.2026 | RMSE: 0.1159 | Consistency: 0.0575 | SSIM: 0.8614 | Grad: 51026.891\n",
      "[Epoch 14, Batch 89/100] Loss: 0.2040 | RMSE: 0.1172 | Consistency: 0.0603 | SSIM: 0.8618 | Grad: 44262.176\n",
      "[Epoch 14, Batch 90/100] Loss: 0.2138 | RMSE: 0.1269 | Consistency: 0.0603 | SSIM: 0.8627 | Grad: 85415.227\n",
      "[Epoch 14, Batch 91/100] Loss: 0.2147 | RMSE: 0.1278 | Consistency: 0.0677 | SSIM: 0.8624 | Grad: 72647.523\n",
      "[Epoch 14, Batch 92/100] Loss: 0.2084 | RMSE: 0.1218 | Consistency: 0.0599 | SSIM: 0.8600 | Grad: 81093.578\n",
      "[Epoch 14, Batch 93/100] Loss: 0.2101 | RMSE: 0.1234 | Consistency: 0.0607 | SSIM: 0.8611 | Grad: 44742.090\n",
      "[Epoch 14, Batch 94/100] Loss: 0.2106 | RMSE: 0.1238 | Consistency: 0.0622 | SSIM: 0.8613 | Grad: 88251.219\n",
      "[Epoch 14, Batch 95/100] Loss: 0.2159 | RMSE: 0.1290 | Consistency: 0.0624 | SSIM: 0.8623 | Grad: 117453.445\n",
      "[Epoch 14, Batch 96/100] Loss: 0.2064 | RMSE: 0.1198 | Consistency: 0.0598 | SSIM: 0.8594 | Grad: 77754.656\n",
      "[Epoch 14, Batch 97/100] Loss: 0.2014 | RMSE: 0.1148 | Consistency: 0.0565 | SSIM: 0.8609 | Grad: 23847.676\n",
      "[Epoch 14, Batch 98/100] Loss: 0.2109 | RMSE: 0.1243 | Consistency: 0.0599 | SSIM: 0.8601 | Grad: 66555.609\n",
      "[Epoch 14, Batch 99/100] Loss: 0.2024 | RMSE: 0.1158 | Consistency: 0.0578 | SSIM: 0.8594 | Grad: 36829.480\n",
      "[Epoch 14, Batch 100/100] Loss: 0.2061 | RMSE: 0.1194 | Consistency: 0.0609 | SSIM: 0.8613 | Grad: 52599.773\n",
      "✅ Epoch 14 완료!\n",
      "   📈 평균 Loss: 0.209051\n",
      "   📊 평균 RMSE: 0.122303\n",
      "   ⚙️  학습률: 0.000983\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 15/80 시작...\n",
      "[Epoch 15, Batch 1/100] Loss: 0.2072 | RMSE: 0.1205 | Consistency: 0.0613 | SSIM: 0.8609 | Grad: 37411.531\n",
      "[Epoch 15, Batch 2/100] Loss: 0.2029 | RMSE: 0.1162 | Consistency: 0.0577 | SSIM: 0.8611 | Grad: 55253.105\n",
      "[Epoch 15, Batch 3/100] Loss: 0.2038 | RMSE: 0.1171 | Consistency: 0.0595 | SSIM: 0.8617 | Grad: 44848.801\n",
      "[Epoch 15, Batch 4/100] Loss: 0.2061 | RMSE: 0.1197 | Consistency: 0.0591 | SSIM: 0.8584 | Grad: 57237.395\n",
      "[Epoch 15, Batch 5/100] Loss: 0.2084 | RMSE: 0.1219 | Consistency: 0.0635 | SSIM: 0.8593 | Grad: 87783.883\n",
      "[Epoch 15, Batch 6/100] Loss: 0.2108 | RMSE: 0.1243 | Consistency: 0.0574 | SSIM: 0.8592 | Grad: 101318.992\n",
      "[Epoch 15, Batch 7/100] Loss: 0.2055 | RMSE: 0.1190 | Consistency: 0.0576 | SSIM: 0.8588 | Grad: 79200.828\n",
      "[Epoch 15, Batch 8/100] Loss: 0.2117 | RMSE: 0.1249 | Consistency: 0.0727 | SSIM: 0.8605 | Grad: 79083.617\n",
      "[Epoch 15, Batch 9/100] Loss: 0.2056 | RMSE: 0.1190 | Consistency: 0.0590 | SSIM: 0.8602 | Grad: 60560.367\n",
      "[Epoch 15, Batch 10/100] Loss: 0.2046 | RMSE: 0.1179 | Consistency: 0.0602 | SSIM: 0.8615 | Grad: 31886.312\n",
      "[Epoch 15, Batch 11/100] Loss: 0.2014 | RMSE: 0.1148 | Consistency: 0.0537 | SSIM: 0.8605 | Grad: 30253.793\n",
      "[Epoch 15, Batch 12/100] Loss: 0.2201 | RMSE: 0.1332 | Consistency: 0.0774 | SSIM: 0.8606 | Grad: 104532.922\n",
      "[Epoch 15, Batch 13/100] Loss: 0.2086 | RMSE: 0.1219 | Consistency: 0.0651 | SSIM: 0.8606 | Grad: 79773.734\n",
      "[Epoch 15, Batch 14/100] Loss: 0.2147 | RMSE: 0.1277 | Consistency: 0.0695 | SSIM: 0.8633 | Grad: 88441.305\n",
      "[Epoch 15, Batch 15/100] Loss: 0.2101 | RMSE: 0.1232 | Consistency: 0.0673 | SSIM: 0.8624 | Grad: 65129.176\n",
      "[Epoch 15, Batch 16/100] Loss: 0.2071 | RMSE: 0.1203 | Consistency: 0.0652 | SSIM: 0.8611 | Grad: 62375.582\n",
      "[Epoch 15, Batch 17/100] Loss: 0.2078 | RMSE: 0.1210 | Consistency: 0.0623 | SSIM: 0.8616 | Grad: 65692.344\n",
      "[Epoch 15, Batch 18/100] Loss: 0.2094 | RMSE: 0.1226 | Consistency: 0.0632 | SSIM: 0.8624 | Grad: 81049.141\n",
      "[Epoch 15, Batch 19/100] Loss: 0.2073 | RMSE: 0.1205 | Consistency: 0.0610 | SSIM: 0.8621 | Grad: 58477.832\n",
      "[Epoch 15, Batch 20/100] Loss: 0.2135 | RMSE: 0.1268 | Consistency: 0.0631 | SSIM: 0.8610 | Grad: 106818.773\n",
      "[Epoch 15, Batch 21/100] Loss: 0.2187 | RMSE: 0.1316 | Consistency: 0.0791 | SSIM: 0.8634 | Grad: 80340.594\n",
      "[Epoch 15, Batch 22/100] Loss: 0.2090 | RMSE: 0.1223 | Consistency: 0.0669 | SSIM: 0.8608 | Grad: 63526.516\n",
      "[Epoch 15, Batch 23/100] Loss: 0.2225 | RMSE: 0.1353 | Consistency: 0.0796 | SSIM: 0.8640 | Grad: 93196.672\n",
      "[Epoch 15, Batch 24/100] Loss: 0.2088 | RMSE: 0.1223 | Consistency: 0.0607 | SSIM: 0.8584 | Grad: 89912.430\n",
      "[Epoch 15, Batch 25/100] Loss: 0.2071 | RMSE: 0.1202 | Consistency: 0.0631 | SSIM: 0.8622 | Grad: 66463.828\n",
      "[Epoch 15, Batch 26/100] Loss: 0.2037 | RMSE: 0.1170 | Consistency: 0.0573 | SSIM: 0.8608 | Grad: 36086.430\n",
      "[Epoch 15, Batch 27/100] Loss: 0.2071 | RMSE: 0.1205 | Consistency: 0.0625 | SSIM: 0.8597 | Grad: 97880.109\n",
      "[Epoch 15, Batch 28/100] Loss: 0.2051 | RMSE: 0.1183 | Consistency: 0.0604 | SSIM: 0.8611 | Grad: 22199.736\n",
      "[Epoch 15, Batch 29/100] Loss: 0.2060 | RMSE: 0.1193 | Consistency: 0.0604 | SSIM: 0.8613 | Grad: 66275.727\n",
      "[Epoch 15, Batch 30/100] Loss: 0.2129 | RMSE: 0.1261 | Consistency: 0.0719 | SSIM: 0.8611 | Grad: 62506.301\n",
      "[Epoch 15, Batch 31/100] Loss: 0.2052 | RMSE: 0.1184 | Consistency: 0.0612 | SSIM: 0.8614 | Grad: 9664.501\n",
      "[Epoch 15, Batch 32/100] Loss: 0.2227 | RMSE: 0.1358 | Consistency: 0.0718 | SSIM: 0.8620 | Grad: 127202.352\n",
      "[Epoch 15, Batch 33/100] Loss: 0.2279 | RMSE: 0.1408 | Consistency: 0.0859 | SSIM: 0.8620 | Grad: 117895.664\n",
      "[Epoch 15, Batch 34/100] Loss: 0.2198 | RMSE: 0.1330 | Consistency: 0.0834 | SSIM: 0.8601 | Grad: 73408.656\n",
      "[Epoch 15, Batch 35/100] Loss: 0.2142 | RMSE: 0.1273 | Consistency: 0.0722 | SSIM: 0.8615 | Grad: 32598.713\n",
      "[Epoch 15, Batch 36/100] Loss: 0.2125 | RMSE: 0.1256 | Consistency: 0.0705 | SSIM: 0.8616 | Grad: 94448.305\n",
      "[Epoch 15, Batch 37/100] Loss: 0.2269 | RMSE: 0.1395 | Consistency: 0.0900 | SSIM: 0.8656 | Grad: 98924.945\n",
      "[Epoch 15, Batch 38/100] Loss: 0.2128 | RMSE: 0.1258 | Consistency: 0.0715 | SSIM: 0.8628 | Grad: 55274.012\n",
      "[Epoch 15, Batch 39/100] Loss: 0.2332 | RMSE: 0.1458 | Consistency: 0.0813 | SSIM: 0.8665 | Grad: 126504.695\n",
      "[Epoch 15, Batch 40/100] Loss: 0.2180 | RMSE: 0.1310 | Consistency: 0.0708 | SSIM: 0.8624 | Grad: 101787.273\n",
      "[Epoch 15, Batch 41/100] Loss: 0.2131 | RMSE: 0.1261 | Consistency: 0.0686 | SSIM: 0.8635 | Grad: 60402.883\n",
      "[Epoch 15, Batch 42/100] Loss: 0.2106 | RMSE: 0.1236 | Consistency: 0.0632 | SSIM: 0.8634 | Grad: 51801.676\n",
      "[Epoch 15, Batch 43/100] Loss: 0.2107 | RMSE: 0.1239 | Consistency: 0.0601 | SSIM: 0.8628 | Grad: 82099.984\n",
      "[Epoch 15, Batch 44/100] Loss: 0.2103 | RMSE: 0.1233 | Consistency: 0.0660 | SSIM: 0.8635 | Grad: 49643.836\n",
      "[Epoch 15, Batch 45/100] Loss: 0.2135 | RMSE: 0.1264 | Consistency: 0.0686 | SSIM: 0.8633 | Grad: 62132.191\n",
      "[Epoch 15, Batch 46/100] Loss: 0.2064 | RMSE: 0.1197 | Consistency: 0.0583 | SSIM: 0.8603 | Grad: 77637.703\n",
      "[Epoch 15, Batch 47/100] Loss: 0.2082 | RMSE: 0.1215 | Consistency: 0.0635 | SSIM: 0.8609 | Grad: 67627.328\n",
      "[Epoch 15, Batch 48/100] Loss: 0.2079 | RMSE: 0.1210 | Consistency: 0.0607 | SSIM: 0.8628 | Grad: 67349.688\n",
      "[Epoch 15, Batch 49/100] Loss: 0.2089 | RMSE: 0.1220 | Consistency: 0.0628 | SSIM: 0.8624 | Grad: 82155.000\n",
      "[Epoch 15, Batch 50/100] Loss: 0.2117 | RMSE: 0.1248 | Consistency: 0.0661 | SSIM: 0.8621 | Grad: 102905.211\n",
      "[Epoch 15, Batch 51/100] Loss: 0.2165 | RMSE: 0.1298 | Consistency: 0.0672 | SSIM: 0.8600 | Grad: 87114.438\n",
      "[Epoch 15, Batch 52/100] Loss: 0.2050 | RMSE: 0.1184 | Consistency: 0.0609 | SSIM: 0.8605 | Grad: 34810.387\n",
      "[Epoch 15, Batch 53/100] Loss: 0.2037 | RMSE: 0.1169 | Consistency: 0.0615 | SSIM: 0.8621 | Grad: 34542.816\n",
      "[Epoch 15, Batch 54/100] Loss: 0.2126 | RMSE: 0.1257 | Consistency: 0.0671 | SSIM: 0.8627 | Grad: 40094.609\n",
      "[Epoch 15, Batch 55/100] Loss: 0.2054 | RMSE: 0.1185 | Consistency: 0.0596 | SSIM: 0.8630 | Grad: 23836.098\n",
      "[Epoch 15, Batch 56/100] Loss: 0.2083 | RMSE: 0.1216 | Consistency: 0.0655 | SSIM: 0.8606 | Grad: 48827.453\n",
      "[Epoch 15, Batch 57/100] Loss: 0.2083 | RMSE: 0.1215 | Consistency: 0.0653 | SSIM: 0.8618 | Grad: 69952.508\n",
      "[Epoch 15, Batch 58/100] Loss: 0.2095 | RMSE: 0.1227 | Consistency: 0.0679 | SSIM: 0.8619 | Grad: 65381.891\n",
      "[Epoch 15, Batch 59/100] Loss: 0.2123 | RMSE: 0.1254 | Consistency: 0.0636 | SSIM: 0.8623 | Grad: 65948.773\n",
      "[Epoch 15, Batch 60/100] Loss: 0.2034 | RMSE: 0.1166 | Consistency: 0.0569 | SSIM: 0.8617 | Grad: 37385.449\n",
      "[Epoch 15, Batch 61/100] Loss: 0.2093 | RMSE: 0.1223 | Consistency: 0.0630 | SSIM: 0.8632 | Grad: 77009.281\n",
      "[Epoch 15, Batch 62/100] Loss: 0.2072 | RMSE: 0.1204 | Consistency: 0.0626 | SSIM: 0.8621 | Grad: 41409.273\n",
      "[Epoch 15, Batch 63/100] Loss: 0.2054 | RMSE: 0.1187 | Consistency: 0.0588 | SSIM: 0.8610 | Grad: 60636.621\n",
      "[Epoch 15, Batch 64/100] Loss: 0.2093 | RMSE: 0.1226 | Consistency: 0.0618 | SSIM: 0.8607 | Grad: 70314.031\n",
      "[Epoch 15, Batch 65/100] Loss: 0.2157 | RMSE: 0.1288 | Consistency: 0.0656 | SSIM: 0.8625 | Grad: 100924.562\n",
      "[Epoch 15, Batch 66/100] Loss: 0.2165 | RMSE: 0.1297 | Consistency: 0.0644 | SSIM: 0.8617 | Grad: 95234.992\n",
      "[Epoch 15, Batch 67/100] Loss: 0.2083 | RMSE: 0.1214 | Consistency: 0.0593 | SSIM: 0.8623 | Grad: 88974.914\n",
      "[Epoch 15, Batch 68/100] Loss: 0.2081 | RMSE: 0.1214 | Consistency: 0.0619 | SSIM: 0.8611 | Grad: 80035.148\n",
      "[Epoch 15, Batch 69/100] Loss: 0.2029 | RMSE: 0.1165 | Consistency: 0.0575 | SSIM: 0.8591 | Grad: 52761.117\n",
      "[Epoch 15, Batch 70/100] Loss: 0.2066 | RMSE: 0.1198 | Consistency: 0.0627 | SSIM: 0.8616 | Grad: 60459.102\n",
      "[Epoch 15, Batch 71/100] Loss: 0.2048 | RMSE: 0.1180 | Consistency: 0.0579 | SSIM: 0.8619 | Grad: 53173.199\n",
      "[Epoch 15, Batch 72/100] Loss: 0.2091 | RMSE: 0.1222 | Consistency: 0.0641 | SSIM: 0.8620 | Grad: 57374.105\n",
      "[Epoch 15, Batch 73/100] Loss: 0.2147 | RMSE: 0.1278 | Consistency: 0.0712 | SSIM: 0.8618 | Grad: 61674.789\n",
      "[Epoch 15, Batch 74/100] Loss: 0.2087 | RMSE: 0.1219 | Consistency: 0.0574 | SSIM: 0.8621 | Grad: 63971.863\n",
      "[Epoch 15, Batch 75/100] Loss: 0.2029 | RMSE: 0.1163 | Consistency: 0.0578 | SSIM: 0.8607 | Grad: 16463.336\n",
      "[Epoch 15, Batch 76/100] Loss: 0.2152 | RMSE: 0.1284 | Consistency: 0.0601 | SSIM: 0.8621 | Grad: 92478.867\n",
      "[Epoch 15, Batch 77/100] Loss: 0.2140 | RMSE: 0.1272 | Consistency: 0.0619 | SSIM: 0.8627 | Grad: 90021.258\n",
      "[Epoch 15, Batch 78/100] Loss: 0.2070 | RMSE: 0.1202 | Consistency: 0.0585 | SSIM: 0.8620 | Grad: 55432.004\n",
      "[Epoch 15, Batch 79/100] Loss: 0.2087 | RMSE: 0.1218 | Consistency: 0.0649 | SSIM: 0.8623 | Grad: 70522.484\n",
      "[Epoch 15, Batch 80/100] Loss: 0.2130 | RMSE: 0.1261 | Consistency: 0.0617 | SSIM: 0.8627 | Grad: 96024.250\n",
      "[Epoch 15, Batch 81/100] Loss: 0.2154 | RMSE: 0.1287 | Consistency: 0.0645 | SSIM: 0.8604 | Grad: 109988.242\n",
      "[Epoch 15, Batch 82/100] Loss: 0.2079 | RMSE: 0.1212 | Consistency: 0.0630 | SSIM: 0.8604 | Grad: 73447.023\n",
      "[Epoch 15, Batch 83/100] Loss: 0.2062 | RMSE: 0.1198 | Consistency: 0.0624 | SSIM: 0.8579 | Grad: 43635.164\n",
      "[Epoch 15, Batch 84/100] Loss: 0.2048 | RMSE: 0.1181 | Consistency: 0.0592 | SSIM: 0.8609 | Grad: 63775.191\n",
      "[Epoch 15, Batch 85/100] Loss: 0.2100 | RMSE: 0.1233 | Consistency: 0.0630 | SSIM: 0.8603 | Grad: 91776.500\n",
      "[Epoch 15, Batch 86/100] Loss: 0.2055 | RMSE: 0.1189 | Consistency: 0.0631 | SSIM: 0.8605 | Grad: 59968.594\n",
      "[Epoch 15, Batch 87/100] Loss: 0.2102 | RMSE: 0.1233 | Consistency: 0.0705 | SSIM: 0.8620 | Grad: 72472.695\n",
      "[Epoch 15, Batch 88/100] Loss: 0.2117 | RMSE: 0.1248 | Consistency: 0.0732 | SSIM: 0.8618 | Grad: 71627.508\n",
      "[Epoch 15, Batch 89/100] Loss: 0.2076 | RMSE: 0.1209 | Consistency: 0.0613 | SSIM: 0.8609 | Grad: 69774.211\n",
      "[Epoch 15, Batch 90/100] Loss: 0.2027 | RMSE: 0.1160 | Consistency: 0.0563 | SSIM: 0.8610 | Grad: 48392.664\n",
      "[Epoch 15, Batch 91/100] Loss: 0.2047 | RMSE: 0.1179 | Consistency: 0.0579 | SSIM: 0.8614 | Grad: 39502.434\n",
      "[Epoch 15, Batch 92/100] Loss: 0.2018 | RMSE: 0.1154 | Consistency: 0.0559 | SSIM: 0.8590 | Grad: 60707.117\n",
      "[Epoch 15, Batch 93/100] Loss: 0.2045 | RMSE: 0.1179 | Consistency: 0.0596 | SSIM: 0.8600 | Grad: 36974.680\n",
      "[Epoch 15, Batch 94/100] Loss: 0.2059 | RMSE: 0.1193 | Consistency: 0.0624 | SSIM: 0.8599 | Grad: 31739.615\n",
      "[Epoch 15, Batch 95/100] Loss: 0.2078 | RMSE: 0.1210 | Consistency: 0.0593 | SSIM: 0.8623 | Grad: 61015.562\n",
      "[Epoch 15, Batch 96/100] Loss: 0.2084 | RMSE: 0.1216 | Consistency: 0.0610 | SSIM: 0.8623 | Grad: 80895.266\n",
      "[Epoch 15, Batch 97/100] Loss: 0.2052 | RMSE: 0.1186 | Consistency: 0.0622 | SSIM: 0.8604 | Grad: 23373.092\n",
      "[Epoch 15, Batch 98/100] Loss: 0.2065 | RMSE: 0.1197 | Consistency: 0.0593 | SSIM: 0.8619 | Grad: 42786.594\n",
      "[Epoch 15, Batch 99/100] Loss: 0.2069 | RMSE: 0.1201 | Consistency: 0.0601 | SSIM: 0.8618 | Grad: 39859.352\n",
      "[Epoch 15, Batch 100/100] Loss: 0.2070 | RMSE: 0.1206 | Consistency: 0.0610 | SSIM: 0.8578 | Grad: 64140.328\n",
      "✅ Epoch 15 완료!\n",
      "   📈 평균 Loss: 0.209754\n",
      "   📊 평균 RMSE: 0.122966\n",
      "   ⚙️  학습률: 0.000977\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 16/80 시작...\n",
      "[Epoch 16, Batch 1/100] Loss: 0.2029 | RMSE: 0.1163 | Consistency: 0.0586 | SSIM: 0.8602 | Grad: 35791.477\n",
      "[Epoch 16, Batch 2/100] Loss: 0.2026 | RMSE: 0.1160 | Consistency: 0.0565 | SSIM: 0.8600 | Grad: 14529.694\n",
      "[Epoch 16, Batch 3/100] Loss: 0.2102 | RMSE: 0.1234 | Consistency: 0.0665 | SSIM: 0.8616 | Grad: 93292.664\n",
      "[Epoch 16, Batch 4/100] Loss: 0.2193 | RMSE: 0.1324 | Consistency: 0.0712 | SSIM: 0.8618 | Grad: 132547.469\n",
      "[Epoch 16, Batch 5/100] Loss: 0.2116 | RMSE: 0.1247 | Consistency: 0.0630 | SSIM: 0.8623 | Grad: 82366.836\n",
      "[Epoch 16, Batch 6/100] Loss: 0.2058 | RMSE: 0.1190 | Consistency: 0.0602 | SSIM: 0.8621 | Grad: 30480.871\n",
      "[Epoch 16, Batch 7/100] Loss: 0.2180 | RMSE: 0.1308 | Consistency: 0.0698 | SSIM: 0.8642 | Grad: 96397.609\n",
      "[Epoch 16, Batch 8/100] Loss: 0.2128 | RMSE: 0.1257 | Consistency: 0.0690 | SSIM: 0.8642 | Grad: 77872.320\n",
      "[Epoch 16, Batch 9/100] Loss: 0.2043 | RMSE: 0.1176 | Consistency: 0.0608 | SSIM: 0.8618 | Grad: 33934.266\n",
      "[Epoch 16, Batch 10/100] Loss: 0.2066 | RMSE: 0.1198 | Consistency: 0.0624 | SSIM: 0.8618 | Grad: 73500.695\n",
      "[Epoch 16, Batch 11/100] Loss: 0.2217 | RMSE: 0.1345 | Consistency: 0.0894 | SSIM: 0.8634 | Grad: 110122.672\n",
      "[Epoch 16, Batch 12/100] Loss: 0.2063 | RMSE: 0.1197 | Consistency: 0.0664 | SSIM: 0.8593 | Grad: 86921.352\n",
      "[Epoch 16, Batch 13/100] Loss: 0.2063 | RMSE: 0.1196 | Consistency: 0.0635 | SSIM: 0.8609 | Grad: 56221.234\n",
      "[Epoch 16, Batch 14/100] Loss: 0.2075 | RMSE: 0.1209 | Consistency: 0.0666 | SSIM: 0.8594 | Grad: 78866.375\n",
      "[Epoch 16, Batch 15/100] Loss: 0.2049 | RMSE: 0.1184 | Consistency: 0.0610 | SSIM: 0.8593 | Grad: 63054.906\n",
      "[Epoch 16, Batch 16/100] Loss: 0.2053 | RMSE: 0.1187 | Consistency: 0.0596 | SSIM: 0.8599 | Grad: 39522.156\n",
      "[Epoch 16, Batch 17/100] Loss: 0.2181 | RMSE: 0.1311 | Consistency: 0.0713 | SSIM: 0.8626 | Grad: 112254.148\n",
      "[Epoch 16, Batch 18/100] Loss: 0.2150 | RMSE: 0.1282 | Consistency: 0.0720 | SSIM: 0.8609 | Grad: 112447.180\n",
      "[Epoch 16, Batch 19/100] Loss: 0.2048 | RMSE: 0.1183 | Consistency: 0.0620 | SSIM: 0.8592 | Grad: 74783.445\n",
      "[Epoch 16, Batch 20/100] Loss: 0.2054 | RMSE: 0.1188 | Consistency: 0.0586 | SSIM: 0.8603 | Grad: 37621.848\n",
      "[Epoch 16, Batch 21/100] Loss: 0.2067 | RMSE: 0.1200 | Consistency: 0.0664 | SSIM: 0.8598 | Grad: 48271.496\n",
      "[Epoch 16, Batch 22/100] Loss: 0.2094 | RMSE: 0.1227 | Consistency: 0.0661 | SSIM: 0.8607 | Grad: 78376.680\n",
      "[Epoch 16, Batch 23/100] Loss: 0.2044 | RMSE: 0.1179 | Consistency: 0.0624 | SSIM: 0.8590 | Grad: 60802.457\n",
      "[Epoch 16, Batch 24/100] Loss: 0.2026 | RMSE: 0.1160 | Consistency: 0.0610 | SSIM: 0.8599 | Grad: 57403.438\n",
      "[Epoch 16, Batch 25/100] Loss: 0.2078 | RMSE: 0.1210 | Consistency: 0.0593 | SSIM: 0.8626 | Grad: 31618.492\n",
      "[Epoch 16, Batch 26/100] Loss: 0.2046 | RMSE: 0.1178 | Consistency: 0.0608 | SSIM: 0.8618 | Grad: 20541.299\n",
      "[Epoch 16, Batch 27/100] Loss: 0.2087 | RMSE: 0.1219 | Consistency: 0.0620 | SSIM: 0.8614 | Grad: 68156.305\n",
      "[Epoch 16, Batch 28/100] Loss: 0.2024 | RMSE: 0.1158 | Consistency: 0.0554 | SSIM: 0.8606 | Grad: 36954.324\n",
      "[Epoch 16, Batch 29/100] Loss: 0.2036 | RMSE: 0.1170 | Consistency: 0.0643 | SSIM: 0.8598 | Grad: 52045.539\n",
      "[Epoch 16, Batch 30/100] Loss: 0.2080 | RMSE: 0.1211 | Consistency: 0.0596 | SSIM: 0.8621 | Grad: 42942.297\n",
      "[Epoch 16, Batch 31/100] Loss: 0.2058 | RMSE: 0.1191 | Consistency: 0.0585 | SSIM: 0.8610 | Grad: 38800.883\n",
      "[Epoch 16, Batch 32/100] Loss: 0.2057 | RMSE: 0.1190 | Consistency: 0.0588 | SSIM: 0.8611 | Grad: 42094.867\n",
      "[Epoch 16, Batch 33/100] Loss: 0.2073 | RMSE: 0.1205 | Consistency: 0.0571 | SSIM: 0.8625 | Grad: 78629.203\n",
      "[Epoch 16, Batch 34/100] Loss: 0.2088 | RMSE: 0.1221 | Consistency: 0.0684 | SSIM: 0.8598 | Grad: 65751.141\n",
      "[Epoch 16, Batch 35/100] Loss: 0.2050 | RMSE: 0.1183 | Consistency: 0.0572 | SSIM: 0.8614 | Grad: 30288.662\n",
      "[Epoch 16, Batch 36/100] Loss: 0.2044 | RMSE: 0.1176 | Consistency: 0.0591 | SSIM: 0.8621 | Grad: 46537.582\n",
      "[Epoch 16, Batch 37/100] Loss: 0.2101 | RMSE: 0.1232 | Consistency: 0.0609 | SSIM: 0.8634 | Grad: 50972.496\n",
      "[Epoch 16, Batch 38/100] Loss: 0.2035 | RMSE: 0.1168 | Consistency: 0.0620 | SSIM: 0.8609 | Grad: 44434.289\n",
      "[Epoch 16, Batch 39/100] Loss: 0.2052 | RMSE: 0.1185 | Consistency: 0.0589 | SSIM: 0.8614 | Grad: 61368.781\n",
      "[Epoch 16, Batch 40/100] Loss: 0.2117 | RMSE: 0.1246 | Consistency: 0.0686 | SSIM: 0.8637 | Grad: 56552.266\n",
      "[Epoch 16, Batch 41/100] Loss: 0.2098 | RMSE: 0.1229 | Consistency: 0.0592 | SSIM: 0.8632 | Grad: 47177.469\n",
      "[Epoch 16, Batch 42/100] Loss: 0.2099 | RMSE: 0.1232 | Consistency: 0.0660 | SSIM: 0.8607 | Grad: 79081.438\n",
      "[Epoch 16, Batch 43/100] Loss: 0.2043 | RMSE: 0.1174 | Consistency: 0.0624 | SSIM: 0.8627 | Grad: 44129.883\n",
      "[Epoch 16, Batch 44/100] Loss: 0.2043 | RMSE: 0.1175 | Consistency: 0.0579 | SSIM: 0.8616 | Grad: 34026.234\n",
      "[Epoch 16, Batch 45/100] Loss: 0.2017 | RMSE: 0.1153 | Consistency: 0.0590 | SSIM: 0.8578 | Grad: 23204.311\n",
      "[Epoch 16, Batch 46/100] Loss: 0.2074 | RMSE: 0.1210 | Consistency: 0.0585 | SSIM: 0.8582 | Grad: 63409.320\n",
      "[Epoch 16, Batch 47/100] Loss: 0.2040 | RMSE: 0.1176 | Consistency: 0.0580 | SSIM: 0.8580 | Grad: 64084.090\n",
      "[Epoch 16, Batch 48/100] Loss: 0.2062 | RMSE: 0.1196 | Consistency: 0.0610 | SSIM: 0.8606 | Grad: 54625.234\n",
      "[Epoch 16, Batch 49/100] Loss: 0.2096 | RMSE: 0.1230 | Consistency: 0.0604 | SSIM: 0.8605 | Grad: 77868.086\n",
      "[Epoch 16, Batch 50/100] Loss: 0.2115 | RMSE: 0.1246 | Consistency: 0.0658 | SSIM: 0.8626 | Grad: 78121.609\n",
      "[Epoch 16, Batch 51/100] Loss: 0.2054 | RMSE: 0.1185 | Consistency: 0.0595 | SSIM: 0.8629 | Grad: 67647.555\n",
      "[Epoch 16, Batch 52/100] Loss: 0.2025 | RMSE: 0.1158 | Consistency: 0.0564 | SSIM: 0.8616 | Grad: 54072.000\n",
      "[Epoch 16, Batch 53/100] Loss: 0.2017 | RMSE: 0.1151 | Consistency: 0.0550 | SSIM: 0.8601 | Grad: 38834.477\n",
      "[Epoch 16, Batch 54/100] Loss: 0.2111 | RMSE: 0.1243 | Consistency: 0.0579 | SSIM: 0.8613 | Grad: 37006.668\n",
      "[Epoch 16, Batch 55/100] Loss: 0.2059 | RMSE: 0.1190 | Consistency: 0.0619 | SSIM: 0.8630 | Grad: 42374.594\n",
      "[Epoch 16, Batch 56/100] Loss: 0.2055 | RMSE: 0.1189 | Consistency: 0.0591 | SSIM: 0.8602 | Grad: 75536.016\n",
      "[Epoch 16, Batch 57/100] Loss: 0.2168 | RMSE: 0.1299 | Consistency: 0.0741 | SSIM: 0.8616 | Grad: 47441.949\n",
      "[Epoch 16, Batch 58/100] Loss: 0.2052 | RMSE: 0.1183 | Consistency: 0.0566 | SSIM: 0.8628 | Grad: 26657.307\n",
      "[Epoch 16, Batch 59/100] Loss: 0.2072 | RMSE: 0.1206 | Consistency: 0.0631 | SSIM: 0.8594 | Grad: 86278.531\n",
      "[Epoch 16, Batch 60/100] Loss: 0.2087 | RMSE: 0.1221 | Consistency: 0.0615 | SSIM: 0.8601 | Grad: 76088.242\n",
      "[Epoch 16, Batch 61/100] Loss: 0.2130 | RMSE: 0.1262 | Consistency: 0.0683 | SSIM: 0.8619 | Grad: 56616.539\n",
      "[Epoch 16, Batch 62/100] Loss: 0.2027 | RMSE: 0.1160 | Consistency: 0.0602 | SSIM: 0.8609 | Grad: 48535.688\n",
      "[Epoch 16, Batch 63/100] Loss: 0.2057 | RMSE: 0.1189 | Consistency: 0.0578 | SSIM: 0.8616 | Grad: 73391.648\n",
      "[Epoch 16, Batch 64/100] Loss: 0.2043 | RMSE: 0.1176 | Consistency: 0.0613 | SSIM: 0.8611 | Grad: 60606.230\n",
      "[Epoch 16, Batch 65/100] Loss: 0.2094 | RMSE: 0.1225 | Consistency: 0.0667 | SSIM: 0.8619 | Grad: 65955.531\n",
      "[Epoch 16, Batch 66/100] Loss: 0.2031 | RMSE: 0.1165 | Consistency: 0.0582 | SSIM: 0.8602 | Grad: 62033.520\n",
      "[Epoch 16, Batch 67/100] Loss: 0.2090 | RMSE: 0.1222 | Consistency: 0.0657 | SSIM: 0.8617 | Grad: 74049.906\n",
      "[Epoch 16, Batch 68/100] Loss: 0.2107 | RMSE: 0.1239 | Consistency: 0.0672 | SSIM: 0.8617 | Grad: 71742.586\n",
      "[Epoch 16, Batch 69/100] Loss: 0.2010 | RMSE: 0.1144 | Consistency: 0.0578 | SSIM: 0.8603 | Grad: 31558.449\n",
      "[Epoch 16, Batch 70/100] Loss: 0.2121 | RMSE: 0.1255 | Consistency: 0.0721 | SSIM: 0.8589 | Grad: 99264.172\n",
      "[Epoch 16, Batch 71/100] Loss: 0.2262 | RMSE: 0.1391 | Consistency: 0.0974 | SSIM: 0.8617 | Grad: 143247.938\n",
      "[Epoch 16, Batch 72/100] Loss: 0.2114 | RMSE: 0.1245 | Consistency: 0.0744 | SSIM: 0.8612 | Grad: 90992.109\n",
      "[Epoch 16, Batch 73/100] Loss: 0.2027 | RMSE: 0.1159 | Consistency: 0.0563 | SSIM: 0.8618 | Grad: 20111.270\n",
      "[Epoch 16, Batch 74/100] Loss: 0.2015 | RMSE: 0.1149 | Consistency: 0.0558 | SSIM: 0.8607 | Grad: 54703.746\n",
      "[Epoch 16, Batch 75/100] Loss: 0.2012 | RMSE: 0.1144 | Consistency: 0.0560 | SSIM: 0.8621 | Grad: 17099.186\n",
      "[Epoch 16, Batch 76/100] Loss: 0.2039 | RMSE: 0.1173 | Consistency: 0.0576 | SSIM: 0.8602 | Grad: 38584.223\n",
      "[Epoch 16, Batch 77/100] Loss: 0.2221 | RMSE: 0.1350 | Consistency: 0.0798 | SSIM: 0.8630 | Grad: 87536.688\n",
      "[Epoch 16, Batch 78/100] Loss: 0.2099 | RMSE: 0.1233 | Consistency: 0.0630 | SSIM: 0.8597 | Grad: 56594.641\n",
      "[Epoch 16, Batch 79/100] Loss: 0.2050 | RMSE: 0.1184 | Consistency: 0.0630 | SSIM: 0.8597 | Grad: 50465.746\n",
      "[Epoch 16, Batch 80/100] Loss: 0.2077 | RMSE: 0.1211 | Consistency: 0.0637 | SSIM: 0.8601 | Grad: 72100.039\n",
      "[Epoch 16, Batch 81/100] Loss: 0.2055 | RMSE: 0.1189 | Consistency: 0.0635 | SSIM: 0.8598 | Grad: 33717.781\n",
      "[Epoch 16, Batch 82/100] Loss: 0.2043 | RMSE: 0.1177 | Consistency: 0.0607 | SSIM: 0.8598 | Grad: 45325.859\n",
      "[Epoch 16, Batch 83/100] Loss: 0.2074 | RMSE: 0.1205 | Consistency: 0.0626 | SSIM: 0.8618 | Grad: 51071.578\n",
      "[Epoch 16, Batch 84/100] Loss: 0.2040 | RMSE: 0.1173 | Consistency: 0.0617 | SSIM: 0.8607 | Grad: 44144.578\n",
      "[Epoch 16, Batch 85/100] Loss: 0.2066 | RMSE: 0.1199 | Consistency: 0.0608 | SSIM: 0.8611 | Grad: 36095.742\n",
      "[Epoch 16, Batch 86/100] Loss: 0.2060 | RMSE: 0.1191 | Consistency: 0.0627 | SSIM: 0.8626 | Grad: 41740.805\n",
      "[Epoch 16, Batch 87/100] Loss: 0.2034 | RMSE: 0.1166 | Consistency: 0.0567 | SSIM: 0.8620 | Grad: 29793.250\n",
      "[Epoch 16, Batch 88/100] Loss: 0.2086 | RMSE: 0.1217 | Consistency: 0.0611 | SSIM: 0.8631 | Grad: 69347.562\n",
      "[Epoch 16, Batch 89/100] Loss: 0.2061 | RMSE: 0.1194 | Consistency: 0.0603 | SSIM: 0.8605 | Grad: 66751.898\n",
      "[Epoch 16, Batch 90/100] Loss: 0.2033 | RMSE: 0.1168 | Consistency: 0.0593 | SSIM: 0.8590 | Grad: 71725.266\n",
      "[Epoch 16, Batch 91/100] Loss: 0.2070 | RMSE: 0.1203 | Consistency: 0.0579 | SSIM: 0.8612 | Grad: 72712.172\n",
      "[Epoch 16, Batch 92/100] Loss: 0.2109 | RMSE: 0.1242 | Consistency: 0.0672 | SSIM: 0.8612 | Grad: 94573.172\n",
      "[Epoch 16, Batch 93/100] Loss: 0.2078 | RMSE: 0.1210 | Consistency: 0.0604 | SSIM: 0.8618 | Grad: 65504.617\n",
      "[Epoch 16, Batch 94/100] Loss: 0.2061 | RMSE: 0.1192 | Consistency: 0.0623 | SSIM: 0.8622 | Grad: 59313.098\n",
      "[Epoch 16, Batch 95/100] Loss: 0.2069 | RMSE: 0.1201 | Consistency: 0.0651 | SSIM: 0.8617 | Grad: 46486.406\n",
      "[Epoch 16, Batch 96/100] Loss: 0.2041 | RMSE: 0.1173 | Consistency: 0.0560 | SSIM: 0.8621 | Grad: 50733.754\n",
      "[Epoch 16, Batch 97/100] Loss: 0.2046 | RMSE: 0.1180 | Consistency: 0.0576 | SSIM: 0.8604 | Grad: 36946.852\n",
      "[Epoch 16, Batch 98/100] Loss: 0.2013 | RMSE: 0.1147 | Consistency: 0.0574 | SSIM: 0.8603 | Grad: 45418.117\n",
      "[Epoch 16, Batch 99/100] Loss: 0.2041 | RMSE: 0.1173 | Consistency: 0.0625 | SSIM: 0.8621 | Grad: 45937.023\n",
      "[Epoch 16, Batch 100/100] Loss: 0.2001 | RMSE: 0.1137 | Consistency: 0.0554 | SSIM: 0.8586 | Grad: 16209.997\n",
      "✅ Epoch 16 완료!\n",
      "   📈 평균 Loss: 0.207245\n",
      "   📊 평균 RMSE: 0.120507\n",
      "   ⚙️  학습률: 0.000970\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 17/80 시작...\n",
      "[Epoch 17, Batch 1/100] Loss: 0.2004 | RMSE: 0.1139 | Consistency: 0.0548 | SSIM: 0.8599 | Grad: 32883.355\n",
      "[Epoch 17, Batch 2/100] Loss: 0.2124 | RMSE: 0.1256 | Consistency: 0.0704 | SSIM: 0.8607 | Grad: 93900.594\n",
      "[Epoch 17, Batch 3/100] Loss: 0.2077 | RMSE: 0.1211 | Consistency: 0.0612 | SSIM: 0.8593 | Grad: 91307.547\n",
      "[Epoch 17, Batch 4/100] Loss: 0.2018 | RMSE: 0.1153 | Consistency: 0.0589 | SSIM: 0.8598 | Grad: 42971.285\n",
      "[Epoch 17, Batch 5/100] Loss: 0.2061 | RMSE: 0.1195 | Consistency: 0.0596 | SSIM: 0.8598 | Grad: 60749.176\n",
      "[Epoch 17, Batch 6/100] Loss: 0.2050 | RMSE: 0.1185 | Consistency: 0.0585 | SSIM: 0.8591 | Grad: 68595.984\n",
      "[Epoch 17, Batch 7/100] Loss: 0.2132 | RMSE: 0.1264 | Consistency: 0.0718 | SSIM: 0.8606 | Grad: 69838.031\n",
      "[Epoch 17, Batch 8/100] Loss: 0.2036 | RMSE: 0.1169 | Consistency: 0.0567 | SSIM: 0.8608 | Grad: 32618.463\n",
      "[Epoch 17, Batch 9/100] Loss: 0.2077 | RMSE: 0.1209 | Consistency: 0.0596 | SSIM: 0.8616 | Grad: 83961.453\n",
      "[Epoch 17, Batch 10/100] Loss: 0.2050 | RMSE: 0.1184 | Consistency: 0.0580 | SSIM: 0.8603 | Grad: 59569.473\n",
      "[Epoch 17, Batch 11/100] Loss: 0.2030 | RMSE: 0.1165 | Consistency: 0.0577 | SSIM: 0.8592 | Grad: 53356.891\n",
      "[Epoch 17, Batch 12/100] Loss: 0.2057 | RMSE: 0.1190 | Consistency: 0.0617 | SSIM: 0.8615 | Grad: 36826.609\n",
      "[Epoch 17, Batch 13/100] Loss: 0.2072 | RMSE: 0.1206 | Consistency: 0.0576 | SSIM: 0.8605 | Grad: 83932.680\n",
      "[Epoch 17, Batch 14/100] Loss: 0.2066 | RMSE: 0.1199 | Consistency: 0.0592 | SSIM: 0.8610 | Grad: 66592.688\n",
      "[Epoch 17, Batch 15/100] Loss: 0.2016 | RMSE: 0.1151 | Consistency: 0.0592 | SSIM: 0.8596 | Grad: 43868.801\n",
      "[Epoch 17, Batch 16/100] Loss: 0.2091 | RMSE: 0.1225 | Consistency: 0.0637 | SSIM: 0.8599 | Grad: 75397.734\n",
      "[Epoch 17, Batch 17/100] Loss: 0.2019 | RMSE: 0.1152 | Consistency: 0.0594 | SSIM: 0.8607 | Grad: 47492.605\n",
      "[Epoch 17, Batch 18/100] Loss: 0.2056 | RMSE: 0.1190 | Consistency: 0.0593 | SSIM: 0.8607 | Grad: 67387.953\n",
      "[Epoch 17, Batch 19/100] Loss: 0.1997 | RMSE: 0.1131 | Consistency: 0.0565 | SSIM: 0.8603 | Grad: 16269.351\n",
      "[Epoch 17, Batch 20/100] Loss: 0.2047 | RMSE: 0.1181 | Consistency: 0.0602 | SSIM: 0.8598 | Grad: 49813.863\n",
      "[Epoch 17, Batch 21/100] Loss: 0.2111 | RMSE: 0.1243 | Consistency: 0.0697 | SSIM: 0.8612 | Grad: 71219.164\n",
      "[Epoch 17, Batch 22/100] Loss: 0.2132 | RMSE: 0.1266 | Consistency: 0.0627 | SSIM: 0.8607 | Grad: 90029.906\n",
      "[Epoch 17, Batch 23/100] Loss: 0.2022 | RMSE: 0.1155 | Consistency: 0.0556 | SSIM: 0.8610 | Grad: 41501.012\n",
      "[Epoch 17, Batch 24/100] Loss: 0.2155 | RMSE: 0.1286 | Consistency: 0.0735 | SSIM: 0.8615 | Grad: 84607.859\n",
      "[Epoch 17, Batch 25/100] Loss: 0.2087 | RMSE: 0.1220 | Consistency: 0.0654 | SSIM: 0.8603 | Grad: 76120.664\n",
      "[Epoch 17, Batch 26/100] Loss: 0.2033 | RMSE: 0.1166 | Consistency: 0.0578 | SSIM: 0.8615 | Grad: 35896.812\n",
      "[Epoch 17, Batch 27/100] Loss: 0.2088 | RMSE: 0.1219 | Consistency: 0.0666 | SSIM: 0.8624 | Grad: 63562.035\n",
      "[Epoch 17, Batch 28/100] Loss: 0.2028 | RMSE: 0.1161 | Consistency: 0.0581 | SSIM: 0.8614 | Grad: 46094.070\n",
      "[Epoch 17, Batch 29/100] Loss: 0.2062 | RMSE: 0.1196 | Consistency: 0.0631 | SSIM: 0.8601 | Grad: 49271.273\n",
      "[Epoch 17, Batch 30/100] Loss: 0.2118 | RMSE: 0.1250 | Consistency: 0.0655 | SSIM: 0.8618 | Grad: 82723.914\n",
      "[Epoch 17, Batch 31/100] Loss: 0.2077 | RMSE: 0.1209 | Consistency: 0.0652 | SSIM: 0.8607 | Grad: 47796.223\n",
      "[Epoch 17, Batch 32/100] Loss: 0.2115 | RMSE: 0.1247 | Consistency: 0.0665 | SSIM: 0.8608 | Grad: 73250.234\n",
      "[Epoch 17, Batch 33/100] Loss: 0.2055 | RMSE: 0.1188 | Consistency: 0.0589 | SSIM: 0.8607 | Grad: 85765.141\n",
      "[Epoch 17, Batch 34/100] Loss: 0.2080 | RMSE: 0.1215 | Consistency: 0.0724 | SSIM: 0.8578 | Grad: 73921.219\n",
      "[Epoch 17, Batch 35/100] Loss: 0.2058 | RMSE: 0.1191 | Consistency: 0.0622 | SSIM: 0.8606 | Grad: 55081.000\n",
      "[Epoch 17, Batch 36/100] Loss: 0.2038 | RMSE: 0.1172 | Consistency: 0.0597 | SSIM: 0.8596 | Grad: 46795.086\n",
      "[Epoch 17, Batch 37/100] Loss: 0.2046 | RMSE: 0.1179 | Consistency: 0.0573 | SSIM: 0.8607 | Grad: 46913.125\n",
      "[Epoch 17, Batch 38/100] Loss: 0.2048 | RMSE: 0.1180 | Consistency: 0.0596 | SSIM: 0.8618 | Grad: 57448.859\n",
      "[Epoch 17, Batch 39/100] Loss: 0.2082 | RMSE: 0.1214 | Consistency: 0.0625 | SSIM: 0.8617 | Grad: 59021.633\n",
      "[Epoch 17, Batch 40/100] Loss: 0.2020 | RMSE: 0.1157 | Consistency: 0.0572 | SSIM: 0.8581 | Grad: 50389.949\n",
      "[Epoch 17, Batch 41/100] Loss: 0.2046 | RMSE: 0.1179 | Consistency: 0.0596 | SSIM: 0.8611 | Grad: 51728.258\n",
      "[Epoch 17, Batch 42/100] Loss: 0.2028 | RMSE: 0.1161 | Consistency: 0.0589 | SSIM: 0.8609 | Grad: 37486.734\n",
      "[Epoch 17, Batch 43/100] Loss: 0.2010 | RMSE: 0.1144 | Consistency: 0.0574 | SSIM: 0.8605 | Grad: 50325.574\n",
      "[Epoch 17, Batch 44/100] Loss: 0.2086 | RMSE: 0.1219 | Consistency: 0.0607 | SSIM: 0.8612 | Grad: 96571.375\n",
      "[Epoch 17, Batch 45/100] Loss: 0.2071 | RMSE: 0.1203 | Consistency: 0.0655 | SSIM: 0.8613 | Grad: 71657.539\n",
      "[Epoch 17, Batch 46/100] Loss: 0.2037 | RMSE: 0.1170 | Consistency: 0.0591 | SSIM: 0.8610 | Grad: 67992.523\n",
      "[Epoch 17, Batch 47/100] Loss: 0.2069 | RMSE: 0.1202 | Consistency: 0.0601 | SSIM: 0.8616 | Grad: 70950.820\n",
      "[Epoch 17, Batch 48/100] Loss: 0.2021 | RMSE: 0.1155 | Consistency: 0.0595 | SSIM: 0.8607 | Grad: 59943.965\n",
      "[Epoch 17, Batch 49/100] Loss: 0.2062 | RMSE: 0.1194 | Consistency: 0.0626 | SSIM: 0.8613 | Grad: 30006.971\n",
      "[Epoch 17, Batch 50/100] Loss: 0.1993 | RMSE: 0.1128 | Consistency: 0.0557 | SSIM: 0.8601 | Grad: 39545.574\n",
      "[Epoch 17, Batch 51/100] Loss: 0.2026 | RMSE: 0.1160 | Consistency: 0.0576 | SSIM: 0.8596 | Grad: 29250.162\n",
      "[Epoch 17, Batch 52/100] Loss: 0.2070 | RMSE: 0.1202 | Consistency: 0.0639 | SSIM: 0.8616 | Grad: 39227.070\n",
      "[Epoch 17, Batch 53/100] Loss: 0.2051 | RMSE: 0.1185 | Consistency: 0.0563 | SSIM: 0.8605 | Grad: 9570.100\n",
      "[Epoch 17, Batch 54/100] Loss: 0.2047 | RMSE: 0.1181 | Consistency: 0.0596 | SSIM: 0.8595 | Grad: 84780.695\n",
      "[Epoch 17, Batch 55/100] Loss: 0.2139 | RMSE: 0.1271 | Consistency: 0.0698 | SSIM: 0.8610 | Grad: 110920.516\n",
      "[Epoch 17, Batch 56/100] Loss: 0.2072 | RMSE: 0.1205 | Consistency: 0.0594 | SSIM: 0.8616 | Grad: 66687.594\n",
      "[Epoch 17, Batch 57/100] Loss: 0.2077 | RMSE: 0.1210 | Consistency: 0.0621 | SSIM: 0.8614 | Grad: 33068.766\n",
      "[Epoch 17, Batch 58/100] Loss: 0.2076 | RMSE: 0.1207 | Consistency: 0.0636 | SSIM: 0.8617 | Grad: 63503.129\n",
      "[Epoch 17, Batch 59/100] Loss: 0.2071 | RMSE: 0.1203 | Consistency: 0.0626 | SSIM: 0.8620 | Grad: 59735.168\n",
      "[Epoch 17, Batch 60/100] Loss: 0.2104 | RMSE: 0.1235 | Consistency: 0.0703 | SSIM: 0.8621 | Grad: 45381.566\n",
      "[Epoch 17, Batch 61/100] Loss: 0.2040 | RMSE: 0.1174 | Consistency: 0.0600 | SSIM: 0.8607 | Grad: 33160.938\n",
      "[Epoch 17, Batch 62/100] Loss: 0.2050 | RMSE: 0.1183 | Consistency: 0.0602 | SSIM: 0.8602 | Grad: 50817.832\n",
      "[Epoch 17, Batch 63/100] Loss: 0.2035 | RMSE: 0.1168 | Consistency: 0.0591 | SSIM: 0.8603 | Grad: 55076.457\n",
      "[Epoch 17, Batch 64/100] Loss: 0.2035 | RMSE: 0.1169 | Consistency: 0.0589 | SSIM: 0.8603 | Grad: 39191.859\n",
      "[Epoch 17, Batch 65/100] Loss: 0.2109 | RMSE: 0.1241 | Consistency: 0.0644 | SSIM: 0.8615 | Grad: 66911.547\n",
      "[Epoch 17, Batch 66/100] Loss: 0.2064 | RMSE: 0.1196 | Consistency: 0.0595 | SSIM: 0.8614 | Grad: 65736.680\n",
      "[Epoch 17, Batch 67/100] Loss: 0.2047 | RMSE: 0.1180 | Consistency: 0.0591 | SSIM: 0.8616 | Grad: 53396.801\n",
      "[Epoch 17, Batch 68/100] Loss: 0.2094 | RMSE: 0.1227 | Consistency: 0.0636 | SSIM: 0.8608 | Grad: 65556.133\n",
      "[Epoch 17, Batch 69/100] Loss: 0.2095 | RMSE: 0.1226 | Consistency: 0.0683 | SSIM: 0.8621 | Grad: 81194.789\n",
      "[Epoch 17, Batch 70/100] Loss: 0.2059 | RMSE: 0.1191 | Consistency: 0.0579 | SSIM: 0.8616 | Grad: 61865.348\n",
      "[Epoch 17, Batch 71/100] Loss: 0.2021 | RMSE: 0.1157 | Consistency: 0.0574 | SSIM: 0.8581 | Grad: 38921.254\n",
      "[Epoch 17, Batch 72/100] Loss: 0.2069 | RMSE: 0.1202 | Consistency: 0.0580 | SSIM: 0.8609 | Grad: 19151.590\n",
      "[Epoch 17, Batch 73/100] Loss: 0.2050 | RMSE: 0.1184 | Consistency: 0.0617 | SSIM: 0.8606 | Grad: 66372.211\n",
      "[Epoch 17, Batch 74/100] Loss: 0.2073 | RMSE: 0.1206 | Consistency: 0.0607 | SSIM: 0.8608 | Grad: 58832.371\n",
      "[Epoch 17, Batch 75/100] Loss: 0.2077 | RMSE: 0.1211 | Consistency: 0.0604 | SSIM: 0.8602 | Grad: 51005.867\n",
      "[Epoch 17, Batch 76/100] Loss: 0.2042 | RMSE: 0.1174 | Consistency: 0.0637 | SSIM: 0.8621 | Grad: 60195.289\n",
      "[Epoch 17, Batch 77/100] Loss: 0.2022 | RMSE: 0.1156 | Consistency: 0.0587 | SSIM: 0.8607 | Grad: 26448.566\n",
      "[Epoch 17, Batch 78/100] Loss: 0.2214 | RMSE: 0.1342 | Consistency: 0.0790 | SSIM: 0.8636 | Grad: 93046.367\n",
      "[Epoch 17, Batch 79/100] Loss: 0.2208 | RMSE: 0.1339 | Consistency: 0.0702 | SSIM: 0.8620 | Grad: 106436.539\n",
      "[Epoch 17, Batch 80/100] Loss: 0.2137 | RMSE: 0.1268 | Consistency: 0.0777 | SSIM: 0.8608 | Grad: 71661.547\n",
      "[Epoch 17, Batch 81/100] Loss: 0.2031 | RMSE: 0.1164 | Consistency: 0.0587 | SSIM: 0.8607 | Grad: 46133.324\n",
      "[Epoch 17, Batch 82/100] Loss: 0.2049 | RMSE: 0.1184 | Consistency: 0.0574 | SSIM: 0.8593 | Grad: 67939.359\n",
      "[Epoch 17, Batch 83/100] Loss: 0.2001 | RMSE: 0.1134 | Consistency: 0.0559 | SSIM: 0.8613 | Grad: 19095.320\n",
      "[Epoch 17, Batch 84/100] Loss: 0.2061 | RMSE: 0.1195 | Consistency: 0.0574 | SSIM: 0.8602 | Grad: 79528.508\n",
      "[Epoch 17, Batch 85/100] Loss: 0.2143 | RMSE: 0.1275 | Consistency: 0.0584 | SSIM: 0.8629 | Grad: 84559.469\n",
      "[Epoch 17, Batch 86/100] Loss: 0.2079 | RMSE: 0.1210 | Consistency: 0.0622 | SSIM: 0.8632 | Grad: 36764.789\n",
      "[Epoch 17, Batch 87/100] Loss: 0.2024 | RMSE: 0.1159 | Consistency: 0.0567 | SSIM: 0.8591 | Grad: 30164.734\n",
      "[Epoch 17, Batch 88/100] Loss: 0.2022 | RMSE: 0.1156 | Consistency: 0.0607 | SSIM: 0.8594 | Grad: 38569.348\n",
      "[Epoch 17, Batch 89/100] Loss: 0.2044 | RMSE: 0.1177 | Consistency: 0.0607 | SSIM: 0.8603 | Grad: 50007.812\n",
      "[Epoch 17, Batch 90/100] Loss: 0.2071 | RMSE: 0.1204 | Consistency: 0.0647 | SSIM: 0.8601 | Grad: 48914.293\n",
      "[Epoch 17, Batch 91/100] Loss: 0.2036 | RMSE: 0.1169 | Consistency: 0.0619 | SSIM: 0.8610 | Grad: 20146.350\n",
      "[Epoch 17, Batch 92/100] Loss: 0.2016 | RMSE: 0.1151 | Consistency: 0.0596 | SSIM: 0.8594 | Grad: 65254.590\n",
      "[Epoch 17, Batch 93/100] Loss: 0.2055 | RMSE: 0.1189 | Consistency: 0.0551 | SSIM: 0.8605 | Grad: 70261.117\n",
      "[Epoch 17, Batch 94/100] Loss: 0.2041 | RMSE: 0.1174 | Consistency: 0.0619 | SSIM: 0.8608 | Grad: 25396.719\n",
      "[Epoch 17, Batch 95/100] Loss: 0.2052 | RMSE: 0.1184 | Consistency: 0.0585 | SSIM: 0.8619 | Grad: 12787.854\n",
      "[Epoch 17, Batch 96/100] Loss: 0.2107 | RMSE: 0.1239 | Consistency: 0.0732 | SSIM: 0.8605 | Grad: 97034.680\n",
      "[Epoch 17, Batch 97/100] Loss: 0.2062 | RMSE: 0.1194 | Consistency: 0.0605 | SSIM: 0.8626 | Grad: 35998.672\n",
      "[Epoch 17, Batch 98/100] Loss: 0.2069 | RMSE: 0.1201 | Consistency: 0.0607 | SSIM: 0.8616 | Grad: 41128.082\n",
      "[Epoch 17, Batch 99/100] Loss: 0.2103 | RMSE: 0.1232 | Consistency: 0.0701 | SSIM: 0.8642 | Grad: 85903.492\n",
      "[Epoch 17, Batch 100/100] Loss: 0.2067 | RMSE: 0.1198 | Consistency: 0.0630 | SSIM: 0.8630 | Grad: 45466.801\n",
      "✅ Epoch 17 완료!\n",
      "   📈 평균 Loss: 0.206365\n",
      "   📊 평균 RMSE: 0.119666\n",
      "   ⚙️  학습률: 0.000962\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 18/80 시작...\n",
      "[Epoch 18, Batch 1/100] Loss: 0.2041 | RMSE: 0.1172 | Consistency: 0.0586 | SSIM: 0.8629 | Grad: 17322.014\n",
      "[Epoch 18, Batch 2/100] Loss: 0.2062 | RMSE: 0.1195 | Consistency: 0.0621 | SSIM: 0.8610 | Grad: 38516.457\n",
      "[Epoch 18, Batch 3/100] Loss: 0.2082 | RMSE: 0.1214 | Consistency: 0.0678 | SSIM: 0.8613 | Grad: 51873.738\n",
      "[Epoch 18, Batch 4/100] Loss: 0.2101 | RMSE: 0.1233 | Consistency: 0.0636 | SSIM: 0.8619 | Grad: 64443.328\n",
      "[Epoch 18, Batch 5/100] Loss: 0.2024 | RMSE: 0.1158 | Consistency: 0.0551 | SSIM: 0.8598 | Grad: 70163.359\n",
      "[Epoch 18, Batch 6/100] Loss: 0.2028 | RMSE: 0.1162 | Consistency: 0.0596 | SSIM: 0.8601 | Grad: 23222.809\n",
      "[Epoch 18, Batch 7/100] Loss: 0.2017 | RMSE: 0.1152 | Consistency: 0.0583 | SSIM: 0.8589 | Grad: 30678.693\n",
      "[Epoch 18, Batch 8/100] Loss: 0.2065 | RMSE: 0.1200 | Consistency: 0.0608 | SSIM: 0.8596 | Grad: 45443.828\n",
      "[Epoch 18, Batch 9/100] Loss: 0.2105 | RMSE: 0.1237 | Consistency: 0.0620 | SSIM: 0.8617 | Grad: 49875.215\n",
      "[Epoch 18, Batch 10/100] Loss: 0.2097 | RMSE: 0.1230 | Consistency: 0.0611 | SSIM: 0.8617 | Grad: 47456.977\n",
      "[Epoch 18, Batch 11/100] Loss: 0.2074 | RMSE: 0.1207 | Consistency: 0.0627 | SSIM: 0.8608 | Grad: 55596.965\n",
      "[Epoch 18, Batch 12/100] Loss: 0.2105 | RMSE: 0.1237 | Consistency: 0.0608 | SSIM: 0.8615 | Grad: 86076.766\n",
      "[Epoch 18, Batch 13/100] Loss: 0.2006 | RMSE: 0.1139 | Consistency: 0.0549 | SSIM: 0.8608 | Grad: 30303.791\n",
      "[Epoch 18, Batch 14/100] Loss: 0.2024 | RMSE: 0.1158 | Consistency: 0.0587 | SSIM: 0.8597 | Grad: 27087.422\n",
      "[Epoch 18, Batch 15/100] Loss: 0.2111 | RMSE: 0.1245 | Consistency: 0.0656 | SSIM: 0.8597 | Grad: 71137.430\n",
      "[Epoch 18, Batch 16/100] Loss: 0.2069 | RMSE: 0.1203 | Consistency: 0.0600 | SSIM: 0.8597 | Grad: 60678.430\n",
      "[Epoch 18, Batch 17/100] Loss: 0.2031 | RMSE: 0.1167 | Consistency: 0.0575 | SSIM: 0.8584 | Grad: 73349.500\n",
      "[Epoch 18, Batch 18/100] Loss: 0.2084 | RMSE: 0.1216 | Consistency: 0.0628 | SSIM: 0.8622 | Grad: 38691.254\n",
      "[Epoch 18, Batch 19/100] Loss: 0.2118 | RMSE: 0.1250 | Consistency: 0.0668 | SSIM: 0.8616 | Grad: 52056.492\n",
      "[Epoch 18, Batch 20/100] Loss: 0.2046 | RMSE: 0.1178 | Consistency: 0.0590 | SSIM: 0.8615 | Grad: 67444.562\n",
      "[Epoch 18, Batch 21/100] Loss: 0.2027 | RMSE: 0.1161 | Consistency: 0.0605 | SSIM: 0.8605 | Grad: 51070.672\n",
      "[Epoch 18, Batch 22/100] Loss: 0.2015 | RMSE: 0.1150 | Consistency: 0.0562 | SSIM: 0.8591 | Grad: 49093.988\n",
      "[Epoch 18, Batch 23/100] Loss: 0.2044 | RMSE: 0.1178 | Consistency: 0.0578 | SSIM: 0.8597 | Grad: 36437.297\n",
      "[Epoch 18, Batch 24/100] Loss: 0.2063 | RMSE: 0.1196 | Consistency: 0.0606 | SSIM: 0.8607 | Grad: 52978.492\n",
      "[Epoch 18, Batch 25/100] Loss: 0.2033 | RMSE: 0.1170 | Consistency: 0.0593 | SSIM: 0.8572 | Grad: 50044.102\n",
      "[Epoch 18, Batch 26/100] Loss: 0.2077 | RMSE: 0.1211 | Consistency: 0.0594 | SSIM: 0.8603 | Grad: 43408.164\n",
      "[Epoch 18, Batch 27/100] Loss: 0.2091 | RMSE: 0.1226 | Consistency: 0.0655 | SSIM: 0.8584 | Grad: 78842.656\n",
      "[Epoch 18, Batch 28/100] Loss: 0.2113 | RMSE: 0.1245 | Consistency: 0.0725 | SSIM: 0.8601 | Grad: 87144.047\n",
      "[Epoch 18, Batch 29/100] Loss: 0.2027 | RMSE: 0.1161 | Consistency: 0.0566 | SSIM: 0.8606 | Grad: 37695.441\n",
      "[Epoch 18, Batch 30/100] Loss: 0.2053 | RMSE: 0.1186 | Consistency: 0.0623 | SSIM: 0.8606 | Grad: 50917.496\n",
      "[Epoch 18, Batch 31/100] Loss: 0.2077 | RMSE: 0.1210 | Consistency: 0.0591 | SSIM: 0.8608 | Grad: 54236.215\n",
      "[Epoch 18, Batch 32/100] Loss: 0.2022 | RMSE: 0.1158 | Consistency: 0.0597 | SSIM: 0.8579 | Grad: 9566.514\n",
      "[Epoch 18, Batch 33/100] Loss: 0.2071 | RMSE: 0.1207 | Consistency: 0.0596 | SSIM: 0.8583 | Grad: 81775.930\n",
      "[Epoch 18, Batch 34/100] Loss: 0.2090 | RMSE: 0.1223 | Consistency: 0.0607 | SSIM: 0.8609 | Grad: 50597.195\n",
      "[Epoch 18, Batch 35/100] Loss: 0.2126 | RMSE: 0.1257 | Consistency: 0.0690 | SSIM: 0.8617 | Grad: 32180.729\n",
      "[Epoch 18, Batch 36/100] Loss: 0.2025 | RMSE: 0.1159 | Consistency: 0.0581 | SSIM: 0.8605 | Grad: 37576.379\n",
      "[Epoch 18, Batch 37/100] Loss: 0.2048 | RMSE: 0.1182 | Consistency: 0.0580 | SSIM: 0.8602 | Grad: 53152.168\n",
      "[Epoch 18, Batch 38/100] Loss: 0.2171 | RMSE: 0.1302 | Consistency: 0.0657 | SSIM: 0.8624 | Grad: 76465.227\n",
      "[Epoch 18, Batch 39/100] Loss: 0.2021 | RMSE: 0.1152 | Consistency: 0.0600 | SSIM: 0.8624 | Grad: 27881.271\n",
      "[Epoch 18, Batch 40/100] Loss: 0.2022 | RMSE: 0.1155 | Consistency: 0.0569 | SSIM: 0.8613 | Grad: 37725.855\n",
      "[Epoch 18, Batch 41/100] Loss: 0.2044 | RMSE: 0.1176 | Consistency: 0.0605 | SSIM: 0.8611 | Grad: 28784.359\n",
      "[Epoch 18, Batch 42/100] Loss: 0.2076 | RMSE: 0.1212 | Consistency: 0.0663 | SSIM: 0.8577 | Grad: 41963.492\n",
      "[Epoch 18, Batch 43/100] Loss: 0.2122 | RMSE: 0.1257 | Consistency: 0.0668 | SSIM: 0.8584 | Grad: 84630.062\n",
      "[Epoch 18, Batch 44/100] Loss: 0.2050 | RMSE: 0.1185 | Consistency: 0.0598 | SSIM: 0.8599 | Grad: 68187.883\n",
      "[Epoch 18, Batch 45/100] Loss: 0.2039 | RMSE: 0.1175 | Consistency: 0.0586 | SSIM: 0.8582 | Grad: 44505.020\n",
      "[Epoch 18, Batch 46/100] Loss: 0.2088 | RMSE: 0.1222 | Consistency: 0.0583 | SSIM: 0.8598 | Grad: 71979.133\n",
      "[Epoch 18, Batch 47/100] Loss: 0.2099 | RMSE: 0.1229 | Consistency: 0.0657 | SSIM: 0.8631 | Grad: 89483.266\n",
      "[Epoch 18, Batch 48/100] Loss: 0.2121 | RMSE: 0.1251 | Consistency: 0.0646 | SSIM: 0.8638 | Grad: 94111.062\n",
      "[Epoch 18, Batch 49/100] Loss: 0.2074 | RMSE: 0.1207 | Consistency: 0.0633 | SSIM: 0.8610 | Grad: 45726.902\n",
      "[Epoch 18, Batch 50/100] Loss: 0.2013 | RMSE: 0.1149 | Consistency: 0.0574 | SSIM: 0.8590 | Grad: 35057.648\n",
      "[Epoch 18, Batch 51/100] Loss: 0.2029 | RMSE: 0.1163 | Consistency: 0.0591 | SSIM: 0.8602 | Grad: 37453.523\n",
      "[Epoch 18, Batch 52/100] Loss: 0.2078 | RMSE: 0.1215 | Consistency: 0.0588 | SSIM: 0.8574 | Grad: 73836.422\n",
      "[Epoch 18, Batch 53/100] Loss: 0.2070 | RMSE: 0.1204 | Consistency: 0.0587 | SSIM: 0.8597 | Grad: 79043.281\n",
      "[Epoch 18, Batch 54/100] Loss: 0.2028 | RMSE: 0.1162 | Consistency: 0.0592 | SSIM: 0.8600 | Grad: 64753.094\n",
      "[Epoch 18, Batch 55/100] Loss: 0.2037 | RMSE: 0.1170 | Consistency: 0.0563 | SSIM: 0.8612 | Grad: 50039.824\n",
      "[Epoch 18, Batch 56/100] Loss: 0.2116 | RMSE: 0.1247 | Consistency: 0.0742 | SSIM: 0.8613 | Grad: 77679.102\n",
      "[Epoch 18, Batch 57/100] Loss: 0.2129 | RMSE: 0.1259 | Consistency: 0.0674 | SSIM: 0.8624 | Grad: 56755.832\n",
      "[Epoch 18, Batch 58/100] Loss: 0.2043 | RMSE: 0.1174 | Consistency: 0.0576 | SSIM: 0.8632 | Grad: 20215.695\n",
      "[Epoch 18, Batch 59/100] Loss: 0.2024 | RMSE: 0.1160 | Consistency: 0.0565 | SSIM: 0.8582 | Grad: 44942.008\n",
      "[Epoch 18, Batch 60/100] Loss: 0.2039 | RMSE: 0.1173 | Consistency: 0.0610 | SSIM: 0.8596 | Grad: 47599.406\n",
      "[Epoch 18, Batch 61/100] Loss: 0.2126 | RMSE: 0.1257 | Consistency: 0.0596 | SSIM: 0.8628 | Grad: 57848.953\n",
      "[Epoch 18, Batch 62/100] Loss: 0.2060 | RMSE: 0.1193 | Consistency: 0.0625 | SSIM: 0.8608 | Grad: 69731.508\n",
      "[Epoch 18, Batch 63/100] Loss: 0.2049 | RMSE: 0.1183 | Consistency: 0.0623 | SSIM: 0.8595 | Grad: 36363.449\n",
      "[Epoch 18, Batch 64/100] Loss: 0.2037 | RMSE: 0.1169 | Consistency: 0.0578 | SSIM: 0.8616 | Grad: 36086.453\n",
      "[Epoch 18, Batch 65/100] Loss: 0.2062 | RMSE: 0.1196 | Consistency: 0.0603 | SSIM: 0.8604 | Grad: 23807.930\n",
      "[Epoch 18, Batch 66/100] Loss: 0.2034 | RMSE: 0.1169 | Consistency: 0.0580 | SSIM: 0.8595 | Grad: 70283.492\n",
      "[Epoch 18, Batch 67/100] Loss: 0.2096 | RMSE: 0.1230 | Consistency: 0.0604 | SSIM: 0.8603 | Grad: 91248.406\n",
      "[Epoch 18, Batch 68/100] Loss: 0.2031 | RMSE: 0.1169 | Consistency: 0.0564 | SSIM: 0.8569 | Grad: 42605.938\n",
      "[Epoch 18, Batch 69/100] Loss: 0.2099 | RMSE: 0.1234 | Consistency: 0.0591 | SSIM: 0.8589 | Grad: 104380.445\n",
      "[Epoch 18, Batch 70/100] Loss: 0.2164 | RMSE: 0.1296 | Consistency: 0.0585 | SSIM: 0.8616 | Grad: 138496.969\n",
      "[Epoch 18, Batch 71/100] Loss: 0.2051 | RMSE: 0.1186 | Consistency: 0.0565 | SSIM: 0.8590 | Grad: 98293.211\n",
      "[Epoch 18, Batch 72/100] Loss: 0.1995 | RMSE: 0.1129 | Consistency: 0.0559 | SSIM: 0.8605 | Grad: 8316.927\n",
      "[Epoch 18, Batch 73/100] Loss: 0.2056 | RMSE: 0.1189 | Consistency: 0.0578 | SSIM: 0.8610 | Grad: 80883.203\n",
      "[Epoch 18, Batch 74/100] Loss: 0.2111 | RMSE: 0.1243 | Consistency: 0.0689 | SSIM: 0.8615 | Grad: 73077.945\n",
      "[Epoch 18, Batch 75/100] Loss: 0.2097 | RMSE: 0.1231 | Consistency: 0.0666 | SSIM: 0.8596 | Grad: 53252.543\n",
      "[Epoch 18, Batch 76/100] Loss: 0.2084 | RMSE: 0.1217 | Consistency: 0.0646 | SSIM: 0.8608 | Grad: 52910.426\n",
      "[Epoch 18, Batch 77/100] Loss: 0.2139 | RMSE: 0.1271 | Consistency: 0.0679 | SSIM: 0.8609 | Grad: 80360.633\n",
      "[Epoch 18, Batch 78/100] Loss: 0.2050 | RMSE: 0.1182 | Consistency: 0.0605 | SSIM: 0.8613 | Grad: 25794.148\n",
      "[Epoch 18, Batch 79/100] Loss: 0.2067 | RMSE: 0.1199 | Consistency: 0.0581 | SSIM: 0.8618 | Grad: 73388.539\n",
      "[Epoch 18, Batch 80/100] Loss: 0.2073 | RMSE: 0.1206 | Consistency: 0.0620 | SSIM: 0.8607 | Grad: 58612.480\n",
      "[Epoch 18, Batch 81/100] Loss: 0.2041 | RMSE: 0.1174 | Consistency: 0.0604 | SSIM: 0.8607 | Grad: 32266.971\n",
      "[Epoch 18, Batch 82/100] Loss: 0.2070 | RMSE: 0.1203 | Consistency: 0.0614 | SSIM: 0.8612 | Grad: 63051.188\n",
      "[Epoch 18, Batch 83/100] Loss: 0.2125 | RMSE: 0.1260 | Consistency: 0.0650 | SSIM: 0.8595 | Grad: 108450.250\n",
      "[Epoch 18, Batch 84/100] Loss: 0.2128 | RMSE: 0.1261 | Consistency: 0.0690 | SSIM: 0.8601 | Grad: 85853.242\n",
      "[Epoch 18, Batch 85/100] Loss: 0.2040 | RMSE: 0.1175 | Consistency: 0.0573 | SSIM: 0.8586 | Grad: 38061.488\n",
      "[Epoch 18, Batch 86/100] Loss: 0.2039 | RMSE: 0.1173 | Consistency: 0.0612 | SSIM: 0.8596 | Grad: 48261.391\n",
      "[Epoch 18, Batch 87/100] Loss: 0.2015 | RMSE: 0.1148 | Consistency: 0.0568 | SSIM: 0.8611 | Grad: 54021.441\n",
      "[Epoch 18, Batch 88/100] Loss: 0.2168 | RMSE: 0.1298 | Consistency: 0.0710 | SSIM: 0.8633 | Grad: 108382.898\n",
      "[Epoch 18, Batch 89/100] Loss: 0.2120 | RMSE: 0.1253 | Consistency: 0.0740 | SSIM: 0.8593 | Grad: 107513.039\n",
      "[Epoch 18, Batch 90/100] Loss: 0.2049 | RMSE: 0.1181 | Consistency: 0.0606 | SSIM: 0.8619 | Grad: 66403.648\n",
      "[Epoch 18, Batch 91/100] Loss: 0.2017 | RMSE: 0.1151 | Consistency: 0.0579 | SSIM: 0.8597 | Grad: 37472.789\n",
      "[Epoch 18, Batch 92/100] Loss: 0.2045 | RMSE: 0.1179 | Consistency: 0.0620 | SSIM: 0.8597 | Grad: 77731.953\n",
      "[Epoch 18, Batch 93/100] Loss: 0.2139 | RMSE: 0.1269 | Consistency: 0.0644 | SSIM: 0.8637 | Grad: 87709.398\n",
      "[Epoch 18, Batch 94/100] Loss: 0.2093 | RMSE: 0.1226 | Consistency: 0.0611 | SSIM: 0.8609 | Grad: 67219.461\n",
      "[Epoch 18, Batch 95/100] Loss: 0.2035 | RMSE: 0.1168 | Consistency: 0.0582 | SSIM: 0.8612 | Grad: 29854.428\n",
      "[Epoch 18, Batch 96/100] Loss: 0.2102 | RMSE: 0.1234 | Consistency: 0.0601 | SSIM: 0.8617 | Grad: 90990.445\n",
      "[Epoch 18, Batch 97/100] Loss: 0.2195 | RMSE: 0.1327 | Consistency: 0.0589 | SSIM: 0.8627 | Grad: 114607.133\n",
      "[Epoch 18, Batch 98/100] Loss: 0.2018 | RMSE: 0.1151 | Consistency: 0.0576 | SSIM: 0.8609 | Grad: 64909.719\n",
      "[Epoch 18, Batch 99/100] Loss: 0.2045 | RMSE: 0.1178 | Consistency: 0.0621 | SSIM: 0.8609 | Grad: 37120.078\n",
      "[Epoch 18, Batch 100/100] Loss: 0.2075 | RMSE: 0.1208 | Consistency: 0.0590 | SSIM: 0.8608 | Grad: 91433.547\n",
      "✅ Epoch 18 완료!\n",
      "   📈 평균 Loss: 0.206861\n",
      "   📊 평균 RMSE: 0.120199\n",
      "   ⚙️  학습률: 0.000953\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 19/80 시작...\n",
      "[Epoch 19, Batch 1/100] Loss: 0.2120 | RMSE: 0.1251 | Consistency: 0.0716 | SSIM: 0.8614 | Grad: 50089.414\n",
      "[Epoch 19, Batch 2/100] Loss: 0.2043 | RMSE: 0.1178 | Consistency: 0.0632 | SSIM: 0.8584 | Grad: 75298.844\n",
      "[Epoch 19, Batch 3/100] Loss: 0.2104 | RMSE: 0.1238 | Consistency: 0.0602 | SSIM: 0.8600 | Grad: 83310.000\n",
      "[Epoch 19, Batch 4/100] Loss: 0.2075 | RMSE: 0.1209 | Consistency: 0.0669 | SSIM: 0.8587 | Grad: 67943.164\n",
      "[Epoch 19, Batch 5/100] Loss: 0.1963 | RMSE: 0.1100 | Consistency: 0.0501 | SSIM: 0.8582 | Grad: 36970.883\n",
      "[Epoch 19, Batch 6/100] Loss: 0.2035 | RMSE: 0.1169 | Consistency: 0.0584 | SSIM: 0.8600 | Grad: 19504.916\n",
      "[Epoch 19, Batch 7/100] Loss: 0.2048 | RMSE: 0.1182 | Consistency: 0.0609 | SSIM: 0.8600 | Grad: 31652.613\n",
      "[Epoch 19, Batch 8/100] Loss: 0.2021 | RMSE: 0.1155 | Consistency: 0.0576 | SSIM: 0.8600 | Grad: 60312.965\n",
      "[Epoch 19, Batch 9/100] Loss: 0.2027 | RMSE: 0.1161 | Consistency: 0.0607 | SSIM: 0.8594 | Grad: 79938.477\n",
      "[Epoch 19, Batch 10/100] Loss: 0.2057 | RMSE: 0.1191 | Consistency: 0.0607 | SSIM: 0.8602 | Grad: 62564.922\n",
      "[Epoch 19, Batch 11/100] Loss: 0.2007 | RMSE: 0.1143 | Consistency: 0.0527 | SSIM: 0.8587 | Grad: 20526.924\n",
      "[Epoch 19, Batch 12/100] Loss: 0.2058 | RMSE: 0.1192 | Consistency: 0.0591 | SSIM: 0.8602 | Grad: 74727.492\n",
      "[Epoch 19, Batch 13/100] Loss: 0.2077 | RMSE: 0.1211 | Consistency: 0.0621 | SSIM: 0.8602 | Grad: 82426.836\n",
      "[Epoch 19, Batch 14/100] Loss: 0.2135 | RMSE: 0.1265 | Consistency: 0.0711 | SSIM: 0.8629 | Grad: 25733.457\n",
      "[Epoch 19, Batch 15/100] Loss: 0.2040 | RMSE: 0.1175 | Consistency: 0.0598 | SSIM: 0.8593 | Grad: 28417.174\n",
      "[Epoch 19, Batch 16/100] Loss: 0.2078 | RMSE: 0.1211 | Consistency: 0.0607 | SSIM: 0.8612 | Grad: 60140.293\n",
      "[Epoch 19, Batch 17/100] Loss: 0.2081 | RMSE: 0.1213 | Consistency: 0.0633 | SSIM: 0.8614 | Grad: 51739.133\n",
      "[Epoch 19, Batch 18/100] Loss: 0.2030 | RMSE: 0.1164 | Consistency: 0.0581 | SSIM: 0.8599 | Grad: 29465.408\n",
      "[Epoch 19, Batch 19/100] Loss: 0.2000 | RMSE: 0.1136 | Consistency: 0.0553 | SSIM: 0.8593 | Grad: 51415.875\n",
      "[Epoch 19, Batch 20/100] Loss: 0.2071 | RMSE: 0.1206 | Consistency: 0.0637 | SSIM: 0.8585 | Grad: 66620.703\n",
      "[Epoch 19, Batch 21/100] Loss: 0.2075 | RMSE: 0.1210 | Consistency: 0.0577 | SSIM: 0.8585 | Grad: 97282.828\n",
      "[Epoch 19, Batch 22/100] Loss: 0.2063 | RMSE: 0.1196 | Consistency: 0.0606 | SSIM: 0.8614 | Grad: 55641.773\n",
      "[Epoch 19, Batch 23/100] Loss: 0.2036 | RMSE: 0.1169 | Consistency: 0.0619 | SSIM: 0.8602 | Grad: 64452.094\n",
      "[Epoch 19, Batch 24/100] Loss: 0.1998 | RMSE: 0.1133 | Consistency: 0.0549 | SSIM: 0.8593 | Grad: 18883.643\n",
      "[Epoch 19, Batch 25/100] Loss: 0.2024 | RMSE: 0.1159 | Consistency: 0.0600 | SSIM: 0.8590 | Grad: 26021.719\n",
      "[Epoch 19, Batch 26/100] Loss: 0.2049 | RMSE: 0.1184 | Consistency: 0.0589 | SSIM: 0.8591 | Grad: 25503.266\n",
      "[Epoch 19, Batch 27/100] Loss: 0.2044 | RMSE: 0.1179 | Consistency: 0.0592 | SSIM: 0.8588 | Grad: 71669.594\n",
      "[Epoch 19, Batch 28/100] Loss: 0.2043 | RMSE: 0.1178 | Consistency: 0.0576 | SSIM: 0.8593 | Grad: 57139.949\n",
      "[Epoch 19, Batch 29/100] Loss: 0.2013 | RMSE: 0.1148 | Consistency: 0.0547 | SSIM: 0.8604 | Grad: 30673.803\n",
      "[Epoch 19, Batch 30/100] Loss: 0.2012 | RMSE: 0.1147 | Consistency: 0.0559 | SSIM: 0.8595 | Grad: 52334.742\n",
      "[Epoch 19, Batch 31/100] Loss: 0.2062 | RMSE: 0.1195 | Consistency: 0.0634 | SSIM: 0.8611 | Grad: 57585.105\n",
      "[Epoch 19, Batch 32/100] Loss: 0.2018 | RMSE: 0.1152 | Consistency: 0.0572 | SSIM: 0.8604 | Grad: 35228.504\n",
      "[Epoch 19, Batch 33/100] Loss: 0.2041 | RMSE: 0.1175 | Consistency: 0.0577 | SSIM: 0.8600 | Grad: 59326.680\n",
      "[Epoch 19, Batch 34/100] Loss: 0.2056 | RMSE: 0.1189 | Consistency: 0.0634 | SSIM: 0.8599 | Grad: 60749.223\n",
      "[Epoch 19, Batch 35/100] Loss: 0.2017 | RMSE: 0.1152 | Consistency: 0.0592 | SSIM: 0.8589 | Grad: 39938.730\n",
      "[Epoch 19, Batch 36/100] Loss: 0.2015 | RMSE: 0.1154 | Consistency: 0.0571 | SSIM: 0.8560 | Grad: 14843.738\n",
      "[Epoch 19, Batch 37/100] Loss: 0.2036 | RMSE: 0.1170 | Consistency: 0.0603 | SSIM: 0.8597 | Grad: 37831.734\n",
      "[Epoch 19, Batch 38/100] Loss: 0.2010 | RMSE: 0.1146 | Consistency: 0.0562 | SSIM: 0.8577 | Grad: 16528.211\n",
      "[Epoch 19, Batch 39/100] Loss: 0.2003 | RMSE: 0.1140 | Consistency: 0.0541 | SSIM: 0.8578 | Grad: 46323.496\n",
      "[Epoch 19, Batch 40/100] Loss: 0.2060 | RMSE: 0.1193 | Consistency: 0.0593 | SSIM: 0.8606 | Grad: 37701.512\n",
      "[Epoch 19, Batch 41/100] Loss: 0.2043 | RMSE: 0.1177 | Consistency: 0.0617 | SSIM: 0.8598 | Grad: 51978.488\n",
      "[Epoch 19, Batch 42/100] Loss: 0.2053 | RMSE: 0.1186 | Consistency: 0.0624 | SSIM: 0.8616 | Grad: 63599.629\n",
      "[Epoch 19, Batch 43/100] Loss: 0.2069 | RMSE: 0.1200 | Consistency: 0.0612 | SSIM: 0.8627 | Grad: 54596.719\n",
      "[Epoch 19, Batch 44/100] Loss: 0.2015 | RMSE: 0.1152 | Consistency: 0.0571 | SSIM: 0.8571 | Grad: 45512.891\n",
      "[Epoch 19, Batch 45/100] Loss: 0.2013 | RMSE: 0.1150 | Consistency: 0.0575 | SSIM: 0.8578 | Grad: 25255.779\n",
      "[Epoch 19, Batch 46/100] Loss: 0.2023 | RMSE: 0.1159 | Consistency: 0.0540 | SSIM: 0.8586 | Grad: 24184.439\n",
      "[Epoch 19, Batch 47/100] Loss: 0.2048 | RMSE: 0.1181 | Consistency: 0.0592 | SSIM: 0.8611 | Grad: 37644.781\n",
      "[Epoch 19, Batch 48/100] Loss: 0.2018 | RMSE: 0.1153 | Consistency: 0.0568 | SSIM: 0.8588 | Grad: 50999.254\n",
      "[Epoch 19, Batch 49/100] Loss: 0.2003 | RMSE: 0.1138 | Consistency: 0.0571 | SSIM: 0.8587 | Grad: 44941.852\n",
      "[Epoch 19, Batch 50/100] Loss: 0.2036 | RMSE: 0.1171 | Consistency: 0.0579 | SSIM: 0.8583 | Grad: 67459.227\n",
      "[Epoch 19, Batch 51/100] Loss: 0.2034 | RMSE: 0.1168 | Consistency: 0.0596 | SSIM: 0.8606 | Grad: 29246.986\n",
      "[Epoch 19, Batch 52/100] Loss: 0.2066 | RMSE: 0.1201 | Consistency: 0.0615 | SSIM: 0.8590 | Grad: 89349.836\n",
      "[Epoch 19, Batch 53/100] Loss: 0.2142 | RMSE: 0.1278 | Consistency: 0.0660 | SSIM: 0.8575 | Grad: 89420.461\n",
      "[Epoch 19, Batch 54/100] Loss: 0.2028 | RMSE: 0.1162 | Consistency: 0.0579 | SSIM: 0.8606 | Grad: 40546.289\n",
      "[Epoch 19, Batch 55/100] Loss: 0.2058 | RMSE: 0.1192 | Consistency: 0.0616 | SSIM: 0.8594 | Grad: 22314.287\n",
      "[Epoch 19, Batch 56/100] Loss: 0.2046 | RMSE: 0.1182 | Consistency: 0.0594 | SSIM: 0.8580 | Grad: 79524.047\n",
      "[Epoch 19, Batch 57/100] Loss: 0.2095 | RMSE: 0.1227 | Consistency: 0.0646 | SSIM: 0.8612 | Grad: 84854.516\n",
      "[Epoch 19, Batch 58/100] Loss: 0.2001 | RMSE: 0.1135 | Consistency: 0.0557 | SSIM: 0.8602 | Grad: 30621.480\n",
      "[Epoch 19, Batch 59/100] Loss: 0.2093 | RMSE: 0.1225 | Consistency: 0.0686 | SSIM: 0.8608 | Grad: 73026.180\n",
      "[Epoch 19, Batch 60/100] Loss: 0.2069 | RMSE: 0.1202 | Consistency: 0.0593 | SSIM: 0.8615 | Grad: 69941.172\n",
      "[Epoch 19, Batch 61/100] Loss: 0.2033 | RMSE: 0.1166 | Consistency: 0.0584 | SSIM: 0.8604 | Grad: 58486.359\n",
      "[Epoch 19, Batch 62/100] Loss: 0.2086 | RMSE: 0.1219 | Consistency: 0.0606 | SSIM: 0.8601 | Grad: 35964.598\n",
      "[Epoch 19, Batch 63/100] Loss: 0.2008 | RMSE: 0.1145 | Consistency: 0.0562 | SSIM: 0.8577 | Grad: 26879.996\n",
      "[Epoch 19, Batch 64/100] Loss: 0.2056 | RMSE: 0.1190 | Consistency: 0.0604 | SSIM: 0.8602 | Grad: 26251.123\n",
      "[Epoch 19, Batch 65/100] Loss: 0.2072 | RMSE: 0.1206 | Consistency: 0.0614 | SSIM: 0.8594 | Grad: 72575.523\n",
      "[Epoch 19, Batch 66/100] Loss: 0.2052 | RMSE: 0.1187 | Consistency: 0.0609 | SSIM: 0.8589 | Grad: 70878.742\n",
      "[Epoch 19, Batch 67/100] Loss: 0.2029 | RMSE: 0.1163 | Consistency: 0.0574 | SSIM: 0.8601 | Grad: 17107.596\n",
      "[Epoch 19, Batch 68/100] Loss: 0.2023 | RMSE: 0.1157 | Consistency: 0.0548 | SSIM: 0.8604 | Grad: 35900.234\n",
      "[Epoch 19, Batch 69/100] Loss: 0.2077 | RMSE: 0.1210 | Consistency: 0.0605 | SSIM: 0.8605 | Grad: 74732.031\n",
      "[Epoch 19, Batch 70/100] Loss: 0.2071 | RMSE: 0.1203 | Consistency: 0.0665 | SSIM: 0.8610 | Grad: 62832.480\n",
      "[Epoch 19, Batch 71/100] Loss: 0.2028 | RMSE: 0.1164 | Consistency: 0.0580 | SSIM: 0.8586 | Grad: 31873.080\n",
      "[Epoch 19, Batch 72/100] Loss: 0.2073 | RMSE: 0.1207 | Consistency: 0.0600 | SSIM: 0.8607 | Grad: 49081.188\n",
      "[Epoch 19, Batch 73/100] Loss: 0.2090 | RMSE: 0.1220 | Consistency: 0.0629 | SSIM: 0.8627 | Grad: 57683.980\n",
      "[Epoch 19, Batch 74/100] Loss: 0.2113 | RMSE: 0.1245 | Consistency: 0.0660 | SSIM: 0.8611 | Grad: 80714.031\n",
      "[Epoch 19, Batch 75/100] Loss: 0.2123 | RMSE: 0.1256 | Consistency: 0.0599 | SSIM: 0.8611 | Grad: 87967.117\n",
      "[Epoch 19, Batch 76/100] Loss: 0.2063 | RMSE: 0.1195 | Consistency: 0.0592 | SSIM: 0.8617 | Grad: 47946.824\n",
      "[Epoch 19, Batch 77/100] Loss: 0.2047 | RMSE: 0.1182 | Consistency: 0.0584 | SSIM: 0.8593 | Grad: 58698.723\n",
      "[Epoch 19, Batch 78/100] Loss: 0.2049 | RMSE: 0.1183 | Consistency: 0.0570 | SSIM: 0.8605 | Grad: 42530.098\n",
      "[Epoch 19, Batch 79/100] Loss: 0.2010 | RMSE: 0.1144 | Consistency: 0.0558 | SSIM: 0.8608 | Grad: 65069.352\n",
      "[Epoch 19, Batch 80/100] Loss: 0.2078 | RMSE: 0.1208 | Consistency: 0.0615 | SSIM: 0.8631 | Grad: 74686.742\n",
      "[Epoch 19, Batch 81/100] Loss: 0.2014 | RMSE: 0.1146 | Consistency: 0.0586 | SSIM: 0.8621 | Grad: 15919.554\n",
      "[Epoch 19, Batch 82/100] Loss: 0.2031 | RMSE: 0.1165 | Consistency: 0.0594 | SSIM: 0.8593 | Grad: 51502.406\n",
      "[Epoch 19, Batch 83/100] Loss: 0.2062 | RMSE: 0.1195 | Consistency: 0.0594 | SSIM: 0.8610 | Grad: 59568.035\n",
      "[Epoch 19, Batch 84/100] Loss: 0.2038 | RMSE: 0.1173 | Consistency: 0.0588 | SSIM: 0.8582 | Grad: 55366.281\n",
      "[Epoch 19, Batch 85/100] Loss: 0.2027 | RMSE: 0.1164 | Consistency: 0.0581 | SSIM: 0.8577 | Grad: 27000.809\n",
      "[Epoch 19, Batch 86/100] Loss: 0.2068 | RMSE: 0.1202 | Consistency: 0.0601 | SSIM: 0.8600 | Grad: 39204.934\n",
      "[Epoch 19, Batch 87/100] Loss: 0.2052 | RMSE: 0.1186 | Consistency: 0.0602 | SSIM: 0.8600 | Grad: 70576.609\n",
      "[Epoch 19, Batch 88/100] Loss: 0.2034 | RMSE: 0.1170 | Consistency: 0.0559 | SSIM: 0.8586 | Grad: 69471.211\n",
      "[Epoch 19, Batch 89/100] Loss: 0.2081 | RMSE: 0.1214 | Consistency: 0.0625 | SSIM: 0.8599 | Grad: 26486.785\n",
      "[Epoch 19, Batch 90/100] Loss: 0.2006 | RMSE: 0.1140 | Consistency: 0.0551 | SSIM: 0.8606 | Grad: 41404.020\n",
      "[Epoch 19, Batch 91/100] Loss: 0.2057 | RMSE: 0.1191 | Consistency: 0.0577 | SSIM: 0.8602 | Grad: 68529.961\n",
      "[Epoch 19, Batch 92/100] Loss: 0.2182 | RMSE: 0.1313 | Consistency: 0.0732 | SSIM: 0.8615 | Grad: 57350.688\n",
      "[Epoch 19, Batch 93/100] Loss: 0.2077 | RMSE: 0.1209 | Consistency: 0.0680 | SSIM: 0.8611 | Grad: 69137.242\n",
      "[Epoch 19, Batch 94/100] Loss: 0.2036 | RMSE: 0.1169 | Consistency: 0.0573 | SSIM: 0.8607 | Grad: 61439.844\n",
      "[Epoch 19, Batch 95/100] Loss: 0.2021 | RMSE: 0.1155 | Consistency: 0.0580 | SSIM: 0.8608 | Grad: 39166.871\n",
      "[Epoch 19, Batch 96/100] Loss: 0.2068 | RMSE: 0.1201 | Consistency: 0.0616 | SSIM: 0.8606 | Grad: 61951.605\n",
      "[Epoch 19, Batch 97/100] Loss: 0.2078 | RMSE: 0.1209 | Consistency: 0.0616 | SSIM: 0.8626 | Grad: 66229.055\n",
      "[Epoch 19, Batch 98/100] Loss: 0.2058 | RMSE: 0.1192 | Consistency: 0.0611 | SSIM: 0.8591 | Grad: 55009.020\n",
      "[Epoch 19, Batch 99/100] Loss: 0.2065 | RMSE: 0.1200 | Consistency: 0.0606 | SSIM: 0.8590 | Grad: 47757.266\n",
      "[Epoch 19, Batch 100/100] Loss: 0.2122 | RMSE: 0.1254 | Consistency: 0.0621 | SSIM: 0.8615 | Grad: 96527.047\n",
      "✅ Epoch 19 완료!\n",
      "   📈 평균 Loss: 0.205050\n",
      "   📊 평균 RMSE: 0.118460\n",
      "   ⚙️  학습률: 0.000943\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 20/80 시작...\n",
      "[Epoch 20, Batch 1/100] Loss: 0.1986 | RMSE: 0.1124 | Consistency: 0.0527 | SSIM: 0.8569 | Grad: 46502.895\n",
      "[Epoch 20, Batch 2/100] Loss: 0.2037 | RMSE: 0.1173 | Consistency: 0.0582 | SSIM: 0.8583 | Grad: 47153.969\n",
      "[Epoch 20, Batch 3/100] Loss: 0.2045 | RMSE: 0.1179 | Consistency: 0.0595 | SSIM: 0.8606 | Grad: 25365.184\n",
      "[Epoch 20, Batch 4/100] Loss: 0.2051 | RMSE: 0.1186 | Consistency: 0.0595 | SSIM: 0.8591 | Grad: 69972.820\n",
      "[Epoch 20, Batch 5/100] Loss: 0.2018 | RMSE: 0.1151 | Consistency: 0.0566 | SSIM: 0.8607 | Grad: 32940.910\n",
      "[Epoch 20, Batch 6/100] Loss: 0.2057 | RMSE: 0.1191 | Consistency: 0.0583 | SSIM: 0.8603 | Grad: 48040.879\n",
      "[Epoch 20, Batch 7/100] Loss: 0.2100 | RMSE: 0.1232 | Consistency: 0.0614 | SSIM: 0.8615 | Grad: 56520.645\n",
      "[Epoch 20, Batch 8/100] Loss: 0.2070 | RMSE: 0.1202 | Consistency: 0.0583 | SSIM: 0.8617 | Grad: 63459.289\n",
      "[Epoch 20, Batch 9/100] Loss: 0.2015 | RMSE: 0.1150 | Consistency: 0.0545 | SSIM: 0.8597 | Grad: 46638.906\n",
      "[Epoch 20, Batch 10/100] Loss: 0.2072 | RMSE: 0.1204 | Consistency: 0.0583 | SSIM: 0.8617 | Grad: 58487.859\n",
      "[Epoch 20, Batch 11/100] Loss: 0.2087 | RMSE: 0.1219 | Consistency: 0.0669 | SSIM: 0.8610 | Grad: 89096.461\n",
      "[Epoch 20, Batch 12/100] Loss: 0.2079 | RMSE: 0.1214 | Consistency: 0.0678 | SSIM: 0.8575 | Grad: 66693.883\n",
      "[Epoch 20, Batch 13/100] Loss: 0.2049 | RMSE: 0.1182 | Consistency: 0.0639 | SSIM: 0.8612 | Grad: 53967.309\n",
      "[Epoch 20, Batch 14/100] Loss: 0.2101 | RMSE: 0.1234 | Consistency: 0.0626 | SSIM: 0.8613 | Grad: 75161.273\n",
      "[Epoch 20, Batch 15/100] Loss: 0.2075 | RMSE: 0.1207 | Consistency: 0.0568 | SSIM: 0.8624 | Grad: 68786.406\n",
      "[Epoch 20, Batch 16/100] Loss: 0.2092 | RMSE: 0.1225 | Consistency: 0.0618 | SSIM: 0.8612 | Grad: 70064.523\n",
      "[Epoch 20, Batch 17/100] Loss: 0.2019 | RMSE: 0.1155 | Consistency: 0.0549 | SSIM: 0.8592 | Grad: 64571.684\n",
      "[Epoch 20, Batch 18/100] Loss: 0.2025 | RMSE: 0.1161 | Consistency: 0.0583 | SSIM: 0.8580 | Grad: 70021.883\n",
      "[Epoch 20, Batch 19/100] Loss: 0.2121 | RMSE: 0.1255 | Consistency: 0.0631 | SSIM: 0.8596 | Grad: 87128.125\n",
      "[Epoch 20, Batch 20/100] Loss: 0.2072 | RMSE: 0.1206 | Consistency: 0.0588 | SSIM: 0.8603 | Grad: 75794.211\n",
      "[Epoch 20, Batch 21/100] Loss: 0.2014 | RMSE: 0.1149 | Consistency: 0.0576 | SSIM: 0.8584 | Grad: 19895.977\n",
      "[Epoch 20, Batch 22/100] Loss: 0.2027 | RMSE: 0.1161 | Consistency: 0.0583 | SSIM: 0.8605 | Grad: 63774.281\n",
      "[Epoch 20, Batch 23/100] Loss: 0.2044 | RMSE: 0.1178 | Consistency: 0.0589 | SSIM: 0.8593 | Grad: 59049.793\n",
      "[Epoch 20, Batch 24/100] Loss: 0.2078 | RMSE: 0.1209 | Consistency: 0.0685 | SSIM: 0.8617 | Grad: 40576.543\n",
      "[Epoch 20, Batch 25/100] Loss: 0.2008 | RMSE: 0.1142 | Consistency: 0.0588 | SSIM: 0.8604 | Grad: 43063.254\n",
      "[Epoch 20, Batch 26/100] Loss: 0.2055 | RMSE: 0.1187 | Consistency: 0.0606 | SSIM: 0.8615 | Grad: 38710.359\n",
      "[Epoch 20, Batch 27/100] Loss: 0.2003 | RMSE: 0.1138 | Consistency: 0.0538 | SSIM: 0.8594 | Grad: 24501.361\n",
      "[Epoch 20, Batch 28/100] Loss: 0.2024 | RMSE: 0.1157 | Consistency: 0.0577 | SSIM: 0.8618 | Grad: 58814.289\n",
      "[Epoch 20, Batch 29/100] Loss: 0.2048 | RMSE: 0.1181 | Consistency: 0.0556 | SSIM: 0.8614 | Grad: 28304.500\n",
      "[Epoch 20, Batch 30/100] Loss: 0.2109 | RMSE: 0.1239 | Consistency: 0.0685 | SSIM: 0.8631 | Grad: 36581.574\n",
      "[Epoch 20, Batch 31/100] Loss: 0.2036 | RMSE: 0.1171 | Consistency: 0.0560 | SSIM: 0.8597 | Grad: 32397.008\n",
      "[Epoch 20, Batch 32/100] Loss: 0.1969 | RMSE: 0.1105 | Consistency: 0.0528 | SSIM: 0.8584 | Grad: 17697.643\n",
      "[Epoch 20, Batch 33/100] Loss: 0.2013 | RMSE: 0.1150 | Consistency: 0.0542 | SSIM: 0.8573 | Grad: 29321.406\n",
      "[Epoch 20, Batch 34/100] Loss: 0.2023 | RMSE: 0.1159 | Consistency: 0.0599 | SSIM: 0.8583 | Grad: 33976.488\n",
      "[Epoch 20, Batch 35/100] Loss: 0.1962 | RMSE: 0.1100 | Consistency: 0.0525 | SSIM: 0.8569 | Grad: 18167.273\n",
      "[Epoch 20, Batch 36/100] Loss: 0.2067 | RMSE: 0.1201 | Consistency: 0.0612 | SSIM: 0.8596 | Grad: 68689.289\n",
      "[Epoch 20, Batch 37/100] Loss: 0.2032 | RMSE: 0.1164 | Consistency: 0.0585 | SSIM: 0.8623 | Grad: 27163.748\n",
      "[Epoch 20, Batch 38/100] Loss: 0.2062 | RMSE: 0.1194 | Consistency: 0.0599 | SSIM: 0.8618 | Grad: 19301.010\n",
      "[Epoch 20, Batch 39/100] Loss: 0.2113 | RMSE: 0.1245 | Consistency: 0.0625 | SSIM: 0.8619 | Grad: 88000.000\n",
      "[Epoch 20, Batch 40/100] Loss: 0.2133 | RMSE: 0.1264 | Consistency: 0.0629 | SSIM: 0.8626 | Grad: 74748.609\n",
      "[Epoch 20, Batch 41/100] Loss: 0.2008 | RMSE: 0.1142 | Consistency: 0.0561 | SSIM: 0.8602 | Grad: 25866.553\n",
      "[Epoch 20, Batch 42/100] Loss: 0.2119 | RMSE: 0.1248 | Consistency: 0.0708 | SSIM: 0.8633 | Grad: 51120.598\n",
      "[Epoch 20, Batch 43/100] Loss: 0.2155 | RMSE: 0.1287 | Consistency: 0.0648 | SSIM: 0.8617 | Grad: 91753.648\n",
      "[Epoch 20, Batch 44/100] Loss: 0.2088 | RMSE: 0.1219 | Consistency: 0.0609 | SSIM: 0.8629 | Grad: 72384.414\n",
      "[Epoch 20, Batch 45/100] Loss: 0.2051 | RMSE: 0.1183 | Consistency: 0.0627 | SSIM: 0.8623 | Grad: 63385.762\n",
      "[Epoch 20, Batch 46/100] Loss: 0.2088 | RMSE: 0.1220 | Consistency: 0.0566 | SSIM: 0.8622 | Grad: 89028.383\n",
      "[Epoch 20, Batch 47/100] Loss: 0.2039 | RMSE: 0.1171 | Consistency: 0.0563 | SSIM: 0.8621 | Grad: 40999.527\n",
      "[Epoch 20, Batch 48/100] Loss: 0.2022 | RMSE: 0.1156 | Consistency: 0.0585 | SSIM: 0.8600 | Grad: 33090.035\n",
      "[Epoch 20, Batch 49/100] Loss: 0.2087 | RMSE: 0.1221 | Consistency: 0.0599 | SSIM: 0.8595 | Grad: 73983.172\n",
      "[Epoch 20, Batch 50/100] Loss: 0.2024 | RMSE: 0.1158 | Consistency: 0.0593 | SSIM: 0.8600 | Grad: 38069.801\n",
      "[Epoch 20, Batch 51/100] Loss: 0.2016 | RMSE: 0.1150 | Consistency: 0.0557 | SSIM: 0.8603 | Grad: 22493.320\n",
      "[Epoch 20, Batch 52/100] Loss: 0.2017 | RMSE: 0.1153 | Consistency: 0.0572 | SSIM: 0.8589 | Grad: 37078.145\n",
      "[Epoch 20, Batch 53/100] Loss: 0.2033 | RMSE: 0.1166 | Consistency: 0.0584 | SSIM: 0.8612 | Grad: 45370.648\n",
      "[Epoch 20, Batch 54/100] Loss: 0.2040 | RMSE: 0.1173 | Consistency: 0.0615 | SSIM: 0.8608 | Grad: 41401.117\n",
      "[Epoch 20, Batch 55/100] Loss: 0.2034 | RMSE: 0.1169 | Consistency: 0.0561 | SSIM: 0.8599 | Grad: 31146.559\n",
      "[Epoch 20, Batch 56/100] Loss: 0.2071 | RMSE: 0.1207 | Consistency: 0.0594 | SSIM: 0.8580 | Grad: 75914.023\n",
      "[Epoch 20, Batch 57/100] Loss: 0.2182 | RMSE: 0.1317 | Consistency: 0.0657 | SSIM: 0.8591 | Grad: 121835.828\n",
      "[Epoch 20, Batch 58/100] Loss: 0.2154 | RMSE: 0.1288 | Consistency: 0.0595 | SSIM: 0.8594 | Grad: 114285.312\n",
      "[Epoch 20, Batch 59/100] Loss: 0.2015 | RMSE: 0.1152 | Consistency: 0.0556 | SSIM: 0.8569 | Grad: 41669.598\n",
      "[Epoch 20, Batch 60/100] Loss: 0.1996 | RMSE: 0.1133 | Consistency: 0.0537 | SSIM: 0.8576 | Grad: 32378.576\n",
      "[Epoch 20, Batch 61/100] Loss: 0.2097 | RMSE: 0.1231 | Consistency: 0.0650 | SSIM: 0.8588 | Grad: 71616.781\n",
      "[Epoch 20, Batch 62/100] Loss: 0.2029 | RMSE: 0.1163 | Consistency: 0.0634 | SSIM: 0.8593 | Grad: 53081.477\n",
      "[Epoch 20, Batch 63/100] Loss: 0.2044 | RMSE: 0.1179 | Consistency: 0.0597 | SSIM: 0.8586 | Grad: 30164.309\n",
      "[Epoch 20, Batch 64/100] Loss: 0.2156 | RMSE: 0.1287 | Consistency: 0.0761 | SSIM: 0.8615 | Grad: 105117.242\n",
      "[Epoch 20, Batch 65/100] Loss: 0.2127 | RMSE: 0.1260 | Consistency: 0.0686 | SSIM: 0.8600 | Grad: 99402.336\n",
      "[Epoch 20, Batch 66/100] Loss: 0.2040 | RMSE: 0.1175 | Consistency: 0.0624 | SSIM: 0.8588 | Grad: 58727.617\n",
      "[Epoch 20, Batch 67/100] Loss: 0.2097 | RMSE: 0.1231 | Consistency: 0.0594 | SSIM: 0.8594 | Grad: 62259.789\n",
      "[Epoch 20, Batch 68/100] Loss: 0.2092 | RMSE: 0.1222 | Consistency: 0.0674 | SSIM: 0.8624 | Grad: 92451.422\n",
      "[Epoch 20, Batch 69/100] Loss: 0.2046 | RMSE: 0.1180 | Consistency: 0.0634 | SSIM: 0.8600 | Grad: 57534.852\n",
      "[Epoch 20, Batch 70/100] Loss: 0.2121 | RMSE: 0.1253 | Consistency: 0.0678 | SSIM: 0.8613 | Grad: 68443.219\n",
      "[Epoch 20, Batch 71/100] Loss: 0.2092 | RMSE: 0.1225 | Consistency: 0.0648 | SSIM: 0.8605 | Grad: 54026.234\n",
      "[Epoch 20, Batch 72/100] Loss: 0.2076 | RMSE: 0.1207 | Consistency: 0.0681 | SSIM: 0.8623 | Grad: 76263.656\n",
      "[Epoch 20, Batch 73/100] Loss: 0.2067 | RMSE: 0.1201 | Consistency: 0.0621 | SSIM: 0.8603 | Grad: 74834.109\n",
      "[Epoch 20, Batch 74/100] Loss: 0.2015 | RMSE: 0.1150 | Consistency: 0.0577 | SSIM: 0.8594 | Grad: 66141.773\n",
      "[Epoch 20, Batch 75/100] Loss: 0.2032 | RMSE: 0.1165 | Consistency: 0.0608 | SSIM: 0.8610 | Grad: 61382.645\n",
      "[Epoch 20, Batch 76/100] Loss: 0.2042 | RMSE: 0.1177 | Consistency: 0.0594 | SSIM: 0.8593 | Grad: 39925.430\n",
      "[Epoch 20, Batch 77/100] Loss: 0.2029 | RMSE: 0.1164 | Consistency: 0.0572 | SSIM: 0.8598 | Grad: 38720.430\n",
      "[Epoch 20, Batch 78/100] Loss: 0.2040 | RMSE: 0.1173 | Consistency: 0.0575 | SSIM: 0.8605 | Grad: 57039.477\n",
      "[Epoch 20, Batch 79/100] Loss: 0.2064 | RMSE: 0.1197 | Consistency: 0.0616 | SSIM: 0.8615 | Grad: 62977.816\n",
      "[Epoch 20, Batch 80/100] Loss: 0.2015 | RMSE: 0.1148 | Consistency: 0.0575 | SSIM: 0.8611 | Grad: 39906.746\n",
      "[Epoch 20, Batch 81/100] Loss: 0.2037 | RMSE: 0.1173 | Consistency: 0.0565 | SSIM: 0.8586 | Grad: 46583.824\n",
      "[Epoch 20, Batch 82/100] Loss: 0.2027 | RMSE: 0.1163 | Consistency: 0.0585 | SSIM: 0.8581 | Grad: 62811.711\n",
      "[Epoch 20, Batch 83/100] Loss: 0.2054 | RMSE: 0.1187 | Consistency: 0.0619 | SSIM: 0.8605 | Grad: 55729.863\n",
      "[Epoch 20, Batch 84/100] Loss: 0.2009 | RMSE: 0.1144 | Consistency: 0.0544 | SSIM: 0.8591 | Grad: 60707.859\n",
      "[Epoch 20, Batch 85/100] Loss: 0.2059 | RMSE: 0.1194 | Consistency: 0.0567 | SSIM: 0.8598 | Grad: 70363.781\n",
      "[Epoch 20, Batch 86/100] Loss: 0.2058 | RMSE: 0.1192 | Consistency: 0.0595 | SSIM: 0.8602 | Grad: 79597.938\n",
      "[Epoch 20, Batch 87/100] Loss: 0.1999 | RMSE: 0.1137 | Consistency: 0.0561 | SSIM: 0.8567 | Grad: 41228.500\n",
      "[Epoch 20, Batch 88/100] Loss: 0.1977 | RMSE: 0.1116 | Consistency: 0.0523 | SSIM: 0.8564 | Grad: 6534.153\n",
      "[Epoch 20, Batch 89/100] Loss: 0.2017 | RMSE: 0.1150 | Consistency: 0.0602 | SSIM: 0.8607 | Grad: 47071.648\n",
      "[Epoch 20, Batch 90/100] Loss: 0.2031 | RMSE: 0.1164 | Consistency: 0.0577 | SSIM: 0.8615 | Grad: 27584.213\n",
      "[Epoch 20, Batch 91/100] Loss: 0.2041 | RMSE: 0.1174 | Consistency: 0.0579 | SSIM: 0.8608 | Grad: 35598.211\n",
      "[Epoch 20, Batch 92/100] Loss: 0.2095 | RMSE: 0.1226 | Consistency: 0.0592 | SSIM: 0.8629 | Grad: 47794.652\n",
      "[Epoch 20, Batch 93/100] Loss: 0.2018 | RMSE: 0.1152 | Consistency: 0.0599 | SSIM: 0.8594 | Grad: 56701.586\n",
      "[Epoch 20, Batch 94/100] Loss: 0.2080 | RMSE: 0.1213 | Consistency: 0.0582 | SSIM: 0.8616 | Grad: 60716.348\n",
      "[Epoch 20, Batch 95/100] Loss: 0.2022 | RMSE: 0.1158 | Consistency: 0.0570 | SSIM: 0.8576 | Grad: 13893.646\n",
      "[Epoch 20, Batch 96/100] Loss: 0.2102 | RMSE: 0.1235 | Consistency: 0.0653 | SSIM: 0.8601 | Grad: 69157.469\n",
      "[Epoch 20, Batch 97/100] Loss: 0.2037 | RMSE: 0.1172 | Consistency: 0.0560 | SSIM: 0.8598 | Grad: 49554.477\n",
      "[Epoch 20, Batch 98/100] Loss: 0.2042 | RMSE: 0.1176 | Consistency: 0.0570 | SSIM: 0.8599 | Grad: 41612.871\n",
      "[Epoch 20, Batch 99/100] Loss: 0.1990 | RMSE: 0.1127 | Consistency: 0.0531 | SSIM: 0.8578 | Grad: 22187.080\n",
      "[Epoch 20, Batch 100/100] Loss: 0.2108 | RMSE: 0.1240 | Consistency: 0.0664 | SSIM: 0.8618 | Grad: 93950.523\n",
      "✅ Epoch 20 완료!\n",
      "   📈 평균 Loss: 0.205352\n",
      "   📊 평균 RMSE: 0.118740\n",
      "   ⚙️  학습률: 0.000933\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 21/80 시작...\n",
      "[Epoch 21, Batch 1/100] Loss: 0.2087 | RMSE: 0.1219 | Consistency: 0.0663 | SSIM: 0.8617 | Grad: 70085.016\n",
      "[Epoch 21, Batch 2/100] Loss: 0.2138 | RMSE: 0.1270 | Consistency: 0.0628 | SSIM: 0.8616 | Grad: 73053.938\n",
      "[Epoch 21, Batch 3/100] Loss: 0.2038 | RMSE: 0.1173 | Consistency: 0.0639 | SSIM: 0.8591 | Grad: 59888.887\n",
      "[Epoch 21, Batch 4/100] Loss: 0.2046 | RMSE: 0.1178 | Consistency: 0.0603 | SSIM: 0.8613 | Grad: 44466.496\n",
      "[Epoch 21, Batch 5/100] Loss: 0.2071 | RMSE: 0.1205 | Consistency: 0.0610 | SSIM: 0.8607 | Grad: 46680.527\n",
      "[Epoch 21, Batch 6/100] Loss: 0.2113 | RMSE: 0.1243 | Consistency: 0.0618 | SSIM: 0.8642 | Grad: 53777.840\n",
      "[Epoch 21, Batch 7/100] Loss: 0.2112 | RMSE: 0.1244 | Consistency: 0.0686 | SSIM: 0.8617 | Grad: 43055.625\n",
      "[Epoch 21, Batch 8/100] Loss: 0.2061 | RMSE: 0.1194 | Consistency: 0.0583 | SSIM: 0.8610 | Grad: 83710.320\n",
      "[Epoch 21, Batch 9/100] Loss: 0.2053 | RMSE: 0.1186 | Consistency: 0.0624 | SSIM: 0.8602 | Grad: 57757.719\n",
      "[Epoch 21, Batch 10/100] Loss: 0.2025 | RMSE: 0.1159 | Consistency: 0.0583 | SSIM: 0.8599 | Grad: 42252.891\n",
      "[Epoch 21, Batch 11/100] Loss: 0.2015 | RMSE: 0.1148 | Consistency: 0.0556 | SSIM: 0.8610 | Grad: 31205.736\n",
      "[Epoch 21, Batch 12/100] Loss: 0.2057 | RMSE: 0.1190 | Consistency: 0.0576 | SSIM: 0.8611 | Grad: 39655.445\n",
      "[Epoch 21, Batch 13/100] Loss: 0.2043 | RMSE: 0.1176 | Consistency: 0.0574 | SSIM: 0.8610 | Grad: 40847.293\n",
      "[Epoch 21, Batch 14/100] Loss: 0.2042 | RMSE: 0.1175 | Consistency: 0.0608 | SSIM: 0.8607 | Grad: 58947.719\n",
      "[Epoch 21, Batch 15/100] Loss: 0.2022 | RMSE: 0.1159 | Consistency: 0.0580 | SSIM: 0.8575 | Grad: 66814.195\n",
      "[Epoch 21, Batch 16/100] Loss: 0.2075 | RMSE: 0.1210 | Consistency: 0.0622 | SSIM: 0.8585 | Grad: 59509.844\n",
      "[Epoch 21, Batch 17/100] Loss: 0.2024 | RMSE: 0.1161 | Consistency: 0.0576 | SSIM: 0.8574 | Grad: 64852.953\n",
      "[Epoch 21, Batch 18/100] Loss: 0.2010 | RMSE: 0.1145 | Consistency: 0.0563 | SSIM: 0.8586 | Grad: 48584.906\n",
      "[Epoch 21, Batch 19/100] Loss: 0.2063 | RMSE: 0.1197 | Consistency: 0.0600 | SSIM: 0.8607 | Grad: 57959.965\n",
      "[Epoch 21, Batch 20/100] Loss: 0.2057 | RMSE: 0.1193 | Consistency: 0.0584 | SSIM: 0.8583 | Grad: 54800.266\n",
      "[Epoch 21, Batch 21/100] Loss: 0.2057 | RMSE: 0.1191 | Consistency: 0.0655 | SSIM: 0.8589 | Grad: 52298.297\n",
      "[Epoch 21, Batch 22/100] Loss: 0.2143 | RMSE: 0.1276 | Consistency: 0.0617 | SSIM: 0.8614 | Grad: 117239.109\n",
      "[Epoch 21, Batch 23/100] Loss: 0.2110 | RMSE: 0.1243 | Consistency: 0.0610 | SSIM: 0.8614 | Grad: 104593.508\n",
      "[Epoch 21, Batch 24/100] Loss: 0.2018 | RMSE: 0.1154 | Consistency: 0.0572 | SSIM: 0.8592 | Grad: 58524.711\n",
      "[Epoch 21, Batch 25/100] Loss: 0.2108 | RMSE: 0.1240 | Consistency: 0.0644 | SSIM: 0.8622 | Grad: 40713.168\n",
      "[Epoch 21, Batch 26/100] Loss: 0.2095 | RMSE: 0.1230 | Consistency: 0.0588 | SSIM: 0.8592 | Grad: 87642.883\n",
      "[Epoch 21, Batch 27/100] Loss: 0.2104 | RMSE: 0.1238 | Consistency: 0.0596 | SSIM: 0.8605 | Grad: 74740.289\n",
      "[Epoch 21, Batch 28/100] Loss: 0.2041 | RMSE: 0.1179 | Consistency: 0.0595 | SSIM: 0.8567 | Grad: 70163.547\n",
      "[Epoch 21, Batch 29/100] Loss: 0.2046 | RMSE: 0.1181 | Consistency: 0.0555 | SSIM: 0.8591 | Grad: 55122.457\n",
      "[Epoch 21, Batch 30/100] Loss: 0.2045 | RMSE: 0.1180 | Consistency: 0.0554 | SSIM: 0.8595 | Grad: 72423.945\n",
      "[Epoch 21, Batch 31/100] Loss: 0.2008 | RMSE: 0.1143 | Consistency: 0.0550 | SSIM: 0.8594 | Grad: 48949.621\n",
      "[Epoch 21, Batch 32/100] Loss: 0.2018 | RMSE: 0.1153 | Consistency: 0.0574 | SSIM: 0.8584 | Grad: 28107.805\n",
      "[Epoch 21, Batch 33/100] Loss: 0.2015 | RMSE: 0.1151 | Consistency: 0.0548 | SSIM: 0.8594 | Grad: 45133.723\n",
      "[Epoch 21, Batch 34/100] Loss: 0.2003 | RMSE: 0.1137 | Consistency: 0.0586 | SSIM: 0.8595 | Grad: 48192.301\n",
      "[Epoch 21, Batch 35/100] Loss: 0.2003 | RMSE: 0.1138 | Consistency: 0.0557 | SSIM: 0.8594 | Grad: 35479.383\n",
      "[Epoch 21, Batch 36/100] Loss: 0.2025 | RMSE: 0.1160 | Consistency: 0.0567 | SSIM: 0.8595 | Grad: 29512.480\n",
      "[Epoch 21, Batch 37/100] Loss: 0.2044 | RMSE: 0.1177 | Consistency: 0.0589 | SSIM: 0.8608 | Grad: 33866.777\n",
      "[Epoch 21, Batch 38/100] Loss: 0.2038 | RMSE: 0.1172 | Consistency: 0.0595 | SSIM: 0.8600 | Grad: 34300.180\n",
      "[Epoch 21, Batch 39/100] Loss: 0.2051 | RMSE: 0.1184 | Consistency: 0.0571 | SSIM: 0.8617 | Grad: 62071.758\n",
      "[Epoch 21, Batch 40/100] Loss: 0.2076 | RMSE: 0.1211 | Consistency: 0.0653 | SSIM: 0.8586 | Grad: 67983.312\n",
      "[Epoch 21, Batch 41/100] Loss: 0.2042 | RMSE: 0.1175 | Consistency: 0.0571 | SSIM: 0.8617 | Grad: 35774.141\n",
      "[Epoch 21, Batch 42/100] Loss: 0.2053 | RMSE: 0.1187 | Consistency: 0.0616 | SSIM: 0.8599 | Grad: 50372.910\n",
      "[Epoch 21, Batch 43/100] Loss: 0.2046 | RMSE: 0.1180 | Consistency: 0.0612 | SSIM: 0.8606 | Grad: 36161.227\n",
      "[Epoch 21, Batch 44/100] Loss: 0.2136 | RMSE: 0.1268 | Consistency: 0.0697 | SSIM: 0.8610 | Grad: 89733.945\n",
      "[Epoch 21, Batch 45/100] Loss: 0.2090 | RMSE: 0.1223 | Consistency: 0.0688 | SSIM: 0.8602 | Grad: 55525.117\n",
      "[Epoch 21, Batch 46/100] Loss: 0.2002 | RMSE: 0.1137 | Consistency: 0.0551 | SSIM: 0.8592 | Grad: 25342.662\n",
      "[Epoch 21, Batch 47/100] Loss: 0.2037 | RMSE: 0.1171 | Consistency: 0.0606 | SSIM: 0.8606 | Grad: 33449.094\n",
      "[Epoch 21, Batch 48/100] Loss: 0.2042 | RMSE: 0.1176 | Consistency: 0.0632 | SSIM: 0.8593 | Grad: 60988.438\n",
      "[Epoch 21, Batch 49/100] Loss: 0.2015 | RMSE: 0.1151 | Consistency: 0.0560 | SSIM: 0.8583 | Grad: 7292.261\n",
      "[Epoch 21, Batch 50/100] Loss: 0.2034 | RMSE: 0.1172 | Consistency: 0.0607 | SSIM: 0.8556 | Grad: 69275.844\n",
      "[Epoch 21, Batch 51/100] Loss: 0.2120 | RMSE: 0.1252 | Consistency: 0.0697 | SSIM: 0.8605 | Grad: 68172.430\n",
      "[Epoch 21, Batch 52/100] Loss: 0.2093 | RMSE: 0.1225 | Consistency: 0.0610 | SSIM: 0.8613 | Grad: 72305.367\n",
      "[Epoch 21, Batch 53/100] Loss: 0.2049 | RMSE: 0.1182 | Consistency: 0.0609 | SSIM: 0.8617 | Grad: 23179.080\n",
      "[Epoch 21, Batch 54/100] Loss: 0.2011 | RMSE: 0.1146 | Consistency: 0.0561 | SSIM: 0.8590 | Grad: 31237.584\n",
      "[Epoch 21, Batch 55/100] Loss: 0.2073 | RMSE: 0.1206 | Consistency: 0.0667 | SSIM: 0.8596 | Grad: 82599.453\n",
      "[Epoch 21, Batch 56/100] Loss: 0.2122 | RMSE: 0.1251 | Consistency: 0.0648 | SSIM: 0.8642 | Grad: 82371.023\n",
      "[Epoch 21, Batch 57/100] Loss: 0.2129 | RMSE: 0.1258 | Consistency: 0.0717 | SSIM: 0.8640 | Grad: 44799.629\n",
      "[Epoch 21, Batch 58/100] Loss: 0.2020 | RMSE: 0.1154 | Consistency: 0.0559 | SSIM: 0.8604 | Grad: 49574.195\n",
      "[Epoch 21, Batch 59/100] Loss: 0.2022 | RMSE: 0.1156 | Consistency: 0.0567 | SSIM: 0.8601 | Grad: 26770.840\n",
      "[Epoch 21, Batch 60/100] Loss: 0.1990 | RMSE: 0.1125 | Consistency: 0.0537 | SSIM: 0.8598 | Grad: 41169.125\n",
      "[Epoch 21, Batch 61/100] Loss: 0.2047 | RMSE: 0.1181 | Consistency: 0.0573 | SSIM: 0.8602 | Grad: 58971.410\n",
      "[Epoch 21, Batch 62/100] Loss: 0.2055 | RMSE: 0.1188 | Consistency: 0.0624 | SSIM: 0.8607 | Grad: 16943.736\n",
      "[Epoch 21, Batch 63/100] Loss: 0.2053 | RMSE: 0.1190 | Consistency: 0.0566 | SSIM: 0.8575 | Grad: 55512.418\n",
      "[Epoch 21, Batch 64/100] Loss: 0.2042 | RMSE: 0.1175 | Consistency: 0.0585 | SSIM: 0.8612 | Grad: 37253.164\n",
      "[Epoch 21, Batch 65/100] Loss: 0.2050 | RMSE: 0.1182 | Consistency: 0.0649 | SSIM: 0.8612 | Grad: 61109.879\n",
      "[Epoch 21, Batch 66/100] Loss: 0.2036 | RMSE: 0.1170 | Consistency: 0.0594 | SSIM: 0.8602 | Grad: 39624.824\n",
      "[Epoch 21, Batch 67/100] Loss: 0.1998 | RMSE: 0.1132 | Consistency: 0.0540 | SSIM: 0.8604 | Grad: 7895.061\n",
      "[Epoch 21, Batch 68/100] Loss: 0.2044 | RMSE: 0.1179 | Consistency: 0.0585 | SSIM: 0.8589 | Grad: 69940.609\n",
      "[Epoch 21, Batch 69/100] Loss: 0.2113 | RMSE: 0.1246 | Consistency: 0.0604 | SSIM: 0.8612 | Grad: 87883.812\n",
      "[Epoch 21, Batch 70/100] Loss: 0.2082 | RMSE: 0.1215 | Consistency: 0.0605 | SSIM: 0.8613 | Grad: 24525.320\n",
      "[Epoch 21, Batch 71/100] Loss: 0.2080 | RMSE: 0.1214 | Consistency: 0.0590 | SSIM: 0.8603 | Grad: 80593.133\n",
      "[Epoch 21, Batch 72/100] Loss: 0.2041 | RMSE: 0.1177 | Consistency: 0.0548 | SSIM: 0.8588 | Grad: 81259.578\n",
      "[Epoch 21, Batch 73/100] Loss: 0.2075 | RMSE: 0.1208 | Consistency: 0.0577 | SSIM: 0.8610 | Grad: 88975.852\n",
      "[Epoch 21, Batch 74/100] Loss: 0.2041 | RMSE: 0.1175 | Consistency: 0.0614 | SSIM: 0.8603 | Grad: 41797.496\n",
      "[Epoch 21, Batch 75/100] Loss: 0.2003 | RMSE: 0.1137 | Consistency: 0.0531 | SSIM: 0.8604 | Grad: 39068.145\n",
      "[Epoch 21, Batch 76/100] Loss: 0.2046 | RMSE: 0.1178 | Consistency: 0.0608 | SSIM: 0.8622 | Grad: 43209.383\n",
      "[Epoch 21, Batch 77/100] Loss: 0.2031 | RMSE: 0.1168 | Consistency: 0.0562 | SSIM: 0.8576 | Grad: 27899.648\n",
      "[Epoch 21, Batch 78/100] Loss: 0.2050 | RMSE: 0.1181 | Consistency: 0.0624 | SSIM: 0.8626 | Grad: 37139.492\n",
      "[Epoch 21, Batch 79/100] Loss: 0.2045 | RMSE: 0.1178 | Consistency: 0.0630 | SSIM: 0.8607 | Grad: 43872.699\n",
      "[Epoch 21, Batch 80/100] Loss: 0.2067 | RMSE: 0.1202 | Consistency: 0.0619 | SSIM: 0.8594 | Grad: 49333.637\n",
      "[Epoch 21, Batch 81/100] Loss: 0.2022 | RMSE: 0.1156 | Consistency: 0.0561 | SSIM: 0.8606 | Grad: 26372.670\n",
      "[Epoch 21, Batch 82/100] Loss: 0.2042 | RMSE: 0.1175 | Consistency: 0.0590 | SSIM: 0.8615 | Grad: 28844.906\n",
      "[Epoch 21, Batch 83/100] Loss: 0.2047 | RMSE: 0.1181 | Consistency: 0.0604 | SSIM: 0.8601 | Grad: 40574.547\n",
      "[Epoch 21, Batch 84/100] Loss: 0.2043 | RMSE: 0.1177 | Consistency: 0.0577 | SSIM: 0.8606 | Grad: 62858.871\n",
      "[Epoch 21, Batch 85/100] Loss: 0.2005 | RMSE: 0.1142 | Consistency: 0.0541 | SSIM: 0.8575 | Grad: 31932.875\n",
      "[Epoch 21, Batch 86/100] Loss: 0.2015 | RMSE: 0.1148 | Consistency: 0.0543 | SSIM: 0.8614 | Grad: 32035.541\n",
      "[Epoch 21, Batch 87/100] Loss: 0.2064 | RMSE: 0.1200 | Consistency: 0.0610 | SSIM: 0.8582 | Grad: 74672.219\n",
      "[Epoch 21, Batch 88/100] Loss: 0.2069 | RMSE: 0.1202 | Consistency: 0.0623 | SSIM: 0.8612 | Grad: 47580.246\n",
      "[Epoch 21, Batch 89/100] Loss: 0.2036 | RMSE: 0.1168 | Consistency: 0.0578 | SSIM: 0.8624 | Grad: 37163.004\n",
      "[Epoch 21, Batch 90/100] Loss: 0.2029 | RMSE: 0.1163 | Consistency: 0.0585 | SSIM: 0.8601 | Grad: 61598.348\n",
      "[Epoch 21, Batch 91/100] Loss: 0.2071 | RMSE: 0.1204 | Consistency: 0.0593 | SSIM: 0.8618 | Grad: 61068.152\n",
      "[Epoch 21, Batch 92/100] Loss: 0.2026 | RMSE: 0.1160 | Consistency: 0.0568 | SSIM: 0.8601 | Grad: 48263.094\n",
      "[Epoch 21, Batch 93/100] Loss: 0.2028 | RMSE: 0.1161 | Consistency: 0.0549 | SSIM: 0.8616 | Grad: 41626.445\n",
      "[Epoch 21, Batch 94/100] Loss: 0.2022 | RMSE: 0.1156 | Consistency: 0.0569 | SSIM: 0.8607 | Grad: 48448.938\n",
      "[Epoch 21, Batch 95/100] Loss: 0.2082 | RMSE: 0.1213 | Consistency: 0.0609 | SSIM: 0.8622 | Grad: 71495.555\n",
      "[Epoch 21, Batch 96/100] Loss: 0.2046 | RMSE: 0.1181 | Consistency: 0.0555 | SSIM: 0.8591 | Grad: 65663.508\n",
      "[Epoch 21, Batch 97/100] Loss: 0.2113 | RMSE: 0.1247 | Consistency: 0.0635 | SSIM: 0.8598 | Grad: 65234.203\n",
      "[Epoch 21, Batch 98/100] Loss: 0.2066 | RMSE: 0.1197 | Consistency: 0.0588 | SSIM: 0.8636 | Grad: 67756.680\n",
      "[Epoch 21, Batch 99/100] Loss: 0.2005 | RMSE: 0.1140 | Consistency: 0.0541 | SSIM: 0.8593 | Grad: 44963.906\n",
      "[Epoch 21, Batch 100/100] Loss: 0.2057 | RMSE: 0.1191 | Consistency: 0.0591 | SSIM: 0.8599 | Grad: 42099.645\n",
      "✅ Epoch 21 완료!\n",
      "   📈 평균 Loss: 0.205213\n",
      "   📊 평균 RMSE: 0.118591\n",
      "   ⚙️  학습률: 0.000922\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 22/80 시작...\n",
      "[Epoch 22, Batch 1/100] Loss: 0.2006 | RMSE: 0.1142 | Consistency: 0.0575 | SSIM: 0.8582 | Grad: 29162.584\n",
      "[Epoch 22, Batch 2/100] Loss: 0.1993 | RMSE: 0.1130 | Consistency: 0.0557 | SSIM: 0.8576 | Grad: 58918.363\n",
      "[Epoch 22, Batch 3/100] Loss: 0.1992 | RMSE: 0.1128 | Consistency: 0.0555 | SSIM: 0.8585 | Grad: 35024.328\n",
      "[Epoch 22, Batch 4/100] Loss: 0.2011 | RMSE: 0.1144 | Consistency: 0.0558 | SSIM: 0.8608 | Grad: 47559.883\n",
      "[Epoch 22, Batch 5/100] Loss: 0.2144 | RMSE: 0.1274 | Consistency: 0.0711 | SSIM: 0.8621 | Grad: 95241.484\n",
      "[Epoch 22, Batch 6/100] Loss: 0.2095 | RMSE: 0.1226 | Consistency: 0.0736 | SSIM: 0.8611 | Grad: 104561.594\n",
      "[Epoch 22, Batch 7/100] Loss: 0.2067 | RMSE: 0.1201 | Consistency: 0.0681 | SSIM: 0.8597 | Grad: 91567.695\n",
      "[Epoch 22, Batch 8/100] Loss: 0.2023 | RMSE: 0.1157 | Consistency: 0.0574 | SSIM: 0.8602 | Grad: 18840.859\n",
      "[Epoch 22, Batch 9/100] Loss: 0.2010 | RMSE: 0.1146 | Consistency: 0.0564 | SSIM: 0.8587 | Grad: 45853.289\n",
      "[Epoch 22, Batch 10/100] Loss: 0.2039 | RMSE: 0.1172 | Consistency: 0.0576 | SSIM: 0.8610 | Grad: 44776.219\n",
      "[Epoch 22, Batch 11/100] Loss: 0.2032 | RMSE: 0.1166 | Consistency: 0.0567 | SSIM: 0.8605 | Grad: 69895.430\n",
      "[Epoch 22, Batch 12/100] Loss: 0.2029 | RMSE: 0.1164 | Consistency: 0.0554 | SSIM: 0.8599 | Grad: 41491.883\n",
      "[Epoch 22, Batch 13/100] Loss: 0.2033 | RMSE: 0.1166 | Consistency: 0.0599 | SSIM: 0.8603 | Grad: 58284.207\n",
      "[Epoch 22, Batch 14/100] Loss: 0.2008 | RMSE: 0.1144 | Consistency: 0.0564 | SSIM: 0.8581 | Grad: 19821.191\n",
      "[Epoch 22, Batch 15/100] Loss: 0.2014 | RMSE: 0.1151 | Consistency: 0.0571 | SSIM: 0.8568 | Grad: 44144.551\n",
      "[Epoch 22, Batch 16/100] Loss: 0.1995 | RMSE: 0.1132 | Consistency: 0.0548 | SSIM: 0.8583 | Grad: 10154.875\n",
      "[Epoch 22, Batch 17/100] Loss: 0.2104 | RMSE: 0.1235 | Consistency: 0.0645 | SSIM: 0.8627 | Grad: 42190.832\n",
      "[Epoch 22, Batch 18/100] Loss: 0.2028 | RMSE: 0.1162 | Consistency: 0.0569 | SSIM: 0.8598 | Grad: 18668.750\n",
      "[Epoch 22, Batch 19/100] Loss: 0.2066 | RMSE: 0.1199 | Consistency: 0.0600 | SSIM: 0.8608 | Grad: 78995.180\n",
      "[Epoch 22, Batch 20/100] Loss: 0.2084 | RMSE: 0.1216 | Consistency: 0.0621 | SSIM: 0.8621 | Grad: 62852.395\n",
      "[Epoch 22, Batch 21/100] Loss: 0.2080 | RMSE: 0.1213 | Consistency: 0.0631 | SSIM: 0.8609 | Grad: 22271.309\n",
      "[Epoch 22, Batch 22/100] Loss: 0.2061 | RMSE: 0.1194 | Consistency: 0.0619 | SSIM: 0.8607 | Grad: 76360.750\n",
      "[Epoch 22, Batch 23/100] Loss: 0.2057 | RMSE: 0.1190 | Consistency: 0.0591 | SSIM: 0.8606 | Grad: 74556.422\n",
      "[Epoch 22, Batch 24/100] Loss: 0.2021 | RMSE: 0.1154 | Consistency: 0.0566 | SSIM: 0.8611 | Grad: 31053.146\n",
      "[Epoch 22, Batch 25/100] Loss: 0.2036 | RMSE: 0.1171 | Consistency: 0.0588 | SSIM: 0.8587 | Grad: 64298.559\n",
      "[Epoch 22, Batch 26/100] Loss: 0.2092 | RMSE: 0.1226 | Consistency: 0.0584 | SSIM: 0.8606 | Grad: 87469.109\n",
      "[Epoch 22, Batch 27/100] Loss: 0.2083 | RMSE: 0.1217 | Consistency: 0.0611 | SSIM: 0.8596 | Grad: 86471.773\n",
      "[Epoch 22, Batch 28/100] Loss: 0.1991 | RMSE: 0.1127 | Consistency: 0.0543 | SSIM: 0.8590 | Grad: 33359.078\n",
      "[Epoch 22, Batch 29/100] Loss: 0.2063 | RMSE: 0.1197 | Consistency: 0.0603 | SSIM: 0.8602 | Grad: 34085.621\n",
      "[Epoch 22, Batch 30/100] Loss: 0.2042 | RMSE: 0.1176 | Consistency: 0.0588 | SSIM: 0.8608 | Grad: 38820.848\n",
      "[Epoch 22, Batch 31/100] Loss: 0.1993 | RMSE: 0.1127 | Consistency: 0.0548 | SSIM: 0.8613 | Grad: 24397.488\n",
      "[Epoch 22, Batch 32/100] Loss: 0.2075 | RMSE: 0.1210 | Consistency: 0.0607 | SSIM: 0.8594 | Grad: 82111.883\n",
      "[Epoch 22, Batch 33/100] Loss: 0.2126 | RMSE: 0.1255 | Consistency: 0.0688 | SSIM: 0.8639 | Grad: 77042.195\n",
      "[Epoch 22, Batch 34/100] Loss: 0.2048 | RMSE: 0.1181 | Consistency: 0.0598 | SSIM: 0.8612 | Grad: 50330.754\n",
      "[Epoch 22, Batch 35/100] Loss: 0.2021 | RMSE: 0.1154 | Consistency: 0.0552 | SSIM: 0.8607 | Grad: 32239.129\n",
      "[Epoch 22, Batch 36/100] Loss: 0.2044 | RMSE: 0.1178 | Consistency: 0.0579 | SSIM: 0.8604 | Grad: 44561.688\n",
      "[Epoch 22, Batch 37/100] Loss: 0.2039 | RMSE: 0.1171 | Consistency: 0.0565 | SSIM: 0.8621 | Grad: 56951.645\n",
      "[Epoch 22, Batch 38/100] Loss: 0.2049 | RMSE: 0.1182 | Consistency: 0.0595 | SSIM: 0.8611 | Grad: 33747.820\n",
      "[Epoch 22, Batch 39/100] Loss: 0.2047 | RMSE: 0.1182 | Consistency: 0.0565 | SSIM: 0.8592 | Grad: 49943.367\n",
      "[Epoch 22, Batch 40/100] Loss: 0.1986 | RMSE: 0.1121 | Consistency: 0.0551 | SSIM: 0.8593 | Grad: 18105.826\n",
      "[Epoch 22, Batch 41/100] Loss: 0.2141 | RMSE: 0.1271 | Consistency: 0.0735 | SSIM: 0.8619 | Grad: 50810.605\n",
      "[Epoch 22, Batch 42/100] Loss: 0.2047 | RMSE: 0.1182 | Consistency: 0.0591 | SSIM: 0.8595 | Grad: 66882.477\n",
      "[Epoch 22, Batch 43/100] Loss: 0.2089 | RMSE: 0.1221 | Consistency: 0.0594 | SSIM: 0.8621 | Grad: 67752.930\n",
      "[Epoch 22, Batch 44/100] Loss: 0.2000 | RMSE: 0.1139 | Consistency: 0.0530 | SSIM: 0.8559 | Grad: 47857.496\n",
      "[Epoch 22, Batch 45/100] Loss: 0.2037 | RMSE: 0.1170 | Consistency: 0.0548 | SSIM: 0.8621 | Grad: 41475.629\n",
      "[Epoch 22, Batch 46/100] Loss: 0.2119 | RMSE: 0.1253 | Consistency: 0.0688 | SSIM: 0.8582 | Grad: 66828.719\n",
      "[Epoch 22, Batch 47/100] Loss: 0.2033 | RMSE: 0.1167 | Consistency: 0.0576 | SSIM: 0.8601 | Grad: 44684.773\n",
      "[Epoch 22, Batch 48/100] Loss: 0.2036 | RMSE: 0.1169 | Consistency: 0.0578 | SSIM: 0.8611 | Grad: 51697.941\n",
      "[Epoch 22, Batch 49/100] Loss: 0.2038 | RMSE: 0.1171 | Consistency: 0.0560 | SSIM: 0.8614 | Grad: 86681.102\n",
      "[Epoch 22, Batch 50/100] Loss: 0.2033 | RMSE: 0.1167 | Consistency: 0.0567 | SSIM: 0.8601 | Grad: 18696.947\n",
      "[Epoch 22, Batch 51/100] Loss: 0.2023 | RMSE: 0.1157 | Consistency: 0.0574 | SSIM: 0.8602 | Grad: 25365.592\n",
      "[Epoch 22, Batch 52/100] Loss: 0.2028 | RMSE: 0.1162 | Consistency: 0.0582 | SSIM: 0.8602 | Grad: 26273.992\n",
      "[Epoch 22, Batch 53/100] Loss: 0.2015 | RMSE: 0.1150 | Consistency: 0.0550 | SSIM: 0.8595 | Grad: 40034.582\n",
      "[Epoch 22, Batch 54/100] Loss: 0.2026 | RMSE: 0.1159 | Consistency: 0.0579 | SSIM: 0.8607 | Grad: 29298.658\n",
      "[Epoch 22, Batch 55/100] Loss: 0.1990 | RMSE: 0.1126 | Consistency: 0.0539 | SSIM: 0.8587 | Grad: 12834.451\n",
      "[Epoch 22, Batch 56/100] Loss: 0.2013 | RMSE: 0.1148 | Consistency: 0.0565 | SSIM: 0.8595 | Grad: 32840.789\n",
      "[Epoch 22, Batch 57/100] Loss: 0.2080 | RMSE: 0.1211 | Consistency: 0.0618 | SSIM: 0.8626 | Grad: 37469.707\n",
      "[Epoch 22, Batch 58/100] Loss: 0.2050 | RMSE: 0.1186 | Consistency: 0.0601 | SSIM: 0.8580 | Grad: 64686.246\n",
      "[Epoch 22, Batch 59/100] Loss: 0.2064 | RMSE: 0.1196 | Consistency: 0.0614 | SSIM: 0.8615 | Grad: 54167.887\n",
      "[Epoch 22, Batch 60/100] Loss: 0.2115 | RMSE: 0.1247 | Consistency: 0.0680 | SSIM: 0.8612 | Grad: 56232.363\n",
      "[Epoch 22, Batch 61/100] Loss: 0.2037 | RMSE: 0.1171 | Consistency: 0.0586 | SSIM: 0.8604 | Grad: 45526.656\n",
      "[Epoch 22, Batch 62/100] Loss: 0.1985 | RMSE: 0.1122 | Consistency: 0.0531 | SSIM: 0.8581 | Grad: 36273.453\n",
      "[Epoch 22, Batch 63/100] Loss: 0.2106 | RMSE: 0.1240 | Consistency: 0.0613 | SSIM: 0.8605 | Grad: 76920.102\n",
      "[Epoch 22, Batch 64/100] Loss: 0.2161 | RMSE: 0.1292 | Consistency: 0.0616 | SSIM: 0.8632 | Grad: 86676.398\n",
      "[Epoch 22, Batch 65/100] Loss: 0.2049 | RMSE: 0.1183 | Consistency: 0.0577 | SSIM: 0.8599 | Grad: 51193.094\n",
      "[Epoch 22, Batch 66/100] Loss: 0.2056 | RMSE: 0.1190 | Consistency: 0.0577 | SSIM: 0.8599 | Grad: 72039.875\n",
      "[Epoch 22, Batch 67/100] Loss: 0.2119 | RMSE: 0.1251 | Consistency: 0.0633 | SSIM: 0.8624 | Grad: 93739.922\n",
      "[Epoch 22, Batch 68/100] Loss: 0.2131 | RMSE: 0.1263 | Consistency: 0.0604 | SSIM: 0.8618 | Grad: 99786.367\n",
      "[Epoch 22, Batch 69/100] Loss: 0.2042 | RMSE: 0.1175 | Consistency: 0.0576 | SSIM: 0.8608 | Grad: 60509.895\n",
      "[Epoch 22, Batch 70/100] Loss: 0.2099 | RMSE: 0.1232 | Consistency: 0.0628 | SSIM: 0.8605 | Grad: 85975.453\n",
      "[Epoch 22, Batch 71/100] Loss: 0.2054 | RMSE: 0.1189 | Consistency: 0.0563 | SSIM: 0.8599 | Grad: 83941.641\n",
      "[Epoch 22, Batch 72/100] Loss: 0.2065 | RMSE: 0.1201 | Consistency: 0.0689 | SSIM: 0.8571 | Grad: 88829.734\n",
      "[Epoch 22, Batch 73/100] Loss: 0.2046 | RMSE: 0.1182 | Consistency: 0.0573 | SSIM: 0.8584 | Grad: 74696.773\n",
      "[Epoch 22, Batch 74/100] Loss: 0.2064 | RMSE: 0.1200 | Consistency: 0.0574 | SSIM: 0.8588 | Grad: 62626.480\n",
      "[Epoch 22, Batch 75/100] Loss: 0.2067 | RMSE: 0.1200 | Consistency: 0.0634 | SSIM: 0.8601 | Grad: 47433.969\n",
      "[Epoch 22, Batch 76/100] Loss: 0.2036 | RMSE: 0.1172 | Consistency: 0.0583 | SSIM: 0.8585 | Grad: 51914.234\n",
      "[Epoch 22, Batch 77/100] Loss: 0.2019 | RMSE: 0.1154 | Consistency: 0.0573 | SSIM: 0.8597 | Grad: 30604.365\n",
      "[Epoch 22, Batch 78/100] Loss: 0.2007 | RMSE: 0.1141 | Consistency: 0.0573 | SSIM: 0.8600 | Grad: 36852.180\n",
      "[Epoch 22, Batch 79/100] Loss: 0.2017 | RMSE: 0.1150 | Consistency: 0.0563 | SSIM: 0.8606 | Grad: 43946.070\n",
      "[Epoch 22, Batch 80/100] Loss: 0.2062 | RMSE: 0.1195 | Consistency: 0.0620 | SSIM: 0.8611 | Grad: 49491.996\n",
      "[Epoch 22, Batch 81/100] Loss: 0.1985 | RMSE: 0.1119 | Consistency: 0.0550 | SSIM: 0.8597 | Grad: 28716.764\n",
      "[Epoch 22, Batch 82/100] Loss: 0.2086 | RMSE: 0.1221 | Consistency: 0.0614 | SSIM: 0.8592 | Grad: 38885.734\n",
      "[Epoch 22, Batch 83/100] Loss: 0.2081 | RMSE: 0.1213 | Consistency: 0.0589 | SSIM: 0.8619 | Grad: 31366.443\n",
      "[Epoch 22, Batch 84/100] Loss: 0.2102 | RMSE: 0.1234 | Consistency: 0.0644 | SSIM: 0.8620 | Grad: 104547.477\n",
      "[Epoch 22, Batch 85/100] Loss: 0.2098 | RMSE: 0.1230 | Consistency: 0.0713 | SSIM: 0.8608 | Grad: 103462.891\n",
      "[Epoch 22, Batch 86/100] Loss: 0.2047 | RMSE: 0.1182 | Consistency: 0.0637 | SSIM: 0.8588 | Grad: 62827.605\n",
      "[Epoch 22, Batch 87/100] Loss: 0.2011 | RMSE: 0.1146 | Consistency: 0.0561 | SSIM: 0.8595 | Grad: 33564.293\n",
      "[Epoch 22, Batch 88/100] Loss: 0.2025 | RMSE: 0.1159 | Consistency: 0.0579 | SSIM: 0.8601 | Grad: 31353.088\n",
      "[Epoch 22, Batch 89/100] Loss: 0.2044 | RMSE: 0.1177 | Consistency: 0.0581 | SSIM: 0.8606 | Grad: 61535.969\n",
      "[Epoch 22, Batch 90/100] Loss: 0.2030 | RMSE: 0.1168 | Consistency: 0.0564 | SSIM: 0.8563 | Grad: 40536.102\n",
      "[Epoch 22, Batch 91/100] Loss: 0.2003 | RMSE: 0.1138 | Consistency: 0.0540 | SSIM: 0.8599 | Grad: 43286.703\n",
      "[Epoch 22, Batch 92/100] Loss: 0.2011 | RMSE: 0.1145 | Consistency: 0.0587 | SSIM: 0.8607 | Grad: 20627.883\n",
      "[Epoch 22, Batch 93/100] Loss: 0.2014 | RMSE: 0.1149 | Consistency: 0.0538 | SSIM: 0.8601 | Grad: 74163.102\n",
      "[Epoch 22, Batch 94/100] Loss: 0.2044 | RMSE: 0.1181 | Consistency: 0.0587 | SSIM: 0.8570 | Grad: 58036.520\n",
      "[Epoch 22, Batch 95/100] Loss: 0.1994 | RMSE: 0.1130 | Consistency: 0.0547 | SSIM: 0.8585 | Grad: 38204.926\n",
      "[Epoch 22, Batch 96/100] Loss: 0.2065 | RMSE: 0.1198 | Consistency: 0.0613 | SSIM: 0.8608 | Grad: 58420.391\n",
      "[Epoch 22, Batch 97/100] Loss: 0.2054 | RMSE: 0.1189 | Consistency: 0.0565 | SSIM: 0.8592 | Grad: 65071.531\n",
      "[Epoch 22, Batch 98/100] Loss: 0.2042 | RMSE: 0.1177 | Consistency: 0.0584 | SSIM: 0.8600 | Grad: 47339.395\n",
      "[Epoch 22, Batch 99/100] Loss: 0.2021 | RMSE: 0.1156 | Consistency: 0.0584 | SSIM: 0.8592 | Grad: 39490.047\n",
      "[Epoch 22, Batch 100/100] Loss: 0.2038 | RMSE: 0.1173 | Consistency: 0.0568 | SSIM: 0.8595 | Grad: 61088.410\n",
      "✅ Epoch 22 완료!\n",
      "   📈 평균 Loss: 0.204721\n",
      "   📊 평균 RMSE: 0.118120\n",
      "   ⚙️  학습률: 0.000909\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 23/80 시작...\n",
      "[Epoch 23, Batch 1/100] Loss: 0.2094 | RMSE: 0.1226 | Consistency: 0.0604 | SSIM: 0.8614 | Grad: 77182.781\n",
      "[Epoch 23, Batch 2/100] Loss: 0.2017 | RMSE: 0.1151 | Consistency: 0.0569 | SSIM: 0.8600 | Grad: 32014.779\n",
      "[Epoch 23, Batch 3/100] Loss: 0.1961 | RMSE: 0.1097 | Consistency: 0.0515 | SSIM: 0.8585 | Grad: 10685.789\n",
      "[Epoch 23, Batch 4/100] Loss: 0.2111 | RMSE: 0.1243 | Consistency: 0.0683 | SSIM: 0.8617 | Grad: 67533.102\n",
      "[Epoch 23, Batch 5/100] Loss: 0.2026 | RMSE: 0.1160 | Consistency: 0.0555 | SSIM: 0.8607 | Grad: 50354.543\n",
      "[Epoch 23, Batch 6/100] Loss: 0.2021 | RMSE: 0.1152 | Consistency: 0.0554 | SSIM: 0.8626 | Grad: 29277.834\n",
      "[Epoch 23, Batch 7/100] Loss: 0.2043 | RMSE: 0.1180 | Consistency: 0.0564 | SSIM: 0.8580 | Grad: 45216.660\n",
      "[Epoch 23, Batch 8/100] Loss: 0.2054 | RMSE: 0.1187 | Consistency: 0.0597 | SSIM: 0.8611 | Grad: 28345.492\n",
      "[Epoch 23, Batch 9/100] Loss: 0.2007 | RMSE: 0.1145 | Consistency: 0.0556 | SSIM: 0.8565 | Grad: 43854.043\n",
      "[Epoch 23, Batch 10/100] Loss: 0.2038 | RMSE: 0.1171 | Consistency: 0.0598 | SSIM: 0.8615 | Grad: 54606.473\n",
      "[Epoch 23, Batch 11/100] Loss: 0.2055 | RMSE: 0.1188 | Consistency: 0.0613 | SSIM: 0.8601 | Grad: 68106.211\n",
      "[Epoch 23, Batch 12/100] Loss: 0.2147 | RMSE: 0.1278 | Consistency: 0.0659 | SSIM: 0.8624 | Grad: 88618.258\n",
      "[Epoch 23, Batch 13/100] Loss: 0.2019 | RMSE: 0.1153 | Consistency: 0.0582 | SSIM: 0.8604 | Grad: 27761.479\n",
      "[Epoch 23, Batch 14/100] Loss: 0.2047 | RMSE: 0.1180 | Consistency: 0.0593 | SSIM: 0.8609 | Grad: 62398.523\n",
      "[Epoch 23, Batch 15/100] Loss: 0.2028 | RMSE: 0.1161 | Consistency: 0.0576 | SSIM: 0.8611 | Grad: 58534.277\n",
      "[Epoch 23, Batch 16/100] Loss: 0.1987 | RMSE: 0.1124 | Consistency: 0.0545 | SSIM: 0.8575 | Grad: 46178.930\n",
      "[Epoch 23, Batch 17/100] Loss: 0.2033 | RMSE: 0.1165 | Consistency: 0.0578 | SSIM: 0.8621 | Grad: 24383.045\n",
      "[Epoch 23, Batch 18/100] Loss: 0.2042 | RMSE: 0.1176 | Consistency: 0.0634 | SSIM: 0.8600 | Grad: 80901.922\n",
      "[Epoch 23, Batch 19/100] Loss: 0.2055 | RMSE: 0.1189 | Consistency: 0.0593 | SSIM: 0.8596 | Grad: 65620.500\n",
      "[Epoch 23, Batch 20/100] Loss: 0.2004 | RMSE: 0.1141 | Consistency: 0.0563 | SSIM: 0.8569 | Grad: 32674.514\n",
      "[Epoch 23, Batch 21/100] Loss: 0.2089 | RMSE: 0.1222 | Consistency: 0.0594 | SSIM: 0.8609 | Grad: 85601.641\n",
      "[Epoch 23, Batch 22/100] Loss: 0.2013 | RMSE: 0.1150 | Consistency: 0.0566 | SSIM: 0.8572 | Grad: 54076.941\n",
      "[Epoch 23, Batch 23/100] Loss: 0.2034 | RMSE: 0.1167 | Consistency: 0.0592 | SSIM: 0.8605 | Grad: 46934.348\n",
      "[Epoch 23, Batch 24/100] Loss: 0.2067 | RMSE: 0.1200 | Consistency: 0.0596 | SSIM: 0.8610 | Grad: 72643.992\n",
      "[Epoch 23, Batch 25/100] Loss: 0.2021 | RMSE: 0.1158 | Consistency: 0.0575 | SSIM: 0.8571 | Grad: 68669.992\n",
      "[Epoch 23, Batch 26/100] Loss: 0.2054 | RMSE: 0.1186 | Consistency: 0.0634 | SSIM: 0.8616 | Grad: 71968.172\n",
      "[Epoch 23, Batch 27/100] Loss: 0.1993 | RMSE: 0.1130 | Consistency: 0.0541 | SSIM: 0.8570 | Grad: 29312.809\n",
      "[Epoch 23, Batch 28/100] Loss: 0.2043 | RMSE: 0.1177 | Consistency: 0.0575 | SSIM: 0.8600 | Grad: 56077.055\n",
      "[Epoch 23, Batch 29/100] Loss: 0.2121 | RMSE: 0.1255 | Consistency: 0.0664 | SSIM: 0.8597 | Grad: 98225.883\n",
      "[Epoch 23, Batch 30/100] Loss: 0.2086 | RMSE: 0.1222 | Consistency: 0.0601 | SSIM: 0.8586 | Grad: 90216.266\n",
      "[Epoch 23, Batch 31/100] Loss: 0.2035 | RMSE: 0.1169 | Consistency: 0.0601 | SSIM: 0.8603 | Grad: 28279.072\n",
      "[Epoch 23, Batch 32/100] Loss: 0.2034 | RMSE: 0.1169 | Consistency: 0.0517 | SSIM: 0.8593 | Grad: 84042.625\n",
      "[Epoch 23, Batch 33/100] Loss: 0.2068 | RMSE: 0.1200 | Consistency: 0.0595 | SSIM: 0.8623 | Grad: 57206.402\n",
      "[Epoch 23, Batch 34/100] Loss: 0.2045 | RMSE: 0.1179 | Consistency: 0.0584 | SSIM: 0.8601 | Grad: 30566.143\n",
      "[Epoch 23, Batch 35/100] Loss: 0.2025 | RMSE: 0.1159 | Consistency: 0.0592 | SSIM: 0.8602 | Grad: 60024.113\n",
      "[Epoch 23, Batch 36/100] Loss: 0.2113 | RMSE: 0.1243 | Consistency: 0.0687 | SSIM: 0.8635 | Grad: 26901.430\n",
      "[Epoch 23, Batch 37/100] Loss: 0.2092 | RMSE: 0.1225 | Consistency: 0.0593 | SSIM: 0.8610 | Grad: 78003.648\n",
      "[Epoch 23, Batch 38/100] Loss: 0.2045 | RMSE: 0.1177 | Consistency: 0.0612 | SSIM: 0.8613 | Grad: 59485.988\n",
      "[Epoch 23, Batch 39/100] Loss: 0.2009 | RMSE: 0.1144 | Consistency: 0.0580 | SSIM: 0.8597 | Grad: 47374.777\n",
      "[Epoch 23, Batch 40/100] Loss: 0.2041 | RMSE: 0.1174 | Consistency: 0.0588 | SSIM: 0.8616 | Grad: 64802.086\n",
      "[Epoch 23, Batch 41/100] Loss: 0.2002 | RMSE: 0.1137 | Consistency: 0.0531 | SSIM: 0.8597 | Grad: 39480.656\n",
      "[Epoch 23, Batch 42/100] Loss: 0.2018 | RMSE: 0.1154 | Consistency: 0.0567 | SSIM: 0.8577 | Grad: 45577.926\n",
      "[Epoch 23, Batch 43/100] Loss: 0.2020 | RMSE: 0.1156 | Consistency: 0.0567 | SSIM: 0.8584 | Grad: 37091.578\n",
      "[Epoch 23, Batch 44/100] Loss: 0.1995 | RMSE: 0.1129 | Consistency: 0.0554 | SSIM: 0.8603 | Grad: 48312.559\n",
      "[Epoch 23, Batch 45/100] Loss: 0.2013 | RMSE: 0.1150 | Consistency: 0.0561 | SSIM: 0.8575 | Grad: 39862.277\n",
      "[Epoch 23, Batch 46/100] Loss: 0.1969 | RMSE: 0.1104 | Consistency: 0.0514 | SSIM: 0.8600 | Grad: 33286.559\n",
      "[Epoch 23, Batch 47/100] Loss: 0.2018 | RMSE: 0.1151 | Consistency: 0.0594 | SSIM: 0.8607 | Grad: 60120.535\n",
      "[Epoch 23, Batch 48/100] Loss: 0.2152 | RMSE: 0.1283 | Consistency: 0.0628 | SSIM: 0.8626 | Grad: 93397.461\n",
      "[Epoch 23, Batch 49/100] Loss: 0.2011 | RMSE: 0.1144 | Consistency: 0.0577 | SSIM: 0.8610 | Grad: 36977.863\n",
      "[Epoch 23, Batch 50/100] Loss: 0.2030 | RMSE: 0.1165 | Consistency: 0.0564 | SSIM: 0.8589 | Grad: 43254.418\n",
      "[Epoch 23, Batch 51/100] Loss: 0.2047 | RMSE: 0.1181 | Consistency: 0.0566 | SSIM: 0.8596 | Grad: 64809.918\n",
      "[Epoch 23, Batch 52/100] Loss: 0.2049 | RMSE: 0.1183 | Consistency: 0.0637 | SSIM: 0.8594 | Grad: 78958.586\n",
      "[Epoch 23, Batch 53/100] Loss: 0.2039 | RMSE: 0.1173 | Consistency: 0.0569 | SSIM: 0.8604 | Grad: 68602.781\n",
      "[Epoch 23, Batch 54/100] Loss: 0.2057 | RMSE: 0.1190 | Consistency: 0.0574 | SSIM: 0.8611 | Grad: 63228.590\n",
      "[Epoch 23, Batch 55/100] Loss: 0.2015 | RMSE: 0.1150 | Consistency: 0.0570 | SSIM: 0.8591 | Grad: 39192.535\n",
      "[Epoch 23, Batch 56/100] Loss: 0.2048 | RMSE: 0.1182 | Consistency: 0.0590 | SSIM: 0.8606 | Grad: 73475.914\n",
      "[Epoch 23, Batch 57/100] Loss: 0.2049 | RMSE: 0.1182 | Consistency: 0.0630 | SSIM: 0.8606 | Grad: 68203.461\n",
      "[Epoch 23, Batch 58/100] Loss: 0.2002 | RMSE: 0.1137 | Consistency: 0.0546 | SSIM: 0.8600 | Grad: 46192.137\n",
      "[Epoch 23, Batch 59/100] Loss: 0.2051 | RMSE: 0.1186 | Consistency: 0.0650 | SSIM: 0.8586 | Grad: 91586.375\n",
      "[Epoch 23, Batch 60/100] Loss: 0.2073 | RMSE: 0.1206 | Consistency: 0.0687 | SSIM: 0.8598 | Grad: 100795.016\n",
      "[Epoch 23, Batch 61/100] Loss: 0.2010 | RMSE: 0.1146 | Consistency: 0.0574 | SSIM: 0.8582 | Grad: 66189.031\n",
      "[Epoch 23, Batch 62/100] Loss: 0.2056 | RMSE: 0.1193 | Consistency: 0.0570 | SSIM: 0.8569 | Grad: 70817.461\n",
      "[Epoch 23, Batch 63/100] Loss: 0.2070 | RMSE: 0.1203 | Consistency: 0.0663 | SSIM: 0.8609 | Grad: 90946.344\n",
      "[Epoch 23, Batch 64/100] Loss: 0.2067 | RMSE: 0.1200 | Consistency: 0.0675 | SSIM: 0.8612 | Grad: 86232.742\n",
      "[Epoch 23, Batch 65/100] Loss: 0.2062 | RMSE: 0.1197 | Consistency: 0.0651 | SSIM: 0.8585 | Grad: 49694.453\n",
      "[Epoch 23, Batch 66/100] Loss: 0.2022 | RMSE: 0.1156 | Consistency: 0.0553 | SSIM: 0.8596 | Grad: 23880.891\n",
      "[Epoch 23, Batch 67/100] Loss: 0.2120 | RMSE: 0.1253 | Consistency: 0.0774 | SSIM: 0.8587 | Grad: 97258.102\n",
      "[Epoch 23, Batch 68/100] Loss: 0.2222 | RMSE: 0.1349 | Consistency: 0.0924 | SSIM: 0.8636 | Grad: 90870.891\n",
      "[Epoch 23, Batch 69/100] Loss: 0.2083 | RMSE: 0.1214 | Consistency: 0.0681 | SSIM: 0.8621 | Grad: 55986.930\n",
      "[Epoch 23, Batch 70/100] Loss: 0.2010 | RMSE: 0.1145 | Consistency: 0.0572 | SSIM: 0.8595 | Grad: 28829.580\n",
      "[Epoch 23, Batch 71/100] Loss: 0.2054 | RMSE: 0.1187 | Consistency: 0.0651 | SSIM: 0.8613 | Grad: 56904.301\n",
      "[Epoch 23, Batch 72/100] Loss: 0.2029 | RMSE: 0.1163 | Consistency: 0.0617 | SSIM: 0.8595 | Grad: 53071.441\n",
      "[Epoch 23, Batch 73/100] Loss: 0.2018 | RMSE: 0.1155 | Consistency: 0.0558 | SSIM: 0.8578 | Grad: 47218.219\n",
      "[Epoch 23, Batch 74/100] Loss: 0.2025 | RMSE: 0.1162 | Consistency: 0.0596 | SSIM: 0.8567 | Grad: 59429.434\n",
      "[Epoch 23, Batch 75/100] Loss: 0.2022 | RMSE: 0.1160 | Consistency: 0.0569 | SSIM: 0.8568 | Grad: 73863.102\n",
      "[Epoch 23, Batch 76/100] Loss: 0.2040 | RMSE: 0.1175 | Consistency: 0.0556 | SSIM: 0.8595 | Grad: 67593.477\n",
      "[Epoch 23, Batch 77/100] Loss: 0.2020 | RMSE: 0.1156 | Consistency: 0.0535 | SSIM: 0.8585 | Grad: 42223.180\n",
      "[Epoch 23, Batch 78/100] Loss: 0.2103 | RMSE: 0.1235 | Consistency: 0.0666 | SSIM: 0.8612 | Grad: 80700.742\n",
      "[Epoch 23, Batch 79/100] Loss: 0.2164 | RMSE: 0.1297 | Consistency: 0.0676 | SSIM: 0.8604 | Grad: 105934.703\n",
      "[Epoch 23, Batch 80/100] Loss: 0.2139 | RMSE: 0.1272 | Consistency: 0.0577 | SSIM: 0.8610 | Grad: 102497.555\n",
      "[Epoch 23, Batch 81/100] Loss: 0.1996 | RMSE: 0.1132 | Consistency: 0.0557 | SSIM: 0.8588 | Grad: 31926.062\n",
      "[Epoch 23, Batch 82/100] Loss: 0.2033 | RMSE: 0.1167 | Consistency: 0.0565 | SSIM: 0.8599 | Grad: 33050.254\n",
      "[Epoch 23, Batch 83/100] Loss: 0.2035 | RMSE: 0.1169 | Consistency: 0.0552 | SSIM: 0.8605 | Grad: 65253.191\n",
      "[Epoch 23, Batch 84/100] Loss: 0.2048 | RMSE: 0.1182 | Consistency: 0.0594 | SSIM: 0.8602 | Grad: 79031.664\n",
      "[Epoch 23, Batch 85/100] Loss: 0.2002 | RMSE: 0.1138 | Consistency: 0.0577 | SSIM: 0.8582 | Grad: 36663.336\n",
      "[Epoch 23, Batch 86/100] Loss: 0.1985 | RMSE: 0.1120 | Consistency: 0.0558 | SSIM: 0.8586 | Grad: 48382.918\n",
      "[Epoch 23, Batch 87/100] Loss: 0.2034 | RMSE: 0.1171 | Consistency: 0.0572 | SSIM: 0.8575 | Grad: 61000.137\n",
      "[Epoch 23, Batch 88/100] Loss: 0.1998 | RMSE: 0.1133 | Consistency: 0.0523 | SSIM: 0.8599 | Grad: 49437.055\n",
      "[Epoch 23, Batch 89/100] Loss: 0.2015 | RMSE: 0.1149 | Consistency: 0.0589 | SSIM: 0.8599 | Grad: 33732.109\n",
      "[Epoch 23, Batch 90/100] Loss: 0.2014 | RMSE: 0.1151 | Consistency: 0.0559 | SSIM: 0.8570 | Grad: 33126.117\n",
      "[Epoch 23, Batch 91/100] Loss: 0.2053 | RMSE: 0.1187 | Consistency: 0.0555 | SSIM: 0.8611 | Grad: 69578.766\n",
      "[Epoch 23, Batch 92/100] Loss: 0.1991 | RMSE: 0.1127 | Consistency: 0.0541 | SSIM: 0.8589 | Grad: 30272.924\n",
      "[Epoch 23, Batch 93/100] Loss: 0.2024 | RMSE: 0.1156 | Consistency: 0.0590 | SSIM: 0.8622 | Grad: 36674.871\n",
      "[Epoch 23, Batch 94/100] Loss: 0.2046 | RMSE: 0.1180 | Consistency: 0.0584 | SSIM: 0.8596 | Grad: 63817.512\n",
      "[Epoch 23, Batch 95/100] Loss: 0.2033 | RMSE: 0.1167 | Consistency: 0.0610 | SSIM: 0.8599 | Grad: 61127.391\n",
      "[Epoch 23, Batch 96/100] Loss: 0.2026 | RMSE: 0.1160 | Consistency: 0.0610 | SSIM: 0.8600 | Grad: 18264.711\n",
      "[Epoch 23, Batch 97/100] Loss: 0.2040 | RMSE: 0.1175 | Consistency: 0.0618 | SSIM: 0.8590 | Grad: 42592.848\n",
      "[Epoch 23, Batch 98/100] Loss: 0.1995 | RMSE: 0.1131 | Consistency: 0.0569 | SSIM: 0.8588 | Grad: 40730.191\n",
      "[Epoch 23, Batch 99/100] Loss: 0.1982 | RMSE: 0.1117 | Consistency: 0.0539 | SSIM: 0.8598 | Grad: 20617.910\n",
      "[Epoch 23, Batch 100/100] Loss: 0.1977 | RMSE: 0.1113 | Consistency: 0.0549 | SSIM: 0.8589 | Grad: 22438.096\n",
      "✅ Epoch 23 완료!\n",
      "   📈 평균 Loss: 0.204135\n",
      "   📊 평균 RMSE: 0.117559\n",
      "   ⚙️  학습률: 0.000897\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 24/80 시작...\n",
      "[Epoch 24, Batch 1/100] Loss: 0.2027 | RMSE: 0.1164 | Consistency: 0.0554 | SSIM: 0.8572 | Grad: 61188.344\n",
      "[Epoch 24, Batch 2/100] Loss: 0.2025 | RMSE: 0.1161 | Consistency: 0.0590 | SSIM: 0.8584 | Grad: 62406.289\n",
      "[Epoch 24, Batch 3/100] Loss: 0.2057 | RMSE: 0.1190 | Consistency: 0.0603 | SSIM: 0.8610 | Grad: 69609.750\n",
      "[Epoch 24, Batch 4/100] Loss: 0.2030 | RMSE: 0.1166 | Consistency: 0.0620 | SSIM: 0.8581 | Grad: 35673.891\n",
      "[Epoch 24, Batch 5/100] Loss: 0.2012 | RMSE: 0.1147 | Consistency: 0.0564 | SSIM: 0.8596 | Grad: 51781.434\n",
      "[Epoch 24, Batch 6/100] Loss: 0.2066 | RMSE: 0.1199 | Consistency: 0.0644 | SSIM: 0.8611 | Grad: 82701.594\n",
      "[Epoch 24, Batch 7/100] Loss: 0.2135 | RMSE: 0.1263 | Consistency: 0.0648 | SSIM: 0.8653 | Grad: 71911.188\n",
      "[Epoch 24, Batch 8/100] Loss: 0.2038 | RMSE: 0.1171 | Consistency: 0.0553 | SSIM: 0.8613 | Grad: 48240.898\n",
      "[Epoch 24, Batch 9/100] Loss: 0.2033 | RMSE: 0.1167 | Consistency: 0.0588 | SSIM: 0.8604 | Grad: 57452.801\n",
      "[Epoch 24, Batch 10/100] Loss: 0.2054 | RMSE: 0.1186 | Consistency: 0.0682 | SSIM: 0.8611 | Grad: 74050.148\n",
      "[Epoch 24, Batch 11/100] Loss: 0.2110 | RMSE: 0.1241 | Consistency: 0.0660 | SSIM: 0.8620 | Grad: 39521.543\n",
      "[Epoch 24, Batch 12/100] Loss: 0.2029 | RMSE: 0.1161 | Consistency: 0.0611 | SSIM: 0.8618 | Grad: 51484.133\n",
      "[Epoch 24, Batch 13/100] Loss: 0.2048 | RMSE: 0.1183 | Consistency: 0.0600 | SSIM: 0.8590 | Grad: 52903.004\n",
      "[Epoch 24, Batch 14/100] Loss: 0.2128 | RMSE: 0.1260 | Consistency: 0.0677 | SSIM: 0.8611 | Grad: 17823.627\n",
      "[Epoch 24, Batch 15/100] Loss: 0.2044 | RMSE: 0.1180 | Consistency: 0.0588 | SSIM: 0.8581 | Grad: 68323.742\n",
      "[Epoch 24, Batch 16/100] Loss: 0.2019 | RMSE: 0.1154 | Consistency: 0.0578 | SSIM: 0.8598 | Grad: 20998.711\n",
      "[Epoch 24, Batch 17/100] Loss: 0.2000 | RMSE: 0.1134 | Consistency: 0.0557 | SSIM: 0.8603 | Grad: 18954.701\n",
      "[Epoch 24, Batch 18/100] Loss: 0.2004 | RMSE: 0.1140 | Consistency: 0.0558 | SSIM: 0.8592 | Grad: 32919.336\n",
      "[Epoch 24, Batch 19/100] Loss: 0.2027 | RMSE: 0.1164 | Consistency: 0.0547 | SSIM: 0.8574 | Grad: 53768.617\n",
      "[Epoch 24, Batch 20/100] Loss: 0.2082 | RMSE: 0.1214 | Consistency: 0.0611 | SSIM: 0.8614 | Grad: 56340.180\n",
      "[Epoch 24, Batch 21/100] Loss: 0.2023 | RMSE: 0.1158 | Consistency: 0.0578 | SSIM: 0.8589 | Grad: 23401.139\n",
      "[Epoch 24, Batch 22/100] Loss: 0.2024 | RMSE: 0.1159 | Consistency: 0.0545 | SSIM: 0.8596 | Grad: 53217.324\n",
      "[Epoch 24, Batch 23/100] Loss: 0.2048 | RMSE: 0.1181 | Consistency: 0.0550 | SSIM: 0.8615 | Grad: 65957.422\n",
      "[Epoch 24, Batch 24/100] Loss: 0.2035 | RMSE: 0.1170 | Consistency: 0.0620 | SSIM: 0.8586 | Grad: 60176.938\n",
      "[Epoch 24, Batch 25/100] Loss: 0.2015 | RMSE: 0.1150 | Consistency: 0.0560 | SSIM: 0.8595 | Grad: 59606.312\n",
      "[Epoch 24, Batch 26/100] Loss: 0.2033 | RMSE: 0.1165 | Consistency: 0.0597 | SSIM: 0.8621 | Grad: 69158.438\n",
      "[Epoch 24, Batch 27/100] Loss: 0.2004 | RMSE: 0.1138 | Consistency: 0.0557 | SSIM: 0.8605 | Grad: 54256.984\n",
      "[Epoch 24, Batch 28/100] Loss: 0.2063 | RMSE: 0.1196 | Consistency: 0.0606 | SSIM: 0.8618 | Grad: 41287.402\n",
      "[Epoch 24, Batch 29/100] Loss: 0.2025 | RMSE: 0.1162 | Consistency: 0.0571 | SSIM: 0.8574 | Grad: 50365.301\n",
      "[Epoch 24, Batch 30/100] Loss: 0.2018 | RMSE: 0.1153 | Consistency: 0.0593 | SSIM: 0.8598 | Grad: 53210.559\n",
      "[Epoch 24, Batch 31/100] Loss: 0.2017 | RMSE: 0.1151 | Consistency: 0.0548 | SSIM: 0.8597 | Grad: 34033.285\n",
      "[Epoch 24, Batch 32/100] Loss: 0.1978 | RMSE: 0.1115 | Consistency: 0.0517 | SSIM: 0.8571 | Grad: 25670.062\n",
      "[Epoch 24, Batch 33/100] Loss: 0.2009 | RMSE: 0.1146 | Consistency: 0.0579 | SSIM: 0.8580 | Grad: 51422.457\n",
      "[Epoch 24, Batch 34/100] Loss: 0.2105 | RMSE: 0.1237 | Consistency: 0.0672 | SSIM: 0.8616 | Grad: 71986.703\n",
      "[Epoch 24, Batch 35/100] Loss: 0.2068 | RMSE: 0.1201 | Consistency: 0.0641 | SSIM: 0.8602 | Grad: 72249.617\n",
      "[Epoch 24, Batch 36/100] Loss: 0.2057 | RMSE: 0.1193 | Consistency: 0.0612 | SSIM: 0.8576 | Grad: 39391.727\n",
      "[Epoch 24, Batch 37/100] Loss: 0.2030 | RMSE: 0.1164 | Consistency: 0.0576 | SSIM: 0.8600 | Grad: 49691.031\n",
      "[Epoch 24, Batch 38/100] Loss: 0.2072 | RMSE: 0.1205 | Consistency: 0.0577 | SSIM: 0.8611 | Grad: 22688.264\n",
      "[Epoch 24, Batch 39/100] Loss: 0.2066 | RMSE: 0.1199 | Consistency: 0.0584 | SSIM: 0.8611 | Grad: 56557.551\n",
      "[Epoch 24, Batch 40/100] Loss: 0.2003 | RMSE: 0.1138 | Consistency: 0.0546 | SSIM: 0.8589 | Grad: 30633.047\n",
      "[Epoch 24, Batch 41/100] Loss: 0.1996 | RMSE: 0.1130 | Consistency: 0.0542 | SSIM: 0.8613 | Grad: 10687.959\n",
      "[Epoch 24, Batch 42/100] Loss: 0.2007 | RMSE: 0.1141 | Consistency: 0.0527 | SSIM: 0.8606 | Grad: 46132.363\n",
      "[Epoch 24, Batch 43/100] Loss: 0.2005 | RMSE: 0.1142 | Consistency: 0.0565 | SSIM: 0.8578 | Grad: 57242.480\n",
      "[Epoch 24, Batch 44/100] Loss: 0.2035 | RMSE: 0.1168 | Consistency: 0.0583 | SSIM: 0.8615 | Grad: 55732.645\n",
      "[Epoch 24, Batch 45/100] Loss: 0.2015 | RMSE: 0.1151 | Consistency: 0.0563 | SSIM: 0.8587 | Grad: 39964.926\n",
      "[Epoch 24, Batch 46/100] Loss: 0.1975 | RMSE: 0.1111 | Consistency: 0.0514 | SSIM: 0.8584 | Grad: 38981.559\n",
      "[Epoch 24, Batch 47/100] Loss: 0.2009 | RMSE: 0.1144 | Consistency: 0.0571 | SSIM: 0.8592 | Grad: 66203.062\n",
      "[Epoch 24, Batch 48/100] Loss: 0.2097 | RMSE: 0.1233 | Consistency: 0.0700 | SSIM: 0.8577 | Grad: 72387.906\n",
      "[Epoch 24, Batch 49/100] Loss: 0.1989 | RMSE: 0.1123 | Consistency: 0.0539 | SSIM: 0.8605 | Grad: 20882.391\n",
      "[Epoch 24, Batch 50/100] Loss: 0.2033 | RMSE: 0.1168 | Consistency: 0.0572 | SSIM: 0.8596 | Grad: 50050.102\n",
      "[Epoch 24, Batch 51/100] Loss: 0.2025 | RMSE: 0.1159 | Consistency: 0.0556 | SSIM: 0.8608 | Grad: 62189.203\n",
      "[Epoch 24, Batch 52/100] Loss: 0.2022 | RMSE: 0.1155 | Consistency: 0.0546 | SSIM: 0.8613 | Grad: 47710.926\n",
      "[Epoch 24, Batch 53/100] Loss: 0.1999 | RMSE: 0.1135 | Consistency: 0.0524 | SSIM: 0.8588 | Grad: 18486.838\n",
      "[Epoch 24, Batch 54/100] Loss: 0.2033 | RMSE: 0.1166 | Consistency: 0.0620 | SSIM: 0.8606 | Grad: 52702.891\n",
      "[Epoch 24, Batch 55/100] Loss: 0.2016 | RMSE: 0.1150 | Consistency: 0.0556 | SSIM: 0.8605 | Grad: 22948.098\n",
      "[Epoch 24, Batch 56/100] Loss: 0.2098 | RMSE: 0.1229 | Consistency: 0.0650 | SSIM: 0.8624 | Grad: 65307.613\n",
      "[Epoch 24, Batch 57/100] Loss: 0.2046 | RMSE: 0.1181 | Consistency: 0.0611 | SSIM: 0.8589 | Grad: 67715.422\n",
      "[Epoch 24, Batch 58/100] Loss: 0.1985 | RMSE: 0.1123 | Consistency: 0.0547 | SSIM: 0.8567 | Grad: 14354.007\n",
      "[Epoch 24, Batch 59/100] Loss: 0.2036 | RMSE: 0.1173 | Consistency: 0.0578 | SSIM: 0.8575 | Grad: 84075.562\n",
      "[Epoch 24, Batch 60/100] Loss: 0.2069 | RMSE: 0.1202 | Consistency: 0.0623 | SSIM: 0.8607 | Grad: 59223.402\n",
      "[Epoch 24, Batch 61/100] Loss: 0.1991 | RMSE: 0.1127 | Consistency: 0.0537 | SSIM: 0.8594 | Grad: 21010.707\n",
      "[Epoch 24, Batch 62/100] Loss: 0.1981 | RMSE: 0.1118 | Consistency: 0.0547 | SSIM: 0.8578 | Grad: 43169.785\n",
      "[Epoch 24, Batch 63/100] Loss: 0.2015 | RMSE: 0.1149 | Consistency: 0.0550 | SSIM: 0.8604 | Grad: 58869.344\n",
      "[Epoch 24, Batch 64/100] Loss: 0.2040 | RMSE: 0.1174 | Consistency: 0.0584 | SSIM: 0.8595 | Grad: 33111.848\n",
      "[Epoch 24, Batch 65/100] Loss: 0.2012 | RMSE: 0.1146 | Consistency: 0.0580 | SSIM: 0.8600 | Grad: 35107.555\n",
      "[Epoch 24, Batch 66/100] Loss: 0.2076 | RMSE: 0.1208 | Consistency: 0.0677 | SSIM: 0.8606 | Grad: 77848.773\n",
      "[Epoch 24, Batch 67/100] Loss: 0.2031 | RMSE: 0.1165 | Consistency: 0.0571 | SSIM: 0.8599 | Grad: 54146.660\n",
      "[Epoch 24, Batch 68/100] Loss: 0.2006 | RMSE: 0.1142 | Consistency: 0.0566 | SSIM: 0.8583 | Grad: 23875.777\n",
      "[Epoch 24, Batch 69/100] Loss: 0.2019 | RMSE: 0.1153 | Consistency: 0.0587 | SSIM: 0.8595 | Grad: 47912.547\n",
      "[Epoch 24, Batch 70/100] Loss: 0.2016 | RMSE: 0.1152 | Consistency: 0.0525 | SSIM: 0.8591 | Grad: 51080.191\n",
      "[Epoch 24, Batch 71/100] Loss: 0.2039 | RMSE: 0.1172 | Consistency: 0.0571 | SSIM: 0.8616 | Grad: 57188.523\n",
      "[Epoch 24, Batch 72/100] Loss: 0.2034 | RMSE: 0.1167 | Consistency: 0.0576 | SSIM: 0.8617 | Grad: 52233.211\n",
      "[Epoch 24, Batch 73/100] Loss: 0.2032 | RMSE: 0.1167 | Consistency: 0.0578 | SSIM: 0.8594 | Grad: 58198.898\n",
      "[Epoch 24, Batch 74/100] Loss: 0.1992 | RMSE: 0.1127 | Consistency: 0.0546 | SSIM: 0.8591 | Grad: 34895.391\n",
      "[Epoch 24, Batch 75/100] Loss: 0.2011 | RMSE: 0.1145 | Consistency: 0.0555 | SSIM: 0.8599 | Grad: 53690.344\n",
      "[Epoch 24, Batch 76/100] Loss: 0.2048 | RMSE: 0.1183 | Consistency: 0.0566 | SSIM: 0.8594 | Grad: 59291.008\n",
      "[Epoch 24, Batch 77/100] Loss: 0.2011 | RMSE: 0.1147 | Consistency: 0.0569 | SSIM: 0.8581 | Grad: 30015.516\n",
      "[Epoch 24, Batch 78/100] Loss: 0.1980 | RMSE: 0.1118 | Consistency: 0.0543 | SSIM: 0.8561 | Grad: 37323.570\n",
      "[Epoch 24, Batch 79/100] Loss: 0.2044 | RMSE: 0.1177 | Consistency: 0.0603 | SSIM: 0.8602 | Grad: 74157.555\n",
      "[Epoch 24, Batch 80/100] Loss: 0.1989 | RMSE: 0.1124 | Consistency: 0.0555 | SSIM: 0.8595 | Grad: 35612.957\n",
      "[Epoch 24, Batch 81/100] Loss: 0.1985 | RMSE: 0.1119 | Consistency: 0.0555 | SSIM: 0.8605 | Grad: 41611.832\n",
      "[Epoch 24, Batch 82/100] Loss: 0.2081 | RMSE: 0.1214 | Consistency: 0.0614 | SSIM: 0.8610 | Grad: 77387.250\n",
      "[Epoch 24, Batch 83/100] Loss: 0.2027 | RMSE: 0.1164 | Consistency: 0.0590 | SSIM: 0.8577 | Grad: 67640.703\n",
      "[Epoch 24, Batch 84/100] Loss: 0.2010 | RMSE: 0.1145 | Consistency: 0.0550 | SSIM: 0.8597 | Grad: 34765.094\n",
      "[Epoch 24, Batch 85/100] Loss: 0.2097 | RMSE: 0.1229 | Consistency: 0.0607 | SSIM: 0.8625 | Grad: 100579.922\n",
      "[Epoch 24, Batch 86/100] Loss: 0.2045 | RMSE: 0.1181 | Consistency: 0.0593 | SSIM: 0.8577 | Grad: 59561.207\n",
      "[Epoch 24, Batch 87/100] Loss: 0.1987 | RMSE: 0.1123 | Consistency: 0.0530 | SSIM: 0.8590 | Grad: 55423.855\n",
      "[Epoch 24, Batch 88/100] Loss: 0.2045 | RMSE: 0.1179 | Consistency: 0.0575 | SSIM: 0.8603 | Grad: 61034.719\n",
      "[Epoch 24, Batch 89/100] Loss: 0.2026 | RMSE: 0.1159 | Consistency: 0.0568 | SSIM: 0.8614 | Grad: 27804.004\n",
      "[Epoch 24, Batch 90/100] Loss: 0.2074 | RMSE: 0.1206 | Consistency: 0.0615 | SSIM: 0.8614 | Grad: 104571.047\n",
      "[Epoch 24, Batch 91/100] Loss: 0.2098 | RMSE: 0.1232 | Consistency: 0.0561 | SSIM: 0.8603 | Grad: 107619.023\n",
      "[Epoch 24, Batch 92/100] Loss: 0.2033 | RMSE: 0.1169 | Consistency: 0.0592 | SSIM: 0.8580 | Grad: 45937.719\n",
      "[Epoch 24, Batch 93/100] Loss: 0.2066 | RMSE: 0.1199 | Consistency: 0.0613 | SSIM: 0.8613 | Grad: 77936.102\n",
      "[Epoch 24, Batch 94/100] Loss: 0.2064 | RMSE: 0.1196 | Consistency: 0.0599 | SSIM: 0.8621 | Grad: 82744.859\n",
      "[Epoch 24, Batch 95/100] Loss: 0.2003 | RMSE: 0.1138 | Consistency: 0.0535 | SSIM: 0.8596 | Grad: 54278.824\n",
      "[Epoch 24, Batch 96/100] Loss: 0.2125 | RMSE: 0.1256 | Consistency: 0.0678 | SSIM: 0.8622 | Grad: 61509.125\n",
      "[Epoch 24, Batch 97/100] Loss: 0.2065 | RMSE: 0.1198 | Consistency: 0.0620 | SSIM: 0.8605 | Grad: 60974.832\n",
      "[Epoch 24, Batch 98/100] Loss: 0.2007 | RMSE: 0.1145 | Consistency: 0.0580 | SSIM: 0.8562 | Grad: 49438.012\n",
      "[Epoch 24, Batch 99/100] Loss: 0.2024 | RMSE: 0.1158 | Consistency: 0.0556 | SSIM: 0.8610 | Grad: 48805.457\n",
      "[Epoch 24, Batch 100/100] Loss: 0.2077 | RMSE: 0.1209 | Consistency: 0.0643 | SSIM: 0.8610 | Grad: 62868.977\n",
      "✅ Epoch 24 완료!\n",
      "   📈 평균 Loss: 0.203457\n",
      "   📊 평균 RMSE: 0.116888\n",
      "   ⚙️  학습률: 0.000883\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 25/80 시작...\n",
      "[Epoch 25, Batch 1/100] Loss: 0.2098 | RMSE: 0.1232 | Consistency: 0.0653 | SSIM: 0.8589 | Grad: 82833.773\n",
      "[Epoch 25, Batch 2/100] Loss: 0.2055 | RMSE: 0.1188 | Consistency: 0.0618 | SSIM: 0.8601 | Grad: 65379.270\n",
      "[Epoch 25, Batch 3/100] Loss: 0.2035 | RMSE: 0.1171 | Consistency: 0.0578 | SSIM: 0.8575 | Grad: 31870.434\n",
      "[Epoch 25, Batch 4/100] Loss: 0.2062 | RMSE: 0.1196 | Consistency: 0.0591 | SSIM: 0.8600 | Grad: 43447.957\n",
      "[Epoch 25, Batch 5/100] Loss: 0.2015 | RMSE: 0.1150 | Consistency: 0.0597 | SSIM: 0.8592 | Grad: 61647.297\n",
      "[Epoch 25, Batch 6/100] Loss: 0.1997 | RMSE: 0.1133 | Consistency: 0.0542 | SSIM: 0.8587 | Grad: 41528.938\n",
      "[Epoch 25, Batch 7/100] Loss: 0.2013 | RMSE: 0.1149 | Consistency: 0.0584 | SSIM: 0.8586 | Grad: 57736.543\n",
      "[Epoch 25, Batch 8/100] Loss: 0.2048 | RMSE: 0.1184 | Consistency: 0.0586 | SSIM: 0.8578 | Grad: 20986.711\n",
      "[Epoch 25, Batch 9/100] Loss: 0.2006 | RMSE: 0.1142 | Consistency: 0.0549 | SSIM: 0.8588 | Grad: 57589.066\n",
      "[Epoch 25, Batch 10/100] Loss: 0.2099 | RMSE: 0.1232 | Consistency: 0.0731 | SSIM: 0.8597 | Grad: 71733.656\n",
      "[Epoch 25, Batch 11/100] Loss: 0.2000 | RMSE: 0.1136 | Consistency: 0.0551 | SSIM: 0.8584 | Grad: 19660.404\n",
      "[Epoch 25, Batch 12/100] Loss: 0.2006 | RMSE: 0.1142 | Consistency: 0.0536 | SSIM: 0.8587 | Grad: 57653.254\n",
      "[Epoch 25, Batch 13/100] Loss: 0.2065 | RMSE: 0.1198 | Consistency: 0.0600 | SSIM: 0.8607 | Grad: 70382.297\n",
      "[Epoch 25, Batch 14/100] Loss: 0.1991 | RMSE: 0.1124 | Consistency: 0.0552 | SSIM: 0.8611 | Grad: 48049.363\n",
      "[Epoch 25, Batch 15/100] Loss: 0.2085 | RMSE: 0.1219 | Consistency: 0.0646 | SSIM: 0.8591 | Grad: 7966.526\n",
      "[Epoch 25, Batch 16/100] Loss: 0.2024 | RMSE: 0.1159 | Consistency: 0.0564 | SSIM: 0.8596 | Grad: 54555.539\n",
      "[Epoch 25, Batch 17/100] Loss: 0.2044 | RMSE: 0.1179 | Consistency: 0.0571 | SSIM: 0.8587 | Grad: 48015.695\n",
      "[Epoch 25, Batch 18/100] Loss: 0.2069 | RMSE: 0.1202 | Consistency: 0.0597 | SSIM: 0.8614 | Grad: 50032.426\n",
      "[Epoch 25, Batch 19/100] Loss: 0.2032 | RMSE: 0.1167 | Consistency: 0.0607 | SSIM: 0.8587 | Grad: 11216.896\n",
      "[Epoch 25, Batch 20/100] Loss: 0.2050 | RMSE: 0.1183 | Consistency: 0.0585 | SSIM: 0.8612 | Grad: 51239.727\n",
      "[Epoch 25, Batch 21/100] Loss: 0.2094 | RMSE: 0.1224 | Consistency: 0.0660 | SSIM: 0.8631 | Grad: 84778.914\n",
      "[Epoch 25, Batch 22/100] Loss: 0.2019 | RMSE: 0.1155 | Consistency: 0.0576 | SSIM: 0.8584 | Grad: 30591.375\n",
      "[Epoch 25, Batch 23/100] Loss: 0.2078 | RMSE: 0.1210 | Consistency: 0.0604 | SSIM: 0.8617 | Grad: 33241.891\n",
      "[Epoch 25, Batch 24/100] Loss: 0.2002 | RMSE: 0.1135 | Consistency: 0.0541 | SSIM: 0.8616 | Grad: 31854.439\n",
      "[Epoch 25, Batch 25/100] Loss: 0.2137 | RMSE: 0.1268 | Consistency: 0.0674 | SSIM: 0.8614 | Grad: 73137.258\n",
      "[Epoch 25, Batch 26/100] Loss: 0.2051 | RMSE: 0.1185 | Consistency: 0.0579 | SSIM: 0.8608 | Grad: 32803.797\n",
      "[Epoch 25, Batch 27/100] Loss: 0.2047 | RMSE: 0.1180 | Consistency: 0.0608 | SSIM: 0.8602 | Grad: 35367.305\n",
      "[Epoch 25, Batch 28/100] Loss: 0.2117 | RMSE: 0.1248 | Consistency: 0.0637 | SSIM: 0.8620 | Grad: 91975.805\n",
      "[Epoch 25, Batch 29/100] Loss: 0.2095 | RMSE: 0.1229 | Consistency: 0.0555 | SSIM: 0.8598 | Grad: 88883.992\n",
      "[Epoch 25, Batch 30/100] Loss: 0.2049 | RMSE: 0.1181 | Consistency: 0.0584 | SSIM: 0.8616 | Grad: 56131.742\n",
      "[Epoch 25, Batch 31/100] Loss: 0.2043 | RMSE: 0.1178 | Consistency: 0.0603 | SSIM: 0.8593 | Grad: 45217.906\n",
      "[Epoch 25, Batch 32/100] Loss: 0.2050 | RMSE: 0.1183 | Consistency: 0.0625 | SSIM: 0.8603 | Grad: 59884.508\n",
      "[Epoch 25, Batch 33/100] Loss: 0.2070 | RMSE: 0.1201 | Consistency: 0.0628 | SSIM: 0.8620 | Grad: 80922.797\n",
      "[Epoch 25, Batch 34/100] Loss: 0.2016 | RMSE: 0.1148 | Consistency: 0.0580 | SSIM: 0.8615 | Grad: 30545.363\n",
      "[Epoch 25, Batch 35/100] Loss: 0.2074 | RMSE: 0.1209 | Consistency: 0.0601 | SSIM: 0.8592 | Grad: 67720.492\n",
      "[Epoch 25, Batch 36/100] Loss: 0.1980 | RMSE: 0.1117 | Consistency: 0.0530 | SSIM: 0.8577 | Grad: 26383.641\n",
      "[Epoch 25, Batch 37/100] Loss: 0.1987 | RMSE: 0.1122 | Consistency: 0.0524 | SSIM: 0.8597 | Grad: 48828.531\n",
      "[Epoch 25, Batch 38/100] Loss: 0.2070 | RMSE: 0.1204 | Consistency: 0.0645 | SSIM: 0.8593 | Grad: 68231.297\n",
      "[Epoch 25, Batch 39/100] Loss: 0.2010 | RMSE: 0.1145 | Consistency: 0.0570 | SSIM: 0.8594 | Grad: 18842.264\n",
      "[Epoch 25, Batch 40/100] Loss: 0.2018 | RMSE: 0.1151 | Consistency: 0.0555 | SSIM: 0.8609 | Grad: 29991.773\n",
      "[Epoch 25, Batch 41/100] Loss: 0.2018 | RMSE: 0.1153 | Consistency: 0.0558 | SSIM: 0.8592 | Grad: 56801.691\n",
      "[Epoch 25, Batch 42/100] Loss: 0.2003 | RMSE: 0.1138 | Consistency: 0.0537 | SSIM: 0.8594 | Grad: 46378.297\n",
      "[Epoch 25, Batch 43/100] Loss: 0.2033 | RMSE: 0.1169 | Consistency: 0.0596 | SSIM: 0.8582 | Grad: 26570.729\n",
      "[Epoch 25, Batch 44/100] Loss: 0.2034 | RMSE: 0.1167 | Consistency: 0.0595 | SSIM: 0.8611 | Grad: 62872.195\n",
      "[Epoch 25, Batch 45/100] Loss: 0.2010 | RMSE: 0.1146 | Consistency: 0.0539 | SSIM: 0.8583 | Grad: 57793.371\n",
      "[Epoch 25, Batch 46/100] Loss: 0.2017 | RMSE: 0.1153 | Consistency: 0.0550 | SSIM: 0.8581 | Grad: 32580.598\n",
      "[Epoch 25, Batch 47/100] Loss: 0.1997 | RMSE: 0.1131 | Consistency: 0.0544 | SSIM: 0.8604 | Grad: 14103.820\n",
      "[Epoch 25, Batch 48/100] Loss: 0.2045 | RMSE: 0.1178 | Consistency: 0.0550 | SSIM: 0.8611 | Grad: 71272.023\n",
      "[Epoch 25, Batch 49/100] Loss: 0.2083 | RMSE: 0.1218 | Consistency: 0.0579 | SSIM: 0.8595 | Grad: 90045.742\n",
      "[Epoch 25, Batch 50/100] Loss: 0.2052 | RMSE: 0.1186 | Consistency: 0.0620 | SSIM: 0.8599 | Grad: 45475.328\n",
      "[Epoch 25, Batch 51/100] Loss: 0.2051 | RMSE: 0.1184 | Consistency: 0.0564 | SSIM: 0.8610 | Grad: 27633.195\n",
      "[Epoch 25, Batch 52/100] Loss: 0.2018 | RMSE: 0.1152 | Consistency: 0.0532 | SSIM: 0.8600 | Grad: 54380.840\n",
      "[Epoch 25, Batch 53/100] Loss: 0.2045 | RMSE: 0.1180 | Consistency: 0.0563 | SSIM: 0.8592 | Grad: 76742.953\n",
      "[Epoch 25, Batch 54/100] Loss: 0.2046 | RMSE: 0.1181 | Consistency: 0.0585 | SSIM: 0.8587 | Grad: 51795.797\n",
      "[Epoch 25, Batch 55/100] Loss: 0.2013 | RMSE: 0.1148 | Consistency: 0.0555 | SSIM: 0.8600 | Grad: 18894.354\n",
      "[Epoch 25, Batch 56/100] Loss: 0.2034 | RMSE: 0.1167 | Consistency: 0.0549 | SSIM: 0.8610 | Grad: 68013.672\n",
      "[Epoch 25, Batch 57/100] Loss: 0.2046 | RMSE: 0.1180 | Consistency: 0.0545 | SSIM: 0.8602 | Grad: 70375.945\n",
      "[Epoch 25, Batch 58/100] Loss: 0.2034 | RMSE: 0.1169 | Consistency: 0.0543 | SSIM: 0.8593 | Grad: 70136.883\n",
      "[Epoch 25, Batch 59/100] Loss: 0.2064 | RMSE: 0.1197 | Consistency: 0.0619 | SSIM: 0.8603 | Grad: 61648.602\n",
      "[Epoch 25, Batch 60/100] Loss: 0.2055 | RMSE: 0.1189 | Consistency: 0.0587 | SSIM: 0.8602 | Grad: 50354.523\n",
      "[Epoch 25, Batch 61/100] Loss: 0.2013 | RMSE: 0.1151 | Consistency: 0.0531 | SSIM: 0.8567 | Grad: 45276.297\n",
      "[Epoch 25, Batch 62/100] Loss: 0.1994 | RMSE: 0.1129 | Consistency: 0.0550 | SSIM: 0.8594 | Grad: 16135.067\n",
      "[Epoch 25, Batch 63/100] Loss: 0.2035 | RMSE: 0.1167 | Consistency: 0.0592 | SSIM: 0.8619 | Grad: 54874.160\n",
      "[Epoch 25, Batch 64/100] Loss: 0.2045 | RMSE: 0.1179 | Consistency: 0.0576 | SSIM: 0.8602 | Grad: 55856.699\n",
      "[Epoch 25, Batch 65/100] Loss: 0.2043 | RMSE: 0.1176 | Consistency: 0.0603 | SSIM: 0.8610 | Grad: 39873.879\n",
      "[Epoch 25, Batch 66/100] Loss: 0.2011 | RMSE: 0.1147 | Consistency: 0.0555 | SSIM: 0.8583 | Grad: 52979.570\n",
      "[Epoch 25, Batch 67/100] Loss: 0.2016 | RMSE: 0.1152 | Consistency: 0.0548 | SSIM: 0.8583 | Grad: 46210.613\n",
      "[Epoch 25, Batch 68/100] Loss: 0.2056 | RMSE: 0.1189 | Consistency: 0.0606 | SSIM: 0.8609 | Grad: 62320.047\n",
      "[Epoch 25, Batch 69/100] Loss: 0.2063 | RMSE: 0.1197 | Consistency: 0.0598 | SSIM: 0.8602 | Grad: 49460.789\n",
      "[Epoch 25, Batch 70/100] Loss: 0.2021 | RMSE: 0.1155 | Consistency: 0.0580 | SSIM: 0.8597 | Grad: 25477.803\n",
      "[Epoch 25, Batch 71/100] Loss: 0.2001 | RMSE: 0.1137 | Consistency: 0.0553 | SSIM: 0.8586 | Grad: 41576.715\n",
      "[Epoch 25, Batch 72/100] Loss: 0.2005 | RMSE: 0.1139 | Consistency: 0.0541 | SSIM: 0.8601 | Grad: 15915.600\n",
      "[Epoch 25, Batch 73/100] Loss: 0.2065 | RMSE: 0.1199 | Consistency: 0.0611 | SSIM: 0.8607 | Grad: 86960.953\n",
      "[Epoch 25, Batch 74/100] Loss: 0.2026 | RMSE: 0.1162 | Consistency: 0.0572 | SSIM: 0.8583 | Grad: 60625.809\n",
      "[Epoch 25, Batch 75/100] Loss: 0.2027 | RMSE: 0.1163 | Consistency: 0.0584 | SSIM: 0.8583 | Grad: 56139.152\n",
      "[Epoch 25, Batch 76/100] Loss: 0.2040 | RMSE: 0.1174 | Consistency: 0.0580 | SSIM: 0.8601 | Grad: 60119.703\n",
      "[Epoch 25, Batch 77/100] Loss: 0.2098 | RMSE: 0.1231 | Consistency: 0.0617 | SSIM: 0.8607 | Grad: 93095.109\n",
      "[Epoch 25, Batch 78/100] Loss: 0.2058 | RMSE: 0.1193 | Consistency: 0.0590 | SSIM: 0.8590 | Grad: 87065.609\n",
      "[Epoch 25, Batch 79/100] Loss: 0.2038 | RMSE: 0.1170 | Consistency: 0.0593 | SSIM: 0.8620 | Grad: 55903.438\n",
      "[Epoch 25, Batch 80/100] Loss: 0.2036 | RMSE: 0.1170 | Consistency: 0.0517 | SSIM: 0.8605 | Grad: 75279.453\n",
      "[Epoch 25, Batch 81/100] Loss: 0.2064 | RMSE: 0.1198 | Consistency: 0.0577 | SSIM: 0.8609 | Grad: 74545.562\n",
      "[Epoch 25, Batch 82/100] Loss: 0.2056 | RMSE: 0.1192 | Consistency: 0.0581 | SSIM: 0.8585 | Grad: 86047.875\n",
      "[Epoch 25, Batch 83/100] Loss: 0.2039 | RMSE: 0.1173 | Consistency: 0.0551 | SSIM: 0.8604 | Grad: 60782.121\n",
      "[Epoch 25, Batch 84/100] Loss: 0.2022 | RMSE: 0.1158 | Consistency: 0.0519 | SSIM: 0.8580 | Grad: 69664.219\n",
      "[Epoch 25, Batch 85/100] Loss: 0.2087 | RMSE: 0.1222 | Consistency: 0.0611 | SSIM: 0.8595 | Grad: 95624.328\n",
      "[Epoch 25, Batch 86/100] Loss: 0.2048 | RMSE: 0.1186 | Consistency: 0.0610 | SSIM: 0.8561 | Grad: 52696.000\n",
      "[Epoch 25, Batch 87/100] Loss: 0.2018 | RMSE: 0.1152 | Consistency: 0.0559 | SSIM: 0.8607 | Grad: 46454.523\n",
      "[Epoch 25, Batch 88/100] Loss: 0.2038 | RMSE: 0.1172 | Consistency: 0.0605 | SSIM: 0.8608 | Grad: 51484.508\n",
      "[Epoch 25, Batch 89/100] Loss: 0.2009 | RMSE: 0.1146 | Consistency: 0.0553 | SSIM: 0.8575 | Grad: 49757.223\n",
      "[Epoch 25, Batch 90/100] Loss: 0.1959 | RMSE: 0.1095 | Consistency: 0.0506 | SSIM: 0.8595 | Grad: 12587.154\n",
      "[Epoch 25, Batch 91/100] Loss: 0.2029 | RMSE: 0.1162 | Consistency: 0.0587 | SSIM: 0.8609 | Grad: 34124.797\n",
      "[Epoch 25, Batch 92/100] Loss: 0.2015 | RMSE: 0.1151 | Consistency: 0.0532 | SSIM: 0.8581 | Grad: 43631.238\n",
      "[Epoch 25, Batch 93/100] Loss: 0.2023 | RMSE: 0.1156 | Consistency: 0.0557 | SSIM: 0.8609 | Grad: 34473.695\n",
      "[Epoch 25, Batch 94/100] Loss: 0.1975 | RMSE: 0.1111 | Consistency: 0.0505 | SSIM: 0.8586 | Grad: 17830.803\n",
      "[Epoch 25, Batch 95/100] Loss: 0.2040 | RMSE: 0.1172 | Consistency: 0.0639 | SSIM: 0.8612 | Grad: 74877.531\n",
      "[Epoch 25, Batch 96/100] Loss: 0.2071 | RMSE: 0.1205 | Consistency: 0.0644 | SSIM: 0.8586 | Grad: 79441.070\n",
      "[Epoch 25, Batch 97/100] Loss: 0.2020 | RMSE: 0.1158 | Consistency: 0.0556 | SSIM: 0.8571 | Grad: 41555.281\n",
      "[Epoch 25, Batch 98/100] Loss: 0.2009 | RMSE: 0.1144 | Consistency: 0.0582 | SSIM: 0.8585 | Grad: 46310.902\n",
      "[Epoch 25, Batch 99/100] Loss: 0.2080 | RMSE: 0.1215 | Consistency: 0.0632 | SSIM: 0.8587 | Grad: 52807.902\n",
      "[Epoch 25, Batch 100/100] Loss: 0.1986 | RMSE: 0.1121 | Consistency: 0.0533 | SSIM: 0.8593 | Grad: 36583.363\n",
      "✅ Epoch 25 완료!\n",
      "   📈 평균 Loss: 0.203775\n",
      "   📊 평균 RMSE: 0.117225\n",
      "   ⚙️  학습률: 0.000868\n",
      "🛑 조기 종료! 15 에포크 개선 없음\n",
      "모델이 'best_model_noise.pth'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 훈련 루프 설정\n",
    "# =============================================================================\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "max_batches = min(100, len(train_loader))\n",
    "log_interval = 1\n",
    "print(f\"총 샘플 수: {len(train_dataset)}\")\n",
    "print(f\"배치 크기: {batch_size}\")\n",
    "print(f\"총 배치 개수: {max_batches}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=max_batches,\n",
    "    epochs=training_epochs,\n",
    "    pct_start=0.1\n",
    ")\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_rmse_sum = 0.0\n",
    "\n",
    "    print(f\"\\n📊 Epoch {epoch + 1}/{training_epochs} 시작...\")\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for i, (input_phase, target_phases) in enumerate(train_loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        input_phase = input_phase.to(device)         # shape: (B, 1, H, W)\n",
    "        target_phase1 = target_phases[:, 0:1].to(device)  # shape: (B, 1, H, W)\n",
    "        target_phase2 = target_phases[:, 1:2].to(device)\n",
    "        target_phase3 = target_phases[:, 2:3].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # 모델 예측\n",
    "            output1, output2, output3 = model(input_phase)\n",
    "            # 손실 계산\n",
    "            loss, loss_details = improved_physics_loss(\n",
    "                output1, target_phase1,\n",
    "                output2, target_phase2,\n",
    "                output3, target_phase3,\n",
    "                epoch=epoch\n",
    "            )\n",
    "\n",
    "        # 역전파 및 업데이트\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)  # AMP에서도 가능\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        epoch_rmse_sum += loss_details['rmse']\n",
    "\n",
    "        # ✅ 로그 출력\n",
    "        print(f\"[Epoch {epoch + 1}, Batch {i + 1}/{max_batches}] \"\n",
    "              f\"Loss: {loss.item():.4f} | \"\n",
    "              f\"RMSE: {loss_details['rmse']:.4f} | \"\n",
    "              f\"Consistency: {loss_details['consistency']:.4f} | \"\n",
    "              f\"SSIM: {loss_details['ssim']:.4f} | \"\n",
    "              f\"Grad: {grad_norm:.3f}\")\n",
    "\n",
    "\n",
    "        # 에포크 마무리\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        avg_loss = running_loss / max_batches\n",
    "        avg_rmse = epoch_rmse_sum / max_batches\n",
    "\n",
    "    print(f\"✅ Epoch {epoch + 1} 완료!\")\n",
    "    print(f\"   📈 평균 Loss: {avg_loss:.6f}\")\n",
    "    print(f\"   📊 평균 RMSE: {avg_rmse:.6f}\")\n",
    "    print(f\"   ⚙️  학습률: {current_lr:.6f}\")\n",
    "\n",
    "    # 모델 저장 조건\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_noise.pth')\n",
    "        print(f\"   🏆 NEW BEST! RMSE: {avg_rmse:.6f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # 조기 종료 조건\n",
    "    if patience_counter >= patience_limit:\n",
    "        print(f\"🛑 조기 종료! {patience_limit} 에포크 개선 없음\")\n",
    "        break\n",
    "\n",
    "    if avg_rmse <= 0.0036:\n",
    "        print(f\"🎊 목표 달성! RMSE: {avg_rmse:.6f} <= 0.0036\")\n",
    "        break\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "print(\"모델이 'best_model_noise.pth'로 저장되었습니다.\")\t  \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ae459-0046-4d8d-bec9-94b580528791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
