import os
import json
import cv2
import numpy as np
from datetime import datetime
import time
from typing import Tuple, Optional
import torch
from ultralytics import YOLO
import mediapipe as mp
from collections import deque
import threading
from queue import Queue

# ========================================
# í•´ìƒë„ ì„¤ì • (Camera_1.pyì™€ ë™ì¼)
# ========================================
DEFAULT_WIDTH = 1920          # ê¸°ë³¸ í•´ìƒë„ ë„ˆë¹„
DEFAULT_HEIGHT = 1080         # ê¸°ë³¸ í•´ìƒë„ ë†’ì´
DEFAULT_FPS = 60              # ê¸°ë³¸ FPS

# ========================================
# ì¹´ë©”ë¼ ì¸ë±ìŠ¤ ì„¤ì • (Camera_1.pyì™€ ë™ì¼)
# ========================================
LEFT_CAMERA_INDEX = 2         # ì™¼ìª½ ì¹´ë©”ë¼ (Camera_1.py: cap_L = open_cam(2, selected_res))
RIGHT_CAMERA_INDEX = 0        # ì˜¤ë¥¸ìª½ ì¹´ë©”ë¼ (Camera_1.py: cap_R = open_cam(0, selected_res))
# ========================================

# ========================================
# ë””ìŠ¤í”Œë ˆì´ ìŠ¤ì¼€ì¼ ì„¤ì •
# ========================================
DISPLAY_SCALE = 0.5          # í™”ë©´ í‘œì‹œìš© ìŠ¤ì¼€ì¼ (0.5 = 50% í¬ê¸°, 1.0 = 100% í¬ê¸°)

# ========================================
# ì•„ì´íŠ¸ë˜í‚¹ ì„¤ì •
# ========================================
GAZE_TRACKING_CONFIG = {
    'max_faces': 3,  # ìµœëŒ€ ì¶”ì í•  ì–¼êµ´ ìˆ˜
    'detection_confidence': 0.7,
    'tracking_confidence': 0.7,
    'stabilizer_buffer_size': 5,
    'outlier_threshold': 0.1
}

# ========================================
# ë™ê³µ ì¶”ì  ì‹¤í—˜ ì„¤ì •
# ========================================
GAZE_EXPERIMENT_CONFIG = {
    'experiment_mode': False,  # ì‹¤í—˜ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
    'target_a_mode': False,    # íƒ€ê²Ÿ A ì‘ì‹œ ëª¨ë“œ
    'target_b_mode': False,    # íƒ€ê²Ÿ B ì‘ì‹œ ëª¨ë“œ
    'recording_duration': 10,  # ê¸°ë¡ ì‹œê°„ (ì´ˆ)
    'min_recording_duration': 1,  # ìµœì†Œ ê¸°ë¡ ì‹œê°„ (ì´ˆ)
    'frame_rate': 30,          # ì˜ˆìƒ í”„ë ˆì„ ë ˆì´íŠ¸
    'expected_frames': 300,    # ì˜ˆìƒ í”„ë ˆì„ ìˆ˜ (10ì´ˆ * 30fps)
    'target_distance_cm': 150, # ì‚¬ìš©ìì™€ í™”ë©´ ê°„ ê±°ë¦¬ (cm)
    'target_angle_deg': 0.5,   # ëª©í‘œ ì‹œì„  ì¶”ì  ì •í™•ë„ (ë„)
    'target_distance_pixels': None,  # ê³„ì‚°ëœ ëª©í‘œ í”½ì…€ ê±°ë¦¬
    'min_detection_threshold': 0.1   # ìµœì†Œ ê°ì§€ ì„ê³„ê°’ (í”½ì…€)
}

# ========================================

class GazeStabilizer:
    """ì‹œì„  ì¶”ì  ì•ˆì •í™” í´ë˜ìŠ¤"""
    
    def __init__(self, buffer_size=None, outlier_threshold=None):
        self.buffer_size = buffer_size or GAZE_TRACKING_CONFIG['stabilizer_buffer_size']
        self.outlier_threshold = outlier_threshold or GAZE_TRACKING_CONFIG['outlier_threshold']
        self.gaze_history = deque(maxlen=self.buffer_size)
        self.iris_history = deque(maxlen=self.buffer_size)
        
    def add_sample(self, gaze, iris_coords):
        """ìƒˆë¡œìš´ ìƒ˜í”Œ ì¶”ê°€"""
        self.gaze_history.append(gaze)
        self.iris_history.append(iris_coords)
    
    def get_stabilized_gaze(self):
        """ì•ˆì •í™”ëœ ì‹œì„  ì¢Œí‘œ ë°˜í™˜"""
        if len(self.gaze_history) < 2:
            return self.gaze_history[-1] if self.gaze_history else (0, 0)
        
        # ì•„ì›ƒë¼ì´ì–´ ì œê±°
        valid_gazes = []
        for gaze in self.gaze_history:
            if self._is_valid_gaze(gaze):
                valid_gazes.append(gaze)
        
        if not valid_gazes:
            return self.gaze_history[-1]
        
        # ê°€ì¤‘ í‰ê·  (ìµœê·¼ ê°’ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        weights = np.linspace(0.5, 1.0, len(valid_gazes))
        weights = weights / np.sum(weights)
        
        avg_x = np.average([g[0] for g in valid_gazes], weights=weights)
        avg_y = np.average([g[1] for g in valid_gazes], weights=weights)
        
        return (avg_x, avg_y)
    
    def _is_valid_gaze(self, gaze):
        """ì•„ì›ƒë¼ì´ì–´ ê²€ì¶œ"""
        if len(self.gaze_history) < 2:
            return True
        
        recent_gazes = list(self.gaze_history)[-3:]
        avg_x = np.mean([g[0] for g in recent_gazes])
        avg_y = np.mean([g[1] for g in recent_gazes])
        
        distance = np.sqrt((gaze[0] - avg_x)**2 + (gaze[1] - avg_y)**2)
        return distance < self.outlier_threshold


class GazeExperimentRecorder:
    """ë™ê³µ ì¶”ì  ì‹¤í—˜ ë°ì´í„° ê¸°ë¡ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.reset()
        self._calculate_target_distance()
    
    def reset(self):
        """ì‹¤í—˜ ë°ì´í„° ì´ˆê¸°í™”"""
        self.gaze_coordinates = []  # ì‹œì„  ì¢Œí‘œ ê¸°ë¡
        self.timestamps = []        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡
        self.recording_start_time = None
        self.recording_end_time = None
        self.is_recording = False
        self.current_target = None  # 'A' ë˜ëŠ” 'B'
        self.frame_count = 0

    
    def _calculate_target_distance(self):
        """ëª©í‘œ í”½ì…€ ê±°ë¦¬ ê³„ì‚° (0.5ë„ ê°ë„ ê¸°ì¤€)"""
        # 150cm * tan(0.5ë„) â‰ˆ 1.3cm
        target_distance_cm = GAZE_EXPERIMENT_CONFIG['target_distance_cm']
        target_angle_rad = np.radians(GAZE_EXPERIMENT_CONFIG['target_angle_deg'])
        target_distance_cm = target_distance_cm * np.tan(target_angle_rad)
        
        # í™”ë©´ í•´ìƒë„ì— ë”°ë¥¸ í”½ì…€ ê±°ë¦¬ ê³„ì‚°
        # ì¼ë°˜ì ì¸ 24ì¸ì¹˜ ëª¨ë‹ˆí„° ê¸°ì¤€ (1920x1080, ì•½ 53cm ë„ˆë¹„)
        screen_width_cm = 53.0  # 24ì¸ì¹˜ ëª¨ë‹ˆí„° ë„ˆë¹„
        screen_width_pixels = 1920
        
        # 1cmë‹¹ í”½ì…€ ìˆ˜ ê³„ì‚°
        pixels_per_cm = screen_width_pixels / screen_width_cm
        
        # ëª©í‘œ í”½ì…€ ê±°ë¦¬ ê³„ì‚°
        target_pixels = target_distance_cm * pixels_per_cm
        
        GAZE_EXPERIMENT_CONFIG['target_distance_pixels'] = target_pixels
        
        print(f"ğŸ”¬ ë™ê³µ ì¶”ì  ì‹¤í—˜ ì„¤ì •:")
        print(f"   - ì‚¬ìš©ì-í™”ë©´ ê±°ë¦¬: {target_distance_cm}cm")
        print(f"   - ëª©í‘œ ê°ë„: {GAZE_EXPERIMENT_CONFIG['target_angle_deg']}ë„")
        print(f"   - ê³„ì‚°ëœ ê±°ë¦¬: {target_distance_cm:.2f}cm")
        print(f"   - ëª©í‘œ í”½ì…€ ê±°ë¦¬: {target_pixels:.1f} í”½ì…€")
        print(f"   - ìµœì†Œ ê°ì§€ ì„ê³„ê°’: {GAZE_EXPERIMENT_CONFIG['min_detection_threshold']} í”½ì…€")
    
    def start_recording(self, target):
        """ì‹¤í—˜ ê¸°ë¡ ì‹œì‘"""
        if self.is_recording:
            print(f"âš ï¸ ì´ë¯¸ ê¸°ë¡ ì¤‘ì…ë‹ˆë‹¤. ë¨¼ì € ì¤‘ì§€í•´ì£¼ì„¸ìš”.")
            return False
        
        self.reset()
        self.current_target = target
        self.recording_start_time = time.time()
        self.is_recording = True
        
        print(f"ğŸ”¬ íƒ€ê²Ÿ {target} ì‘ì‹œ ê¸°ë¡ ì‹œì‘")
        print(f"   - ëª©í‘œ ê¸°ë¡ ì‹œê°„: {GAZE_EXPERIMENT_CONFIG['recording_duration']}ì´ˆ")
        print(f"   - ìœ íš¨ ê¸°ë¡ ì‹œê°„: {GAZE_EXPERIMENT_CONFIG['min_recording_duration']}ì´ˆ ì´ìƒ (ì œí•œ ì—†ìŒ)")
        print(f"   - ì˜ˆìƒ í”„ë ˆì„ ìˆ˜: {GAZE_EXPERIMENT_CONFIG['expected_frames']}")
        print(f"   - ëª©í‘œ í”½ì…€ ê±°ë¦¬: {GAZE_EXPERIMENT_CONFIG['target_distance_pixels']:.1f} í”½ì…€")
        print(f"   - ìµœì†Œ ê°ì§€ ì„ê³„ê°’: {GAZE_EXPERIMENT_CONFIG['min_detection_threshold']} í”½ì…€")
        return True
    
    def stop_recording(self):
        """ì‹¤í—˜ ê¸°ë¡ ì¤‘ì§€"""
        if not self.is_recording:
            print(f"âš ï¸ ê¸°ë¡ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return False
        
        self.recording_end_time = time.time()
        self.is_recording = False
        
        actual_duration = self.recording_end_time - self.recording_start_time
        actual_frames = len(self.gaze_coordinates)
        
        print(f"\nğŸ”¬ íƒ€ê²Ÿ {self.current_target} ì‘ì‹œ ê¸°ë¡ ì™„ë£Œ!")
        print(f"   - ì‹¤ì œ ê¸°ë¡ ì‹œê°„: {actual_duration:.2f}ì´ˆ")
        print(f"   - ì‹¤ì œ í”„ë ˆì„ ìˆ˜: {actual_frames}")
        print(f"   - í‰ê·  FPS: {actual_frames/actual_duration:.1f}")
        
        # ìœ íš¨í•œ ê¸°ë¡ ì‹œê°„ì¸ì§€ í™•ì¸ (ìµœì†Œ 1ì´ˆë§Œ í™•ì¸)
        if actual_duration < GAZE_EXPERIMENT_CONFIG['min_recording_duration']:
            print(f"âš ï¸ ê¸°ë¡ ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ {GAZE_EXPERIMENT_CONFIG['min_recording_duration']}ì´ˆ í•„ìš”")
            return False
        
        # 9ì´ˆ ì œí•œ ì œê±° - ëª¨ë“  ë°ì´í„° ì‚¬ìš©!
        print(f"âœ… ê¸°ë¡ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"   - ëª¨ë“  {actual_frames}ê°œ í”„ë ˆì„ ë°ì´í„° ì‚¬ìš©")
        return True
    
    def add_gaze_sample(self, gaze_x, gaze_y):
        """ì‹œì„  ì¢Œí‘œ ìƒ˜í”Œ ì¶”ê°€"""
        if not self.is_recording:
            return
        
        current_time = time.time()
        elapsed_time = current_time - self.recording_start_time
        
        # ëª©í‘œ ì‹œê°„ì— ë„ë‹¬í•˜ë©´ ìë™ ì¤‘ì§€
        if elapsed_time >= GAZE_EXPERIMENT_CONFIG['recording_duration']:
            # ìë™ ì¤‘ì§€ ì‹œ ì¦‰ì‹œ í†µê³„ ì €ì¥
            self.stop_recording()
            
            # ë¶€ëª¨ í´ë˜ìŠ¤ì— í†µê³„ ì €ì¥ ìš”ì²­
            if hasattr(self, '_parent_stitcher'):
                self._parent_stitcher._save_experiment_stats_auto(self.current_target)
            
            return
        
        # ì¢Œí‘œ ê¸°ë¡
        self.gaze_coordinates.append((gaze_x, gaze_y))
        self.timestamps.append(current_time)
        self.frame_count += 1
    
    def get_recording_progress(self):
        """í˜„ì¬ ê¸°ë¡ ì§„í–‰ ìƒí™© ë°˜í™˜"""
        if not self.is_recording:
            return 0.0, 0.0
        
        current_time = time.time()
        elapsed_time = current_time - self.recording_start_time
        progress_percent = (elapsed_time / GAZE_EXPERIMENT_CONFIG['recording_duration']) * 100
        
        return elapsed_time, progress_percent
    
    def get_statistics(self):
        """ê¸°ë¡ëœ ë°ì´í„° í†µê³„ ê³„ì‚°"""
        if not self.gaze_coordinates:
            return None
        
        # ì¢Œí‘œ ë¶„ë¦¬
        x_coords = [coord[0] for coord in self.gaze_coordinates]
        y_coords = [coord[1] for coord in self.gaze_coordinates]
        
        # ê¸°ë³¸ í†µê³„
        stats = {
            'target': self.current_target,
            'frame_count': len(self.gaze_coordinates),
            'duration': self.recording_end_time - self.recording_start_time if self.recording_end_time else 0,
            'mean_x': np.mean(x_coords),
            'mean_y': np.mean(y_coords),
            'std_x': np.std(x_coords),
            'std_y': np.std(y_coords),
            'min_x': np.min(x_coords),
            'max_x': np.max(x_coords),
            'min_y': np.min(y_coords),
            'max_y': np.max(y_coords),
            'range_x': np.max(x_coords) - np.min(x_coords),
            'range_y': np.max(y_coords) - np.min(y_coords)
        }
        
        # ì‹œì„  ì•ˆì •ì„± ì§€í‘œ (í‘œì¤€í¸ì°¨ì˜ í‰ê· )
        stats['stability_score'] = np.mean([stats['std_x'], stats['std_y']])
        
        return stats
    
    def print_statistics(self):
        """í†µê³„ ê²°ê³¼ ì¶œë ¥"""
        stats = self.get_statistics()
        if not stats:
            print("âš ï¸ í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ”¬ íƒ€ê²Ÿ {stats['target']} ì‘ì‹œ í†µê³„ ê²°ê³¼:")
        print(f"{'='*50}")
        print(f"   ğŸ“Š ê¸°ë³¸ ì •ë³´:")
        print(f"      - í”„ë ˆì„ ìˆ˜: {stats['frame_count']}")
        print(f"      - ê¸°ë¡ ì‹œê°„: {stats['duration']:.2f}ì´ˆ")
        print(f"      - í‰ê·  FPS: {stats['frame_count']/stats['duration']:.1f}")
        
        print(f"   ğŸ“ ì‹œì„  ì¢Œí‘œ í†µê³„:")
        print(f"      - í‰ê·  ìœ„ì¹˜: ({stats['mean_x']:.2f}, {stats['mean_y']:.2f})")
        print(f"      - Xì¶• í‘œì¤€í¸ì°¨: {stats['std_x']:.2f} í”½ì…€")
        print(f"      - Yì¶• í‘œì¤€í¸ì°¨: {stats['std_y']:.2f} í”½ì…€")
        print(f"      - Xì¶• ë²”ìœ„: {stats['min_x']:.1f} ~ {stats['max_x']:.1f} ({stats['range_x']:.1f} í”½ì…€)")
        print(f"      - Yì¶• ë²”ìœ„: {stats['min_y']:.1f} ~ {stats['max_y']:.1f} ({stats['range_y']:.1f} í”½ì…€)")
        
        print(f"   ğŸ¯ ì•ˆì •ì„± ì§€í‘œ:")
        print(f"      - ì‹œì„  ì•ˆì •ì„± ì ìˆ˜: {stats['stability_score']:.2f} í”½ì…€")
        print(f"      - ëª©í‘œ ê°ì§€ ê±°ë¦¬: {GAZE_EXPERIMENT_CONFIG['target_distance_pixels']:.1f} í”½ì…€")
        
        # 0.5ë„ ì •í™•ë„ ë‹¬ì„± ì—¬ë¶€ íŒë‹¨
        if stats['stability_score'] <= GAZE_EXPERIMENT_CONFIG['target_distance_pixels']:
            print(f"   âœ… 0.5ë„ ì‹œì„  ì¶”ì  ì •í™•ë„ ë‹¬ì„±!")
            print(f"      - í˜„ì¬ ì•ˆì •ì„±: {stats['stability_score']:.2f} í”½ì…€")
            print(f"      - ëª©í‘œ ì•ˆì •ì„±: {GAZE_EXPERIMENT_CONFIG['target_distance_pixels']:.1f} í”½ì…€ ì´í•˜")
        else:
            print(f"   âŒ 0.5ë„ ì‹œì„  ì¶”ì  ì •í™•ë„ ë¯¸ë‹¬ì„±")
            print(f"      - í•„ìš” ì•ˆì •ì„±: {GAZE_EXPERIMENT_CONFIG['target_distance_pixels']:.1f} í”½ì…€ ì´í•˜")
            print(f"      - í˜„ì¬ ì•ˆì •ì„±: {stats['stability_score']:.2f} í”½ì…€")
        
        print(f"{'='*50}")
        print(f"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        if stats['target'] == 'A':
            print(f"   - 'b' í‚¤ë¥¼ ëˆŒëŸ¬ íƒ€ê²Ÿ B ì‘ì‹œ ê¸°ë¡ì„ ì‹œì‘í•˜ì„¸ìš”")
        else:
            print(f"   - 'c' í‚¤ë¥¼ ëˆŒëŸ¬ ë‘ íƒ€ê²Ÿ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì„¸ìš”")
            print(f"   - 'f' í‚¤ë¥¼ ëˆŒëŸ¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì„¸ìš”")
    
    def compare_targets(self, target_a_stats, target_b_stats):
        """íƒ€ê²Ÿ Aì™€ Bì˜ ê²°ê³¼ ë¹„êµ"""
        if not target_a_stats or not target_b_stats:
            print("âš ï¸ ë‘ íƒ€ê²Ÿì˜ í†µê³„ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ”¬ íƒ€ê²Ÿ A vs íƒ€ê²Ÿ B ë¹„êµ ë¶„ì„:")
        print(f"{'='*60}")
        
        # í‰ê·  ìœ„ì¹˜ ì°¨ì´ ê³„ì‚°
        mean_diff_x = abs(target_b_stats['mean_x'] - target_a_stats['mean_x'])
        mean_diff_y = abs(target_b_stats['mean_y'] - target_a_stats['mean_y'])
        mean_distance = np.sqrt(mean_diff_x**2 + mean_diff_y**2)
        
        print(f"   ğŸ“ í‰ê·  ìœ„ì¹˜ ë³€í™”:")
        print(f"      - íƒ€ê²Ÿ A: ({target_a_stats['mean_x']:.2f}, {target_a_stats['mean_y']:.2f})")
        print(f"      - íƒ€ê²Ÿ B: ({target_b_stats['mean_x']:.2f}, {target_b_stats['mean_y']:.2f})")
        print(f"      - Xì¶• ë³€í™”: {mean_diff_x:.2f} í”½ì…€")
        print(f"      - Yì¶• ë³€í™”: {mean_diff_y:.2f} í”½ì…€")
        print(f"      - ì´ ë³€í™” ê±°ë¦¬: {mean_distance:.2f} í”½ì…€")
        
        # ì•ˆì •ì„± ë¹„êµ
        stability_diff = abs(target_b_stats['stability_score'] - target_a_stats['stability_score'])
        print(f"   ğŸ¯ ì•ˆì •ì„± ë¹„êµ:")
        print(f"      - íƒ€ê²Ÿ A ì•ˆì •ì„±: {target_a_stats['stability_score']:.2f} í”½ì…€")
        print(f"      - íƒ€ê²Ÿ B ì•ˆì •ì„±: {target_b_stats['stability_score']:.2f} í”½ì…€")
        print(f"      - ì•ˆì •ì„± ì°¨ì´: {stability_diff:.2f} í”½ì…€")
        
        # 0.5ë„ ì •í™•ë„ ë‹¬ì„± ì—¬ë¶€
        target_distance = GAZE_EXPERIMENT_CONFIG['target_distance_pixels']
        print(f"   ğŸ¯ 0.5ë„ ì •í™•ë„ ë¶„ì„:")
        print(f"      - ëª©í‘œ ê°ì§€ ê±°ë¦¬: {target_distance:.1f} í”½ì…€")
        print(f"      - ì‹¤ì œ ë³€í™” ê±°ë¦¬: {mean_distance:.2f} í”½ì…€")
        
        if mean_distance >= target_distance:
            print(f"      âœ… ë³€í™” ê±°ë¦¬ê°€ ëª©í‘œ ê±°ë¦¬ ì´ìƒìœ¼ë¡œ ê°ì§€ë¨!")
            print(f"         - ì‹œìŠ¤í…œì´ 0.5ë„ ì‹œì„  ë³€í™”ë¥¼ ê°ì§€í•  ìˆ˜ ìˆìŒ")
        else:
            print(f"      âŒ ë³€í™” ê±°ë¦¬ê°€ ëª©í‘œ ê±°ë¦¬ ë¯¸ë§Œ")
            print(f"         - ì‹œìŠ¤í…œì´ 0.5ë„ ì‹œì„  ë³€í™”ë¥¼ ê°ì§€í•˜ê¸° ì–´ë ¤ì›€")
        
        print(f"{'='*60}")
    
    def save_results_to_file(self, filename=None):
        """ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"gaze_experiment_results_{timestamp}.txt"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("ë™ê³µ ì¶”ì  ì‹¤í—˜ ê²°ê³¼\n")
                f.write("="*50 + "\n\n")
                
                # ì‹¤í—˜ ì„¤ì •
                f.write("ì‹¤í—˜ ì„¤ì •:\n")
                f.write(f"- ëª©í‘œ ê°ë„: {GAZE_EXPERIMENT_CONFIG['target_angle_deg']}ë„\n")
                f.write(f"- ì‚¬ìš©ì-í™”ë©´ ê±°ë¦¬: {GAZE_EXPERIMENT_CONFIG['target_distance_cm']}cm\n")
                f.write(f"- ëª©í‘œ í”½ì…€ ê±°ë¦¬: {GAZE_EXPERIMENT_CONFIG['target_distance_pixels']:.1f} í”½ì…€\n")
                f.write(f"- ê¸°ë¡ ì‹œê°„: {GAZE_EXPERIMENT_CONFIG['recording_duration']}ì´ˆ\n\n")
                
                # ê¸°ë¡ëœ ë°ì´í„°
                if self.gaze_coordinates:
                    f.write("ê¸°ë¡ëœ ì‹œì„  ì¢Œí‘œ:\n")
                    f.write("í”„ë ˆì„\tXì¢Œí‘œ\tYì¢Œí‘œ\tíƒ€ì„ìŠ¤íƒ¬í”„\n")
                    for i, (coord, ts) in enumerate(zip(self.gaze_coordinates, self.timestamps)):
                        f.write(f"{i+1}\t{coord[0]:.2f}\t{coord[1]:.2f}\t{ts:.3f}\n")
                    
                    f.write("\n")
                    
                    # í†µê³„ ê²°ê³¼
                    stats = self.get_statistics()
                    if stats:
                        f.write("í†µê³„ ê²°ê³¼:\n")
                        f.write(f"- í‰ê·  ìœ„ì¹˜: ({stats['mean_x']:.2f}, {stats['mean_y']:.2f})\n")
                        f.write(f"- Xì¶• í‘œì¤€í¸ì°¨: {stats['std_x']:.2f}\n")
                        f.write(f"- Yì¶• í‘œì¤€í¸ì°¨: {stats['std_y']:.2f}\n")
                        f.write(f"- ì‹œì„  ì•ˆì •ì„± ì ìˆ˜: {stats['stability_score']:.2f}\n")
                
            print(f"âœ… ì‹¤í—˜ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filename
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None


class UltraFastVideoStitcherGrayscale:
    """ì´ˆê³ ì† ìµœì í™”ëœ ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ìŠ¤í‹°ì²˜ (FPS í–¥ìƒ ë²„ì „)"""
    
    def __init__(self, calibration_config_path: str):
        self.config_path = calibration_config_path
        
        # FPS ì¸¡ì •ìš© ë³€ìˆ˜ë“¤
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0.0
        
        # ğŸš€ ìµœì í™”: í•œ ë²ˆë§Œ ëª¨ë“  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë“œ
        self._load_all_calibration_data_once()
        
        # ğŸš€ ìµœì í™”: ëª¨ë“  ì‚¬ì „ ê³„ì‚° ì ìš©
        self._apply_ultra_optimizations()
        
        # ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™”
        self._initialize_gaze_tracking()
        
        # ğŸš€ ë©€í‹°ìŠ¤ë ˆë”© ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™”
        self._initialize_threaded_gaze_tracking()
        
        # ğŸš€ ë™ê³µ ì¶”ì  ì‹¤í—˜ ì´ˆê¸°í™”
        self._initialize_gaze_experiment()
        
        # ì¹´ë©”ë¼ ê°ì²´ ì´ˆê¸°í™”
        self.cap_left = None
        self.cap_right = None
        
        # í”„ë ˆì„ ì¹´ìš´í„°
        self.frame_count = 0
        
        # ğŸš€ ì„±ëŠ¥ ì¸¡ì •ìš© ë³€ìˆ˜ë“¤
        self.step_times = {
            'frame_read': [],
            'mirror_flip': [],
            'lens_rectification': [],
            'input_grayscale': [],  # ğŸ†• ì…ë ¥ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ ì‹œê°„
            'canvas_copy': [],
            'homography_warp': [],
            'left_placement': [],
            'blending': [],
            'roi_crop': [],
            'output_grayscale': [],  # ğŸ†• ì¶œë ¥ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ ì‹œê°„
            'display_resize': [],
            'total_stitching': []
        }
        self.performance_measurement = False  # ì„±ëŠ¥ ì¸¡ì • ë¹„í™œì„±í™” (ì½˜ì†” ì¶œë ¥ ì¤„ì´ê¸°)
        print(f"âœ… ì„±ëŠ¥ ì¸¡ì • ì´ˆê¸°í™” ì™„ë£Œ: {len(self.step_times)} ë‹¨ê³„")
        print(f"   - ì„±ëŠ¥ ì¸¡ì •: {'í™œì„±í™”' if self.performance_measurement else 'ë¹„í™œì„±í™”'}")
        print(f"   - FPS ë¡œê·¸: 10ì´ˆë§ˆë‹¤ ì¶œë ¥")
        print(f"   - ì„±ëŠ¥ í†µê³„: 30ì´ˆë§ˆë‹¤ ì¶œë ¥")

    def _load_all_calibration_data_once(self):
        """ğŸš€ ìµœì í™”: JSON íŒŒì¼ì„ í•œ ë²ˆë§Œ ë¡œë”©í•˜ì—¬ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬"""
        print("ğŸ”§ í•œ ë²ˆì— ëª¨ë“  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        config_dir = os.path.dirname(self.config_path)
        homography_file = os.path.join(config_dir, "homography_params.json")
        
        try:
            with open(homography_file, 'r') as f:
                self.homo_data = json.load(f)  # í•œ ë²ˆë§Œ ë¡œë”©!
            
            print("âœ… homography_params.json ë¡œë“œ ì™„ë£Œ")
            
            # ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
            self._process_homography_data()
            self._process_roi_data()  # ğŸ†• ROI ë°ì´í„° ì²˜ë¦¬ ì¶”ê°€
            self._process_blend_masks()  
            self._process_rectification_maps()
            
        except FileNotFoundError as e:
            print(f"homography_params.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("ë¨¼ì € Stitching_Engine.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”")
            raise
        except Exception as e:
            print(f"JSON ë¡œë“œ ì˜¤ë¥˜: {e}")
            raise

    def _process_homography_data(self):
        """í˜¸ëª¨ê·¸ë˜í”¼ ë°ì´í„° ì²˜ë¦¬"""
        data = self.homo_data
        self.homography_matrix = np.array(data['homography_matrix'])
        self.canvas_size = tuple(data['final_size'])
        self.left_offset = tuple(data['left_image_offset'])
        self.camera_resolution = data.get('camera_resolution', [1920, 1080])
        print(f"âœ… í˜¸ëª¨ê·¸ë˜í”¼ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {self.camera_resolution}")

    def _process_roi_data(self):
        """ğŸ†• ì‚¬ìš©ìê°€ ì„ íƒí•œ ROI ë°ì´í„° ì²˜ë¦¬"""
        print("ğŸ”§ ROI ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        # ROI ì •ë³´ íŒŒì¼ ê²½ë¡œ - ì˜¬ë°”ë¥¸ ê²½ë¡œ ê³„ì‚°
        config_dir = os.path.dirname(self.config_path)  # "./data/config"
        data_dir = os.path.dirname(config_dir)          # "./data"
        params_dir = os.path.join(data_dir, "params")   # "./data/params"
        roi_info_path = os.path.join(params_dir, "user_roi_info.json")
        roi_mask_path = os.path.join(params_dir, "user_roi_mask.npy")
        
        print(f"ğŸ” ROI íŒŒì¼ ê²½ë¡œ í™•ì¸:")
        print(f"   config_path: {self.config_path}")
        print(f"   config_dir: {config_dir}")
        print(f"   data_dir: {data_dir}")
        print(f"   params_dir: {params_dir}")
        print(f"   roi_info_path: {roi_info_path}")
        print(f"   roi_mask_path: {roi_mask_path}")
        
        # ROI ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(roi_info_path) and os.path.exists(roi_mask_path):
            try:
                # ROI ì •ë³´ ë¡œë“œ
                with open(roi_info_path, 'r') as f:
                    self.roi_info = json.load(f)
                
                # ROI ë§ˆìŠ¤í¬ ë¡œë“œ
                self.roi_mask = np.load(roi_mask_path).astype(np.uint8)
                
                # ROI ì¢Œí‘œ ì¶”ì¶œ (ì›ë³¸ í•´ìƒë„ ê¸°ì¤€)
                self.roi_x1 = self.roi_info['x1']
                self.roi_y1 = self.roi_info['y1']
                self.roi_x2 = self.roi_info['x2']
                self.roi_y2 = self.roi_info['y2']
                
                # ğŸš¨ ROI ì¢Œí‘œëŠ” ì´ë¯¸ ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€ì´ë¯€ë¡œ ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš”!
                # Stitching_Engineì—ì„œ ìƒì„±ëœ ROI ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                # (ì‚¬ìš©ìê°€ 0.5 ìŠ¤ì¼€ì¼ë¡œ ì„ íƒí–ˆì§€ë§Œ, ì¢Œí‘œëŠ” ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€)
                
                self.roi_width = self.roi_x2 - self.roi_x1
                self.roi_height = self.roi_y2 - self.roi_y1
                
                print(f"ğŸ”§ ROI ì¢Œí‘œ ìŠ¤ì¼€ì¼ ì¡°ì •:")
                print(f"   - ì›ë³¸ ROI: ({self.roi_info['x1']}, {self.roi_info['y1']}) -> ({self.roi_info['x2']}, {self.roi_info['y2']})")
                print(f"   - ìŠ¤ì¼€ì¼ ì¡°ì • í›„: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
                print(f"   - ROI í¬ê¸°: {self.roi_width} x {self.roi_height}")
                
                # ğŸš¨ ROI ì¢Œí‘œ ë²”ìœ„ ê²€ì¦ (ì›ë³¸ í•´ìƒë„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸)
                if hasattr(self, 'canvas_size'):
                    canvas_w, canvas_h = self.canvas_size[0], self.canvas_size[1]
                    if (self.roi_x1 < 0 or self.roi_y1 < 0 or 
                        self.roi_x2 > canvas_w or self.roi_y2 > canvas_h):
                        print(f"âš ï¸ ROI ì¢Œí‘œê°€ ì›ë³¸ í•´ìƒë„({canvas_w}x{canvas_h})ë¥¼ ë²—ì–´ë‚¨!")
                        print(f"   ROI ë²”ìœ„: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
                        # ì¢Œí‘œë¥¼ ì›ë³¸ í•´ìƒë„ ë‚´ë¡œ í´ë¨í”„
                        self.roi_x1 = max(0, min(self.roi_x1, canvas_w-1))
                        self.roi_y1 = max(0, min(self.roi_y1, canvas_h-1))
                        self.roi_x2 = max(self.roi_x1+1, min(self.roi_x2, canvas_w))
                        self.roi_y2 = max(self.roi_y1+1, min(self.roi_y2, canvas_h))
                        self.roi_width = self.roi_x2 - self.roi_x1
                        self.roi_height = self.roi_y2 - self.roi_y1
                        print(f"   í´ë¨í”„ í›„ ROI: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
                        print(f"   í´ë¨í”„ í›„ í¬ê¸°: {self.roi_width} x {self.roi_height}")
                
                self.use_roi = True
                
                # ğŸ†• ROIê°€ ìˆì„ ë•Œ ì›ë³¸ ìº”ë²„ìŠ¤ í¬ê¸° ë³´ì¡´ (ì „ì²´ ìŠ¤í‹°ì¹­ í›„ í¬ë¡­)
                if hasattr(self, 'canvas_size'):
                    self.original_canvas_size = self.canvas_size  # ì›ë³¸ í¬ê¸° ë³´ì¡´
                    print(f"âœ… ì›ë³¸ ìº”ë²„ìŠ¤ í¬ê¸° ë³´ì¡´: {self.canvas_size[0]}x{self.canvas_size[1]}")
                    print(f"   ROI í¬ê¸°: {self.roi_width}x{self.roi_height}")
                
                print(f"âœ… ì‚¬ìš©ì ì •ì˜ ROI ë¡œë“œ ì™„ë£Œ: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
                print(f"   ROI í¬ê¸°: {self.roi_width} x {self.roi_height}")
                
                # ğŸ†• ROI ë¸”ë Œë”© íŒŒë¼ë¯¸í„° ë¡œë“œ
                roi_blending_file = os.path.join(params_dir, 'roi_blending_params.json')
                if os.path.exists(roi_blending_file):
                    with open(roi_blending_file, 'r') as f:
                        self.roi_blending_params = json.load(f)
                    
                    # ROI ë¸”ë Œë”© ë§ˆìŠ¤í¬ ë¡œë“œ
                    roi_left_mask_file = os.path.join(params_dir, 'roi_left_blend_mask.npy')
                    roi_right_mask_file = os.path.join(params_dir, 'roi_right_blend_mask.npy')
                    
                    if os.path.exists(roi_left_mask_file) and os.path.exists(roi_right_mask_file):
                        self.roi_left_blend_mask = np.load(roi_left_mask_file)
                        self.roi_right_blend_mask = np.load(roi_right_mask_file)
                        print(f"âœ… ROI ë¸”ë Œë”© íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ:")
                        print(f"   - ë¸”ë Œë”© ë§ˆìŠ¤í¬: {self.roi_left_blend_mask.shape}")
                        print(f"   - ROI ì™¼ìª½ ì˜¤í”„ì…‹: {self.roi_blending_params['roi_left_offset']}")
                        print(f"   - ROI ì™¼ìª½ í¬ê¸°: {self.roi_blending_params['roi_left_size']}")
                    else:
                        print(f"âš ï¸ ROI ë¸”ë Œë”© ë§ˆìŠ¤í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        print(f"   ì°¾ëŠ” íŒŒì¼: {roi_left_mask_file}, {roi_right_mask_file}")
                else:
                    print(f"â„¹ï¸ ROI ë¸”ë Œë”© íŒŒë¼ë¯¸í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {roi_blending_file}")
                    print(f"   Stitching_Engine_3.pyì—ì„œ ROIë¥¼ ì„ íƒí•˜ì—¬ ë¸”ë Œë”© íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”")
                
            except Exception as e:
                print(f"âš ï¸ ROI ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.use_roi = False
        else:
            print("â„¹ï¸ ì‚¬ìš©ì ì •ì˜ ROIê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í™”ë©´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print(f"   íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:")
            print(f"     roi_info_path: {os.path.exists(roi_info_path)}")
            print(f"     roi_mask_path: {os.path.exists(roi_mask_path)}")
            self.use_roi = False

    def _process_blend_masks(self):
        """ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì²˜ë¦¬"""
        self.use_precomputed_blending = False
        
        blend_info = self.homo_data.get("blending_optimization", None)
        if blend_info is not None:
            data_dir = os.path.dirname(os.path.dirname(self.config_path))
            params_dir = os.path.join(data_dir, "params")
            left_mask_path = os.path.join(params_dir, blend_info["left_mask_file"])
            right_mask_path = os.path.join(params_dir, blend_info["right_mask_file"])
            
            if os.path.exists(left_mask_path) and os.path.exists(right_mask_path):
                # ì›ë³¸ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ë¡œë“œ (ROI ì¡°ì • ì „)
                self.left_blend_mask_original = np.load(left_mask_path).astype(np.float32)
                self.right_blend_mask_original = np.load(right_mask_path).astype(np.float32)
                self.use_precomputed_blending = True
                print(f"âœ… ì›ë³¸ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ë¡œë“œ ì™„ë£Œ: {self.left_blend_mask_original.shape}")
                
                # í•­ìƒ ì›ë³¸ ë§ˆìŠ¤í¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ROIëŠ” í‘œì‹œ ì‹œì—ë§Œ ì ìš©)
                self.left_blend_mask = self.left_blend_mask_original
                self.right_blend_mask = self.right_blend_mask_original
        else:
            print("â„¹ï¸ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def _process_rectification_maps(self):
        """ë Œì¦ˆ ì™œê³¡ ë³´ì • ë§µ ì²˜ë¦¬"""
        rect_maps = self.homo_data.get("rectification_maps", None)
        if rect_maps is None:
            raise ValueError("homography_params.jsonì— 'rectification_maps' ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        data_dir = os.path.dirname(os.path.dirname(self.config_path))
        params_dir = os.path.join(data_dir, "params")
        
        map_left_x_path = os.path.join(params_dir, rect_maps["map_left_x"])
        map_right_x_path = os.path.join(params_dir, rect_maps["map_right_x"])
        
        if not os.path.exists(map_left_x_path) or not os.path.exists(map_right_x_path):
            raise FileNotFoundError("ë Œì¦ˆ ì™œê³¡ ë³´ì • ë§µ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        self.map_left_x = np.load(map_left_x_path).astype(np.float32)
        self.map_right_x = np.load(map_right_x_path).astype(np.float32)
        
        # 2ì±„ë„ ë§µ ì²˜ë¦¬
        if len(self.map_left_x.shape) == 3 and self.map_left_x.shape[2] == 2:
            # 2ì±„ë„ ë§µì„ x, yë¡œ ë¶„ë¦¬
            self.map_left_y = self.map_left_x[:, :, 1].astype(np.float32)
            self.map_left_x = self.map_left_x[:, :, 0].astype(np.float32)
            self.map_right_y = self.map_right_x[:, :, 1].astype(np.float32)
            self.map_right_x = self.map_right_x[:, :, 0].astype(np.float32)
            print("âœ… 2ì±„ë„ ë Œì¦ˆ ì™œê³¡ ë³´ì • ë§µì„ x, yë¡œ ë¶„ë¦¬í•˜ì—¬ ë¡œë“œ ì™„ë£Œ")
        else:
            # 1ì±„ë„ ë§µ ì²˜ë¦¬
            map_left_y_path = os.path.join(params_dir, rect_maps["map_left_y"])
            map_right_y_path = os.path.join(params_dir, rect_maps["map_right_y"])
            self.map_left_y = np.load(map_left_y_path).astype(np.float32)
            self.map_right_y = np.load(map_right_y_path).astype(np.float32)
            print("âœ… 1ì±„ë„ ë Œì¦ˆ ì™œê³¡ ë³´ì • ë§µ ë¡œë“œ ì™„ë£Œ")
        
        print(f"   ì™¼ìª½ ë§µ: {self.map_left_x.shape}")
        print(f"   ì˜¤ë¥¸ìª½ ë§µ: {self.map_right_x.shape}")
        print(f"   ì™¼ìª½ map2: {self.map_left_y.shape}")
        print(f"   ì˜¤ë¥¸ìª½ map2: {self.map_right_y.shape}")

    def _apply_ultra_optimizations(self):
        """ğŸš€ ëª¨ë“  ì´ˆê³ ì† ìµœì í™”ë¥¼ í•œ ë²ˆì— ì ìš©"""
        print("ğŸš€ ì´ˆê³ ì† ìµœì í™” ì ìš© ì¤‘...")
        
        # ğŸš€ 1. ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ë¯¸ë¦¬ ìƒì„± (í•­ìƒ ì „ì²´ í¬ê¸°!)
        # ğŸš¨ í•­ìƒ ì „ì²´ í¬ê¸°ë¡œ ìƒì„± (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
        # ğŸ†• ê·¸ë ˆì´ìŠ¤ì¼€ì¼ìš© 1ì±„ë„ ìº”ë²„ìŠ¤ ìƒì„±
        self.canvas_template = np.zeros((self.canvas_size[1], self.canvas_size[0]), dtype=np.uint8)
        print(f"âœ… ì „ì²´ í¬ê¸° ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ìƒì„±: {self.canvas_template.shape}")
        
        # ğŸš¨ í•­ìƒ ì¹´ë©”ë¼ í•´ìƒë„ ê¸°ì¤€ìœ¼ë¡œ ìŠ¬ë¼ì´ìŠ¤ ì¸ë±ìŠ¤ ì„¤ì •
        self.left_slice_y = slice(self.left_offset[1], self.left_offset[1] + self.camera_resolution[1])
        self.left_slice_x = slice(self.left_offset[0], self.left_offset[0] + self.camera_resolution[0])
        print(f"âœ… ì¹´ë©”ë¼ í•´ìƒë„ ê¸°ë°˜ ìŠ¬ë¼ì´ìŠ¤ ì¸ë±ìŠ¤ ê³„ì‚°: Y{self.left_slice_y}, X{self.left_slice_x}")
        
        # ğŸ†• ROIê°€ ìˆëŠ” ê²½ìš° ROI ìµœì í™” ê³„ì‚°
        if hasattr(self, 'use_roi') and self.use_roi:
            self._calculate_roi_optimizations()
        
        # ğŸš€ 3. ë Œì¦ˆ ë³´ì • í•¨ìˆ˜ ì‚¬ì „ ì„¤ì • (ì¡°ê±´ë¬¸ ì œê±°) - ë” í•˜ë“œì½”ë”©
        # lambda ëŒ€ì‹  ì§ì ‘ í•¨ìˆ˜ ì •ì˜ë¡œ ì„±ëŠ¥ í–¥ìƒ
        def rectify_left_hardcoded(img):
            return cv2.remap(img, self.map_left_x, self.map_left_y,
                           cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        
        def rectify_right_hardcoded(img):
            return cv2.remap(img, self.map_right_x, self.map_right_y,
                           cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        
        self.rectify_left = rectify_left_hardcoded
        self.rectify_right = rectify_right_hardcoded
        
        # ğŸš€ 4. ë¸”ë Œë”© ë§ˆìŠ¤í¬ ìµœì¢… í¬ê¸°ë¡œ ë¯¸ë¦¬ ë¦¬ì‚¬ì´ì¦ˆ (í•­ìƒ ì „ì²´ í¬ê¸°!)
        if hasattr(self, 'use_precomputed_blending') and self.use_precomputed_blending:
            # ğŸš¨ í•­ìƒ ì „ì²´ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
            self.left_mask_final = cv2.resize(
                self.left_blend_mask, 
                self.canvas_size, 
                interpolation=cv2.INTER_LINEAR
            ).astype(np.float32)
            
            self.right_mask_final = cv2.resize(
                self.right_blend_mask, 
                self.canvas_size, 
                interpolation=cv2.INTER_LINEAR
            ).astype(np.float32)
            
            print(f"âœ… ì „ì²´ í¬ê¸° ë¸”ë Œë”© ë§ˆìŠ¤í¬ ìƒì„±: {self.left_mask_final.shape}")
            
            # ğŸ†• ê·¸ë ˆì´ìŠ¤ì¼€ì¼ìš© 1ì±„ë„ ë§ˆìŠ¤í¬ ìƒì„± (3ì±„ë„ ëŒ€ì‹ )
            self.left_mask_1ch = self.left_mask_final  # 1ì±„ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            self.right_mask_1ch = self.right_mask_final  # 1ì±„ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        else:
            # ğŸš€ ë¸”ë Œë”© ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
            self.left_mask_1ch = None
            self.right_mask_1ch = None
        
        # ğŸš€ 5. í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬ ìµœì í™” (ë” í•˜ë“œì½”ë”©)
        self.homography_matrix_opt = self.homography_matrix.astype(np.float32)  # float32ë¡œ ê³ ì •
        
        # ğŸš€ 6. ìº”ë²„ìŠ¤ í¬ê¸° í•˜ë“œì½”ë”© (í•­ìƒ ì „ì²´ í¬ê¸°!)
        # ğŸš¨ í•­ìƒ ì „ì²´ í¬ê¸° ì‚¬ìš© (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
        self.canvas_width = int(self.canvas_size[0])
        self.canvas_height = int(self.canvas_size[1])
        print(f"âœ… ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸° ì„¤ì •: {self.canvas_width}x{self.canvas_height}")
        
        # ğŸš€ 7. í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜ í¬ê¸° ë¯¸ë¦¬ ê³„ì‚° (ìºì‹œ!)
        self.homography_size = (self.canvas_width, self.canvas_height)
        print(f"âœ… í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜ í¬ê¸° ìºì‹œ: {self.homography_size}")
        
        # ğŸš€ 8. ROI ê¸°ë°˜ ì‚¬ì „ ê³„ì‚° ì¢Œí‘œ ë§µ ìƒì„± (ê·¹í•œ ìµœì í™”!)
        if hasattr(self, 'use_roi') and self.use_roi:
            self._precompute_roi_homography_coordinates()
        
        # ğŸš€ 9. ì™¼ìª½ ì˜¤í”„ì…‹ í•˜ë“œì½”ë”©
        self.left_offset_x = int(self.left_offset[0])
        self.left_offset_y = int(self.left_offset[1])
        
        # ğŸš€ 8. ì¹´ë©”ë¼ í•´ìƒë„ í•˜ë“œì½”ë”©
        self.camera_width = int(self.camera_resolution[0])
        self.camera_height = int(self.camera_resolution[1])

        # ğŸš« ë Œì¦ˆ ë³´ì • LUT ìƒì„± ì œê±° (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)
        # self._create_lens_correction_lut()
        
        # ğŸš« í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚° ì œê±° (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)
        # self._precompute_homography_coordinates()

        print("ğŸš€ ì´ˆê³ ì† ìµœì í™” ì™„ë£Œ! ì˜ˆìƒ FPS í–¥ìƒ: 50-100%")
        
        # ğŸš€ ìºì‹œ ìƒíƒœ í™•ì¸
        print(f"   ğŸ“Š ìºì‹œ ìƒíƒœ:")
        print(f"      - ìº”ë²„ìŠ¤ í…œí”Œë¦¿: {self.canvas_template.shape}")
        print(f"      - ìŠ¬ë¼ì´ìŠ¤ ì¸ë±ìŠ¤: Y{self.left_slice_y}, X{self.left_slice_x}")
        print(f"      - ë Œì¦ˆ ë³´ì • í•¨ìˆ˜: {self.rectify_left.__name__}, {self.rectify_right.__name__}")
        print(f"      - ë¸”ë Œë”© ë§ˆìŠ¤í¬: {'í™œì„±í™”' if hasattr(self, 'left_mask_1ch') and self.left_mask_1ch is not None else 'ë¹„í™œì„±í™”'}")
        print(f"      - í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬: {self.homography_matrix_opt.shape}")
        print(f"      - í•˜ë“œì½”ë”© í¬ê¸°: {self.canvas_width}x{self.canvas_height}")
        if hasattr(self, 'use_roi') and self.use_roi:
            print(f"      - ROI ëª¨ë“œ: í™œì„±í™” ({self.roi_width}x{self.roi_height})")
        else:
            print(f"      - ROI ëª¨ë“œ: ë¹„í™œì„±í™” (ì „ì²´ í™”ë©´)")
        print(f"      - ğŸš« ë Œì¦ˆ ë³´ì • LUT: ë¹„í™œì„±í™” (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)")
        print(f"      - ğŸš« í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ: ë¹„í™œì„±í™” (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)")

    def _calculate_roi_optimizations(self):
        """ğŸ†• ROI ì˜ì—­ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ í•˜ë“œì½”ë”© (í•œ ë²ˆë§Œ!)"""
        print("ğŸ”§ ROI ìµœì í™” ê³„ì‚° ì¤‘...")
        
        # ROI ì˜ì—­ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ í•˜ë“œì½”ë”© (ì „ì²´ ìŠ¤í‹°ì¹­ í›„ í¬ë¡­ìš©)
        self.roi_extract_y = slice(self.roi_y1, self.roi_y2)
        self.roi_extract_x = slice(self.roi_x1, self.roi_x2)
        
        print(f"âœ… ROI ìµœì í™” ê³„ì‚° ì™„ë£Œ:")
        print(f"   - ROI ì¶”ì¶œ ì˜ì—­: Y{self.roi_extract_y}, X{self.roi_extract_x}")
        print(f"   - í•˜ë“œì½”ë”© ì™„ë£Œ: ë§¤ í”„ë ˆì„ë§ˆë‹¤ ê³„ì‚°í•˜ì§€ ì•ŠìŒ!")

    def _create_lens_correction_lut(self):
        """ğŸš€ ë Œì¦ˆ ë³´ì • LUT ìƒì„±ìœ¼ë¡œ ë§¤ í”„ë ˆì„ ê³„ì‚° ì œê±°"""
        print("ğŸ”§ ë Œì¦ˆ ë³´ì • LUT ìƒì„± ì¤‘...")
        
        h, w = self.camera_resolution[1], self.camera_resolution[0]
        
        # ì¢Œí‘œ ê·¸ë¦¬ë“œ ìƒì„±
        y_coords, x_coords = np.mgrid[0:h, 0:w].astype(np.float32)
        
        # ë Œì¦ˆ ë³´ì • ë§µ ì ìš©í•˜ì—¬ LUT ìƒì„±
        self.left_lut_x = cv2.remap(x_coords, self.map_left_x, self.map_left_y, 
                                    cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        self.left_lut_y = cv2.remap(y_coords, self.map_left_x, self.map_left_y, 
                                    cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        
        self.right_lut_x = cv2.remap(x_coords, self.map_right_x, self.map_right_y, 
                                     cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        self.right_lut_y = cv2.remap(y_coords, self.map_right_x, self.map_right_y, 
                                     cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        
        print(f"âœ… ë Œì¦ˆ ë³´ì • LUT ìƒì„± ì™„ë£Œ: {self.left_lut_x.shape}")
        print(f"   ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 26-38ms â†’ 2-5ms (5-10ë°° í–¥ìƒ!)")

    def _precompute_homography_coordinates(self):
        """ğŸš€ í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚°ìœ¼ë¡œ ë§¤ í”„ë ˆì„ ë³€í™˜ ì œê±°"""
        print("ğŸ”§ í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚° ì¤‘...")
        
        # ğŸš¨ ì¤‘ìš”: ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°ë¡œ ì¢Œí‘œ ë§µ ìƒì„± (ì¹´ë©”ë¼ í•´ìƒë„ê°€ ì•„ë‹˜!)
        canvas_h, canvas_w = self.canvas_size[1], self.canvas_size[0]
        
        # ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€ ì¢Œí‘œ ê·¸ë¦¬ë“œ
        y_coords, x_coords = np.mgrid[0:canvas_h, 0:canvas_w].astype(np.float32)
        
        # í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬ë¡œ ë³€í™˜ëœ ì¢Œí‘œ ê³„ì‚° (í•œ ë²ˆë§Œ!)
        coords = np.stack([x_coords, y_coords, np.ones_like(x_coords)], axis=-1)
        transformed_coords = (self.homography_matrix_opt @ coords.reshape(-1, 3).T).T
        
        # 2D ì¢Œí‘œë¡œ ë³€í™˜
        transformed_coords = transformed_coords[:, :2] / transformed_coords[:, 2:]
        self.homography_coords = transformed_coords.reshape(canvas_h, canvas_w, 2)
        
        print(f"âœ… í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ìƒì„± ì™„ë£Œ: {self.homography_coords.shape}")
        print(f"   - ìº”ë²„ìŠ¤ í¬ê¸°: {canvas_w}x{canvas_h}")
        print(f"   - ì¹´ë©”ë¼ í•´ìƒë„: {self.camera_resolution[0]}x{self.camera_resolution[1]}")
        print(f"   ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 35-58ms â†’ 3-8ms (5-10ë°° í–¥ìƒ!)")

    def _precompute_roi_homography_coordinates(self):
        """ğŸš€ ROI ì˜ì—­ë§Œì„ ìœ„í•œ í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚° (ì›ë˜ ë°©ì‹!)"""
        print("ğŸ”§ ROI ê¸°ë°˜ í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚° ì¤‘...")
        
        # ğŸ¯ ì›ë˜ëŒ€ë¡œ! ROI í¬ê¸°ë¡œë§Œ ì²˜ë¦¬!
        roi_h, roi_w = self.roi_height, self.roi_width  # 618, 3081
        
        # ROI í¬ê¸°ë¡œ ì¢Œí‘œ ê·¸ë¦¬ë“œ ìƒì„±
        y_coords, x_coords = np.mgrid[0:roi_h, 0:roi_w].astype(np.float32)
        
        # ROI ì¢Œí‘œë¥¼ ì „ì²´ ìº”ë²„ìŠ¤ ì¢Œí‘œë¡œ ë³€í™˜
        x_coords += self.roi_x1  # ROI ì‹œì‘ì  ë”í•˜ê¸°
        y_coords += self.roi_y1
        
        # í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬ë¡œ ë³€í™˜ëœ ì¢Œí‘œ ê³„ì‚° (í•œ ë²ˆë§Œ!)
        coords = np.stack([x_coords, y_coords, np.ones_like(x_coords)], axis=-1)
        transformed_coords = (self.homography_matrix_opt @ coords.reshape(-1, 3).T).T
        
        # 2D ì¢Œí‘œë¡œ ë³€í™˜
        transformed_coords = transformed_coords[:, :2] / transformed_coords[:, 2:]
        self.roi_homography_coords = transformed_coords.reshape(roi_h, roi_w, 2)
        
        # ğŸš€ ROI ì˜ì—­ë§Œì„ ìœ„í•œ ë§ˆìŠ¤í¬ë„ ìƒì„± (ì°¨ì› ì¼ì¹˜ ë³´ì¥!)
        self.roi_mask = np.ones((roi_h, roi_w), dtype=np.uint8)
        
        print(f"âœ… ROI í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ìƒì„± ì™„ë£Œ:")
        print(f"   - ROI í¬ê¸°: {roi_w}x{roi_h}")
        print(f"   - ì¢Œí‘œ ë§µ í¬ê¸°: {self.roi_homography_coords.shape}")
        print(f"   - ROI ë§ˆìŠ¤í¬ í¬ê¸°: {self.roi_mask.shape}")
        print(f"   - ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 36ms â†’ 5-10ms (3-7ë°° í–¥ìƒ!)")

    def _apply_roi_homography_fast(self, frame):
        """ğŸš€ ì‚¬ì „ ê³„ì‚°ëœ ROI ì¢Œí‘œ ë§µì„ ì‚¬ìš©í•˜ì—¬ ê·¹í•œ ë¹ ë¥¸ í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜"""
        if not hasattr(self, 'roi_homography_coords'):
            # ì¢Œí‘œ ë§µì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            return cv2.warpPerspective(frame, self.homography_matrix_opt, 
                                     self.homography_size, flags=cv2.INTER_LINEAR)
        
        # ğŸš€ ROI í¬ê¸°ë¡œ ê²°ê³¼ ì´ë¯¸ì§€ ì´ˆê¸°í™” (ì°¨ì› ì¼ì¹˜ ë³´ì¥!)
        roi_h, roi_w = self.roi_height, self.roi_width
        warped = np.zeros((roi_h, roi_w, 3), dtype=frame.dtype)
        
        # ì‚¬ì „ ê³„ì‚°ëœ ì¢Œí‘œ ë§µ ì‚¬ìš©
        coords = self.roi_homography_coords
        
        # ì¢Œí‘œë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ìƒ˜í”Œë§
        coords_int = coords.astype(np.int32)
        coords_int[:, :, 0] = np.clip(coords_int[:, :, 0], 0, frame.shape[1]-1)
        coords_int[:, :, 1] = np.clip(coords_int[:, :, 1], 0, frame.shape[0]-1)
        
        # ë²¡í„°í™”ëœ ìƒ˜í”Œë§ (í›¨ì”¬ ë¹ ë¦„!)
        warped = frame[coords_int[:, :, 1], coords_int[:, :, 0]]
        
        return warped

    def _initialize_gaze_tracking(self):
        """ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™”"""
        print("ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™” ì¤‘...")
        
        # GPU ì„¤ì • (YOLO ì‹œì„ ì¶”ì ìš©ë§Œ)
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            print(f"ğŸš€ YOLO GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
        else:
            self.device = torch.device('cpu')
            print("âš ï¸ YOLO CPU ì‚¬ìš©")
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        try:
            self.yolo_model = YOLO("models/yolov8n-face.pt")
            self.yolo_model.to(self.device)
            print("âœ… YOLO ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.yolo_model = None
        
        # MediaPipe FaceMesh ì„¤ì •
        try:
            self.mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                static_image_mode=False, 
                max_num_faces=GAZE_TRACKING_CONFIG['max_faces'],
                refine_landmarks=True,
                min_detection_confidence=GAZE_TRACKING_CONFIG['detection_confidence'],
                min_tracking_confidence=GAZE_TRACKING_CONFIG['tracking_confidence']
            )
            print("âœ… MediaPipe FaceMesh ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.face_mesh = None
        
        # ì‹œì„  ì•ˆì •í™” ê°ì²´ë“¤
        self.gaze_stabilizers = [GazeStabilizer() for _ in range(GAZE_TRACKING_CONFIG['max_faces'])]
        
        # 3D ì–¼êµ´ ëª¨ë¸ í¬ì¸íŠ¸
        self.face_3d_model_points = np.array([
            [0.0, 0.0, 0.0], [0.0, -330.0, -65.0],
            [-225.0, 170.0, -135.0], [225.0, 170.0, -135.0],
            [-150.0, -150.0, -125.0], [150.0, -150.0, -125.0]
        ], dtype=np.float64)
        
        # ëœë“œë§ˆí¬ IDë“¤
        self.landmark_ids = [1, 152, 33, 263, 61, 291]
        self.left_eye_landmarks = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]
        self.right_eye_landmarks = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
        self.iris_ids = [468, 473]  # ì™¼ìª½, ì˜¤ë¥¸ìª½ í™ì±„ ì¤‘ì‹¬
        
        print("âœ… ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_threaded_gaze_tracking(self):
        """ğŸš€ ë©€í‹°ìŠ¤ë ˆë”© ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™”"""
        print("ğŸš€ ë©€í‹°ìŠ¤ë ˆë”© ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™” ì¤‘...")
        
        # ìŠ¤ë ˆë“œ ì•ˆì „í•œ íë“¤
        self.gaze_input_queue = Queue(maxsize=2)  # ì…ë ¥ í (ìµœì‹  í”„ë ˆì„ë§Œ)
        self.gaze_output_queue = Queue(maxsize=2)  # ì¶œë ¥ í
        
        # ì•„ì´íŠ¸ë˜í‚¹ ìŠ¤ë ˆë“œ ì‹œì‘
        self.gaze_thread_running = True
        self.gaze_thread = threading.Thread(target=self._gaze_worker_thread, daemon=True)
        self.gaze_thread.start()
        
        print("âœ… ë©€í‹°ìŠ¤ë ˆë”© ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_gaze_experiment(self):
        """ğŸš€ ë™ê³µ ì¶”ì  ì‹¤í—˜ ì´ˆê¸°í™”"""
        print("ğŸš€ ë™ê³µ ì¶”ì  ì‹¤í—˜ ì´ˆê¸°í™” ì¤‘...")
        
        # ì‹¤í—˜ ê¸°ë¡ê¸° ì´ˆê¸°í™”
        self.gaze_experiment = GazeExperimentRecorder()
        
        # ë¶€ëª¨ ì°¸ì¡° ì„¤ì • (ìë™ í†µê³„ ì €ì¥ìš©)
        self.gaze_experiment._parent_stitcher = self
        
        # ì‹¤í—˜ ìƒíƒœ ë³€ìˆ˜ë“¤
        self.experiment_mode = False
        self.target_a_stats = None
        self.target_b_stats = None
        
        # ì‹¤í—˜ UI ìƒíƒœ
        self.show_experiment_info = True
        self.experiment_instruction = "Press 'e' to start experiment mode"
        
        print("âœ… ë™ê³µ ì¶”ì  ì‹¤í—˜ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ëª©í‘œ í”½ì…€ ê±°ë¦¬: {GAZE_EXPERIMENT_CONFIG['target_distance_pixels']:.1f} í”½ì…€")
        print(f"   - ê¸°ë¡ ì‹œê°„: {GAZE_EXPERIMENT_CONFIG['recording_duration']}ì´ˆ")
        print(f"   - ì˜ˆìƒ í”„ë ˆì„ ìˆ˜: {GAZE_EXPERIMENT_CONFIG['expected_frames']}")
    
    def _save_experiment_stats_auto(self, target):
        """ğŸš€ ìë™ ê¸°ë¡ ì™„ë£Œ ì‹œ í†µê³„ ì €ì¥ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        print(f"ğŸ”§ ìë™ í†µê³„ ì €ì¥ ì‹œì‘: íƒ€ê²Ÿ {target}")
        
        # í†µê³„ ê³„ì‚° ë° ì €ì¥
        stats = self.gaze_experiment.get_statistics()
        if stats:
            if target == 'A':
                self.target_a_stats = stats
                print(f"âœ… íƒ€ê²Ÿ A í†µê³„ ìë™ ì €ì¥ ì™„ë£Œ!")
                print(f"   - í‰ê·  ìœ„ì¹˜: ({stats['mean_x']:.1f}, {stats['mean_y']:.1f})")
                print(f"   - ì•ˆì •ì„± ì ìˆ˜: {stats['stability_score']:.2f} í”½ì…€")
                print(f"   - í”„ë ˆì„ ìˆ˜: {stats['frame_count']}")
            else:
                self.target_b_stats = stats
                print(f"âœ… íƒ€ê²Ÿ B í†µê³„ ìë™ ì €ì¥ ì™„ë£Œ!")
                print(f"   - í‰ê·  ìœ„ì¹˜: ({stats['mean_x']:.1f}, {stats['mean_y']:.1f})")
                print(f"   - ì•ˆì •ì„± ì ìˆ˜: {stats['stability_score']:.2f} í”½ì…€")
                print(f"   - í”„ë ˆì„ ìˆ˜: {stats['frame_count']}")
            
            # í†µê³„ ì¶œë ¥
            self.gaze_experiment.print_statistics()
            
            # ì‹¤í—˜ ì§€ì‹œì‚¬í•­ ì—…ë°ì´íŠ¸
            self.experiment_instruction = "Experiment Mode: Press 'a' or 'b' to start recording"
        else:
            print(f"âŒ ìë™ í†µê³„ ì €ì¥ ì‹¤íŒ¨: íƒ€ê²Ÿ {target}")
            print(f"   - gaze_coordinates ê°œìˆ˜: {len(self.gaze_experiment.gaze_coordinates)}")
            print(f"   - timestamps ê°œìˆ˜: {len(self.gaze_experiment.timestamps)}")

    def _gaze_worker_thread(self):
        """ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì „ìš© ì›Œì»¤ ìŠ¤ë ˆë“œ"""
        while self.gaze_thread_running:
            try:
                # ì…ë ¥ íì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¸”ë¡œí‚¹ ë°©ì§€)
                frame = self.gaze_input_queue.get(timeout=0.1)
                
                # ì•„ì´íŠ¸ë˜í‚¹ ì²˜ë¦¬
                processed_frame, gaze_data = self.process_gaze_tracking(frame)
                
                # ì¶œë ¥ íì— ê²°ê³¼ ì „ë‹¬ (ì´ì „ ê²°ê³¼ ë®ì–´ì“°ê¸°)
                try:
                    if not self.gaze_output_queue.empty():
                        self.gaze_output_queue.get_nowait()  # ì´ì „ ê²°ê³¼ ì œê±°
                    self.gaze_output_queue.put_nowait((processed_frame, gaze_data))
                except:
                    pass
                    
            except:
                continue  # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ê³„ì† ì§„í–‰

    def process_gaze_tracking_async(self, frame):
        """ğŸš€ ë¹„ë™ê¸° ì•„ì´íŠ¸ë˜í‚¹ ìš”ì²­"""
        try:
            # ì…ë ¥ íì— í”„ë ˆì„ ì „ë‹¬ (ì´ì „ í”„ë ˆì„ ë®ì–´ì“°ê¸°)
            if not self.gaze_input_queue.empty():
                self.gaze_input_queue.get_nowait()  # ì´ì „ í”„ë ˆì„ ì œê±°
            self.gaze_input_queue.put_nowait(frame)
        except:
            pass

    def get_gaze_result_async(self):
        """ğŸš€ ë¹„ë™ê¸° ì•„ì´íŠ¸ë˜í‚¹ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.gaze_output_queue.get_nowait()
        except:
            return None, []

    def switch_to_mirror_mode(self, enable_mirror: bool):
        """ê±°ìš¸ëª¨ë“œ ì „í™˜ í•¨ìˆ˜"""
        if enable_mirror:
            # ê±°ìš¸ëª¨ë“œ: ì´ë¯¸ì§€ë¥¼ ì¢Œìš° ë°˜ì „
            print(f"ğŸ”„ ê±°ìš¸ëª¨ë“œ í™œì„±í™”")
        else:
            # ì¼ë°˜ëª¨ë“œ: ì›ë³¸ ì´ë¯¸ì§€
            print(f"ğŸ”„ ì¼ë°˜ëª¨ë“œ í™œì„±í™”")

    def toggle_experiment_mode(self):
        """ì‹¤í—˜ ëª¨ë“œ í† ê¸€"""
        self.experiment_mode = not self.experiment_mode
        
        if self.experiment_mode:
            print(f"\nğŸ”¬ ì‹¤í—˜ ëª¨ë“œ í™œì„±í™”")
            print(f"   - 'a': íƒ€ê²Ÿ A ì‘ì‹œ ê¸°ë¡ ì‹œì‘")
            print(f"   - 'b': íƒ€ê²Ÿ B ì‘ì‹œ ê¸°ë¡ ì‹œì‘")
            print(f"   - 's': í˜„ì¬ í†µê³„ ì¶œë ¥")
            print(f"   - 'c': íƒ€ê²Ÿ A vs B ë¹„êµ")
            print(f"   - 'f': ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥")
            print(f"   - 'r': ì‹¤í—˜ ë°ì´í„° ì´ˆê¸°í™”")
            print(f"   - 'x': ìˆ˜ë™ ê¸°ë¡ ì¤‘ì§€")
            print(f"   - 'e': ì‹¤í—˜ ëª¨ë“œ ë¹„í™œì„±í™”")
            self.experiment_instruction = "Experiment Mode: Press 'a' or 'b' to start recording"
            print(f"âœ… ì‹¤í—˜ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. 'a' ë˜ëŠ” 'b' í‚¤ë¥¼ ëˆŒëŸ¬ ê¸°ë¡ì„ ì‹œì‘í•˜ì„¸ìš”.")
        else:
            print(f"\nğŸ”¬ ì‹¤í—˜ ëª¨ë“œ ë¹„í™œì„±í™”")
            self.experiment_instruction = "Press 'e' to start experiment mode"
            print(f"âœ… ì‹¤í—˜ ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def start_target_recording(self, target):
        """íƒ€ê²Ÿ ì‘ì‹œ ê¸°ë¡ ì‹œì‘"""
        if not self.experiment_mode:
            print(f"âš ï¸ ì‹¤í—˜ ëª¨ë“œë¥¼ ë¨¼ì € í™œì„±í™”í•´ì£¼ì„¸ìš” ('e' í‚¤)")
            return False
        
        if self.gaze_experiment.is_recording:
            print(f"âš ï¸ ì´ë¯¸ ê¸°ë¡ ì¤‘ì…ë‹ˆë‹¤. ë¨¼ì € ì¤‘ì§€í•´ì£¼ì„¸ìš”.")
            return False
        
        success = self.gaze_experiment.start_recording(target)
        if success:
            if target == 'A':
                self.experiment_instruction = f"Recording Target A... ({GAZE_EXPERIMENT_CONFIG['recording_duration']}s)"
                print(f"\nğŸ¯ íƒ€ê²Ÿ A ì‘ì‹œ ì‹œì‘!")
                print(f"   - 10ì´ˆê°„ ì‘ì‹œí•´ì£¼ì„¸ìš”")
                print(f"   - íƒ€ì´ë¨¸ ë°”ê°€ í™”ë©´ ìƒë‹¨ì— í‘œì‹œë©ë‹ˆë‹¤")
                print(f"   - ì‹œì„  ìœ„ì¹˜ê°€ ë¹¨ê°„ìƒ‰ ì ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤")
                print(f"   - ìë™ìœ¼ë¡œ 10ì´ˆ í›„ ê¸°ë¡ì´ ì™„ë£Œë©ë‹ˆë‹¤")
                print(f"   - 10ì´ˆë¥¼ ì´ˆê³¼í•´ë„ ëª¨ë“  ë°ì´í„°ê°€ ê¸°ë¡ë©ë‹ˆë‹¤")
            else:
                self.experiment_instruction = f"Recording Target B... ({GAZE_EXPERIMENT_CONFIG['recording_duration']}s)"
                print(f"\nğŸ¯ íƒ€ê²Ÿ B ì‘ì‹œ ì‹œì‘!")
                print(f"   - 10ì´ˆê°„ ì‘ì‹œí•´ì£¼ì„¸ìš” (íƒ€ê²Ÿ Aì—ì„œ 1.3cm ì˜† ì§€ì )")
                print(f"   - íƒ€ì´ë¨¸ ë°”ê°€ í™”ë©´ ìƒë‹¨ì— í‘œì‹œë©ë‹ˆë‹¤")
                print(f"   - ì‹œì„  ìœ„ì¹˜ê°€ ë¹¨ê°„ìƒ‰ ì ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤")
                print(f"   - ìë™ìœ¼ë¡œ 10ì´ˆ í›„ ê¸°ë¡ì´ ì™„ë£Œë©ë‹ˆë‹¤")
                print(f"   - 10ì´ˆë¥¼ ì´ˆê³¼í•´ë„ ëª¨ë“  ë°ì´í„°ê°€ ê¸°ë¡ë©ë‹ˆë‹¤")
        
        return success
    
    def stop_target_recording(self):
        """íƒ€ê²Ÿ ì‘ì‹œ ê¸°ë¡ ì¤‘ì§€"""
        if not self.gaze_experiment.is_recording:
            print(f"âš ï¸ ê¸°ë¡ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return False
        
        success = self.gaze_experiment.stop_recording()
        if success:
            # í†µê³„ ê³„ì‚° ë° ì €ì¥
            stats = self.gaze_experiment.get_statistics()
            if stats:
                if stats['target'] == 'A':
                    self.target_a_stats = stats
                    print(f"âœ… íƒ€ê²Ÿ A í†µê³„ ì €ì¥ ì™„ë£Œ")
                    print(f"   - í‰ê·  ìœ„ì¹˜: ({stats['mean_x']:.1f}, {stats['mean_y']:.1f})")
                    print(f"   - ì•ˆì •ì„± ì ìˆ˜: {stats['stability_score']:.2f} í”½ì…€")
                else:
                    self.target_b_stats = stats
                    print(f"âœ… íƒ€ê²Ÿ B í†µê³„ ì €ì¥ ì™„ë£Œ")
                    print(f"   - í‰ê·  ìœ„ì¹˜: ({stats['mean_x']:.1f}, {stats['mean_y']:.1f})")
                    print(f"   - ì•ˆì •ì„± ì ìˆ˜: {stats['stability_score']:.2f} í”½ì…€")
                
                # í†µê³„ ì¶œë ¥
                self.gaze_experiment.print_statistics()
            
            self.experiment_instruction = "Experiment Mode: Press 'a' or 'b' to start recording"
        else:
            print(f"âŒ ê¸°ë¡ ì¤‘ì§€ ì‹¤íŒ¨. ê¸°ë¡ ì‹œê°„ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ê¹ë‹ˆë‹¤.")
        
        return success
    
    def get_current_gaze_coordinates(self):
        """í˜„ì¬ ì‹œì„  ì¢Œí‘œ ë°˜í™˜ (ì‹¤í—˜ ëª¨ë“œì—ì„œë§Œ)"""
        if not self.experiment_mode or not hasattr(self, 'gaze_experiment'):
            return None, None
        
        if self.gaze_experiment.is_recording and self.gaze_experiment.gaze_coordinates:
            # ê°€ì¥ ìµœê·¼ ì¢Œí‘œ ë°˜í™˜
            latest_coord = self.gaze_experiment.gaze_coordinates[-1]
            return latest_coord[0], latest_coord[1]
        
        return None, None
    
    def show_experiment_statistics(self):
        """ì‹¤í—˜ í†µê³„ ì¶œë ¥"""
        if not self.experiment_mode:
            print(f"âš ï¸ ì‹¤í—˜ ëª¨ë“œë¥¼ ë¨¼ì € í™œì„±í™”í•´ì£¼ì„¸ìš” ('e' í‚¤)")
            return
        
        print(f"\nğŸ”¬ í˜„ì¬ ì‹¤í—˜ ìƒíƒœ:")
        print(f"{'='*40}")
        
        if self.target_a_stats:
            print(f"âœ… íƒ€ê²Ÿ A: ì™„ë£Œ")
            print(f"   - í‰ê·  ìœ„ì¹˜: ({self.target_a_stats['mean_x']:.2f}, {self.target_a_stats['mean_y']:.2f})")
            print(f"   - ì•ˆì •ì„± ì ìˆ˜: {self.target_a_stats['stability_score']:.2f} í”½ì…€")
        else:
            print(f"âŒ íƒ€ê²Ÿ A: ë¯¸ì™„ë£Œ")
        
        if self.target_b_stats:
            print(f"âœ… íƒ€ê²Ÿ B: ì™„ë£Œ")
            print(f"   - í‰ê·  ìœ„ì¹˜: ({self.target_b_stats['mean_x']:.2f}, {self.target_b_stats['mean_y']:.2f})")
            print(f"   - ì•ˆì •ì„± ì ìˆ˜: {self.target_b_stats['stability_score']:.2f} í”½ì…€")
        else:
            print(f"âŒ íƒ€ê²Ÿ B: ë¯¸ì™„ë£Œ")
        
        if self.target_a_stats and self.target_b_stats:
            print(f"\nğŸ¯ ë‘ íƒ€ê²Ÿ ëª¨ë‘ ì™„ë£Œ! 'c' í‚¤ë¡œ ë¹„êµ ë¶„ì„ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        print(f"{'='*40}")
    
    def compare_experiment_targets(self):
        """íƒ€ê²Ÿ Aì™€ B ë¹„êµ ë¶„ì„"""
        if not self.experiment_mode:
            print(f"âš ï¸ ì‹¤í—˜ ëª¨ë“œë¥¼ ë¨¼ì € í™œì„±í™”í•´ì£¼ì„¸ìš” ('e' í‚¤)")
            return
        
        if not self.target_a_stats or not self.target_b_stats:
            print(f"âš ï¸ ë‘ íƒ€ê²Ÿì˜ ë°ì´í„°ê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")
            print(f"   - íƒ€ê²Ÿ A: {'ì™„ë£Œ' if self.target_a_stats else 'ë¯¸ì™„ë£Œ'}")
            print(f"   - íƒ€ê²Ÿ B: {'ì™„ë£Œ' if self.target_b_stats else 'ë¯¸ì™„ë£Œ'}")
            return
        
        # ë¹„êµ ë¶„ì„ ì‹¤í–‰
        self.gaze_experiment.compare_targets(self.target_a_stats, self.target_b_stats)
    
    def save_experiment_results(self):
        """ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not self.experiment_mode:
            print(f"âš ï¸ ì‹¤í—˜ ëª¨ë“œë¥¼ ë¨¼ì € í™œì„±í™”í•´ì£¼ì„¸ìš” ('e' í‚¤)")
            return
        
        if not self.target_a_stats and not self.target_b_stats:
            print(f"âš ï¸ ì €ì¥í•  ì‹¤í—˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        filename = self.gaze_experiment.save_results_to_file()
        if filename:
            print(f"âœ… ì‹¤í—˜ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def reset_experiment_data(self):
        """ì‹¤í—˜ ë°ì´í„° ì´ˆê¸°í™”"""
        if not self.experiment_mode:
            print(f"âš ï¸ ì‹¤í—˜ ëª¨ë“œë¥¼ ë¨¼ì € í™œì„±í™”í•´ì£¼ì„¸ìš” ('e' í‚¤)")
            return
        
        self.gaze_experiment.reset()
        self.target_a_stats = None
        self.target_b_stats = None
        self.experiment_instruction = "Experiment Mode: Press 'a' or 'b' to start recording"
        print(f"âœ… ì‹¤í—˜ ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def setup_cameras(self, left_id: int = LEFT_CAMERA_INDEX, right_id: int = RIGHT_CAMERA_INDEX) -> bool:
        """ì¹´ë©”ë¼ ì´ˆê¸°í™” ë° ì„¤ì •"""
        print(f"ì¹´ë©”ë¼ ì´ˆê¸°í™”: ì™¼ìª½={left_id}, ì˜¤ë¥¸ìª½={right_id}")
        
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ìˆœ)
            backends = [
                (cv2.CAP_DSHOW, "DirectShow"),
                (cv2.CAP_MSMF, "Media Foundation"),
                (cv2.CAP_ANY, "Auto")
            ]
            
            # ê° ë°±ì—”ë“œë¡œ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹œë„
            for backend_id, backend_name in backends:
                try:
                    print(f"ë°±ì—”ë“œ {backend_name} ({backend_id}) ì‹œë„ ì¤‘...")
                    self.cap_left = cv2.VideoCapture(left_id, backend_id)
                    self.cap_right = cv2.VideoCapture(right_id, backend_id)
                    
                    if self.cap_left.isOpened() and self.cap_right.isOpened():
                        print(f"ë°±ì—”ë“œ {backend_name} ({backend_id}) ì‚¬ìš©")
                        break
                    else:
                        print(f"ë°±ì—”ë“œ {backend_name} ({backend_id}) ì‹¤íŒ¨")
                        self.cap_left.release()
                        self.cap_right.release()
                except Exception as e:
                    print(f"ë°±ì—”ë“œ {backend_name} ({backend_id}) ì˜¤ë¥˜: {e}")
                    continue
            else:
                print("ëª¨ë“  ë°±ì—”ë“œì—ì„œ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            # ê³ ì„±ëŠ¥ ì¹´ë©”ë¼ ì„¤ì • (í•˜ë“œì½”ë”©)
            target_width = self.camera_width
            target_height = self.camera_height
            
            for i, cap in enumerate([self.cap_left, self.cap_right]):
                camera_name = "ì™¼ìª½" if i == 0 else "ì˜¤ë¥¸ìª½"
                
                # í•´ìƒë„ ì„¤ì • (í•˜ë“œì½”ë”©)
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, target_width)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, target_height)
                
                # ğŸš€ FPS ìµœì í™”: 60fpsë¡œ ì„¤ì • (í•˜ë“œì½”ë”©)
                cap.set(cv2.CAP_PROP_FPS, 60)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ë²„í¼ ìµœì†Œí™”
                cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
                
                actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                actual_fps = cap.get(cv2.CAP_PROP_FPS)
                
                print(f"   {camera_name} ì¹´ë©”ë¼: {actual_width}x{actual_height} @ {actual_fps:.1f}fps")
                
                # í•´ìƒë„ ë¶ˆì¼ì¹˜ ê²½ê³  (í•˜ë“œì½”ë”©ëœ ì„ê³„ê°’)
                if abs(actual_width - target_width) > 100 or abs(actual_height - target_height) > 100:
                    print(f"âš ï¸ {camera_name} ì¹´ë©”ë¼ í•´ìƒë„ ë¶ˆì¼ì¹˜: ëª©í‘œ {target_width}x{target_height}, ì‹¤ì œ {actual_width}x{actual_height}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì¹´ë©”ë¼ ì„¤ì • ì˜¤ë¥˜: {e}")
            return False

    def _blend_images_ultra_fast(self, left_translated: np.ndarray, right_warped: np.ndarray) -> np.ndarray:
        """ğŸš€ ê·¹í•œ ì†ë„ ë¸”ë Œë”© (ì§„ì§œ ë¹ ë¥¸ ì•Œê³ ë¦¬ì¦˜!)"""
        
        # ğŸš€ ë°©ë²• 1: ì œìë¦¬ ì—°ì‚° (ê°€ì¥ ë¹ ë¦„ - ë©”ëª¨ë¦¬ ë³µì‚¬ ì—†ìŒ!)
        # left_translatedì— ì§ì ‘ ë®ì–´ì“°ê¸° (ì›ë³¸ ìˆ˜ì •ë¨ ì£¼ì˜!)
        mask = right_warped > 10
        left_translated[mask] = right_warped[mask]
        return left_translated
        
        # ğŸš« ê¸°ì¡´ ë°©ë²•ë“¤ (ëŠë¦¼)
        # result = left_translated.copy()  # 14.6MB ë³µì‚¬!
        # mask = np.any(right_warped > 10, axis=2)  # 4.9MB ë§ˆìŠ¤í¬!
        # result[mask] = right_warped[mask]
        # return result

    def update_fps(self):
        """ğŸš€ í•˜ë“œì½”ë”©ëœ FPS ì—…ë°ì´íŠ¸ (ìµœëŒ€ ì„±ëŠ¥)"""
        self.fps_counter += 1
        current_time = time.time()
        
        # 1ì´ˆë§ˆë‹¤ FPS ê³„ì‚° (í•˜ë“œì½”ë”©ëœ ì‹œê°„)
        if current_time - self.fps_start_time >= 1.0:
            self.current_fps = self.fps_counter / (current_time - self.fps_start_time)
            self.fps_counter = 0
            self.fps_start_time = current_time
            
            # FPS ë¡œê·¸ ì¶œë ¥ ì œí•œ (10ì´ˆë§ˆë‹¤ë§Œ ì¶œë ¥, ì„±ëŠ¥ ì¸¡ì •ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
            if self.performance_measurement:
                if hasattr(self, '_last_fps_log_time'):
                    if current_time - self._last_fps_log_time >= 10.0:
                        print(f"ğŸ“Š í˜„ì¬ FPS: {self.current_fps:.1f}")
                        self._last_fps_log_time = current_time
                else:
                    self._last_fps_log_time = current_time
                    print(f"ğŸ“Š í˜„ì¬ FPS: {self.current_fps:.1f}")
                    
                # ğŸš€ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ ì¶œë ¥ ì œí•œ (30ì´ˆë§ˆë‹¤ë§Œ ì¶œë ¥)
                if self.frame_count > 0:
                    if hasattr(self, '_last_performance_log_time'):
                        if current_time - self._last_performance_log_time >= 30.0:
                            print(f"ğŸ” ì„±ëŠ¥ ì¸¡ì • í˜¸ì¶œ: frame_count={self.frame_count}, step_times={len(self.step_times['total_stitching'])}")
                            self._print_performance_stats()
                            self._last_performance_log_time = current_time
                    else:
                        self._last_performance_log_time = current_time
                        print(f"ğŸ” ì„±ëŠ¥ ì¸¡ì • í˜¸ì¶œ: frame_count={self.frame_count}, step_times={len(self.step_times['total_stitching'])}")
                        self._print_performance_stats()

    def _print_performance_stats(self):
        """ğŸš€ ê° ë‹¨ê³„ë³„ ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ” ì„±ëŠ¥ ë¶„ì„ (í”„ë ˆì„ {self.frame_count}):")
        print(f"{'='*50}")
        
        for step_name, times in self.step_times.items():
            if times:
                avg_time = np.mean(times) * 1000  # msë¡œ ë³€í™˜
                min_time = np.min(times) * 1000
                max_time = np.max(times) * 1000
                print(f"   {step_name:15s}: í‰ê·  {avg_time:6.2f}ms (ìµœì†Œ {min_time:6.2f}ms, ìµœëŒ€ {max_time:6.2f}ms)")
        
        # ì´ ìŠ¤í‹°ì¹­ ì‹œê°„ ë¶„ì„
        if self.step_times['total_stitching']:
            total_avg = np.mean(self.step_times['total_stitching']) * 1000
            theoretical_fps = 1000 / total_avg if total_avg > 0 else 0
            print(f"{'='*50}")
            print(f"   ğŸ¯ ì´ ìŠ¤í‹°ì¹­ ì‹œê°„: {total_avg:.2f}ms")
            print(f"   ğŸš€ ì´ë¡ ì  ìµœëŒ€ FPS: {theoretical_fps:.1f}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‘œì‹œ
        print(f"   ğŸ’¾ ìº”ë²„ìŠ¤ í¬ê¸°: {self.canvas_width}x{self.canvas_height}")
        if hasattr(self, 'use_roi') and self.use_roi:
            print(f"   ğŸ¯ ROI í¬ê¸°: {self.roi_width}x{self.roi_height}")
        
        print(f"{'='*50}")
        
        # ì¸¡ì • ë°ì´í„° ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        for step_name in self.step_times:
            self.step_times[step_name] = []

    def show_stitched_video_grayscale(self):
        """ğŸš€ ë‹¨ìƒ‰(ê·¸ë ˆì´ìŠ¤ì¼€ì¼) ì´ˆê³ ì† ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ ë¹„ë””ì˜¤ í‘œì‹œ"""
        print(f"\nğŸš€ ë‹¨ìƒ‰(ê·¸ë ˆì´ìŠ¤ì¼€ì¼) ì´ˆê³ ì† ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ ì‹œì‘ (ì¤‘ê°„ ì„±ëŠ¥ ë²„ì „)")
        print(f"ğŸ“¸ ì¹´ë©”ë¼ í•´ìƒë„: {self.camera_resolution[0]}x{self.camera_resolution[1]}")
        print(f"ğŸ¯ ëª©í‘œ FPS: 40+ (ë‹¨ìƒ‰ ì²˜ë¦¬ë¡œ ì¸í•œ ì„±ëŠ¥ í–¥ìƒ)")
        print(f"ğŸ”„ ê±°ìš¸ëª¨ë“œ: {'í™œì„±í™”' if True else 'ë¹„í™œì„±í™”'}") # ê±°ìš¸ëª¨ë“œ ê¸°ë³¸ í™œì„±í™”
        print(f"ğŸ“± ë””ìŠ¤í”Œë ˆì´ ìŠ¤ì¼€ì¼: {DISPLAY_SCALE:.1f}x ({int(DISPLAY_SCALE*100)}%)")
        print(f"ğŸ“ ìµœì¢… ìº”ë²„ìŠ¤ í¬ê¸°: {self.canvas_size[0]}x{self.canvas_size[1]}")
        print(f"ğŸ“ ì™¼ìª½ ì´ë¯¸ì§€ ì˜¤í”„ì…‹: ({self.left_offset[0]}, {self.left_offset[1]})")
        print(f"ğŸ¨ ìƒ‰ìƒ ëª¨ë“œ: ê·¸ë ˆì´ìŠ¤ì¼€ì¼ (1ì±„ë„) - ì…ë ¥ë¶€í„° 1ì±„ë„ ì²˜ë¦¬ë¡œ ê·¹í•œ ë©”ëª¨ë¦¬ ì ˆì•½")
        print(f"   - ì…ë ¥ ë³€í™˜: BGR â†’ Gray (ë©”ëª¨ë¦¬ 1/3)")
        print(f"   - ìŠ¤í‹°ì¹­ ì²˜ë¦¬: 1ì±„ë„ë¡œ ì²˜ë¦¬ (ì²˜ë¦¬ ì†ë„ 2-3ë°° í–¥ìƒ)")
        print(f"   - ì•„ì´íŠ¸ë˜í‚¹: 1ì±„ë„ â†’ 3ì±„ë„ ë³µì œ (YOLO í˜¸í™˜ì„±)")
        
        # ğŸ†• ROI ì •ë³´ í‘œì‹œ
        if hasattr(self, 'use_roi') and self.use_roi:
            print(f"ğŸ¯ ROI ëª¨ë“œ: í™œì„±í™”")
            print(f"   ROI ì¢Œí‘œ: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
            print(f"   ROI í¬ê¸°: {self.roi_width} x {self.roi_height}")
            print(f"   ì˜ˆìƒ FPS í–¥ìƒ: ë‹¨ìƒ‰ ì²˜ë¦¬ + ROIë¡œ ì¸í•œ ì¶”ê°€ 100-150% í–¥ìƒ")
        else:
            print(f"ğŸ¯ ROI ëª¨ë“œ: ë¹„í™œì„±í™” (ì „ì²´ í™”ë©´)")
        
        print(f"ğŸš€ ì ìš©ëœ ìµœì í™”: ë‹¨ìƒ‰ ì²˜ë¦¬ + JSON ë‹¨ì¼ë¡œë”© + ìº”ë²„ìŠ¤ìºì‹œ + ì¡°ê±´ë¬¸ì œê±°")
        print(f"ğŸ‘ï¸ ì•„ì´íŠ¸ë˜í‚¹: YOLO + MediaPipe FaceMesh + ì‹œì„  ì•ˆì •í™” (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì—ì„œë„ ì •ìƒ ì‘ë™)")
        print(f"ğŸš€ ë©€í‹°ìŠ¤ë ˆë”©: ì•„ì´íŠ¸ë˜í‚¹ ë³„ë„ ìŠ¤ë ˆë“œë¡œ ë¶„ë¦¬í•˜ì—¬ ë©”ì¸ ë£¨í”„ FPS í–¥ìƒ")
        print(f"{'='*70}")
        
        # ğŸš€ ì„±ëŠ¥ ìµœì í™”: ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰!
        print("ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì‹œì‘...")
        
        print("âœ… ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ!")
        
        print(f"ğŸ¬ ë‹¨ìƒ‰ ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ ì‹œì‘...")
        print(f"ğŸ’¡ ì¢…ë£Œ: 'q' í‚¤")
        print(f"ğŸ’¡ ê±°ìš¸ëª¨ë“œ í† ê¸€: 'm' í‚¤")
        print(f"ğŸ¨ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ëª¨ë“œë¡œ ì•„ì´íŠ¸ë˜í‚¹ê³¼ í•¨ê»˜ ì‘ë™í•©ë‹ˆë‹¤")
        print(f"{'='*60}")
        
        # ì‹œì‘ ì‹œ ê±°ìš¸ëª¨ë“œ í™œì„±í™”
        print("ğŸ”„ ê±°ìš¸ëª¨ë“œê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        frame_count = 0
        mirror_mode = True  # ê±°ìš¸ëª¨ë“œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        
        try:
            while True:
                # ğŸš€ ì „ì²´ ìŠ¤í‹°ì¹­ ì‹œê°„ ì¸¡ì • ì‹œì‘
                total_start_time = time.time()
                
                # 1ï¸âƒ£ í”„ë ˆì„ ì½ê¸° ì‹œê°„ ì¸¡ì •
                frame_read_start = time.time()
                retL, frameL = self.cap_left.read()
                retR, frameR = self.cap_right.read()
                frame_read_time = time.time() - frame_read_start
                self.step_times['frame_read'].append(frame_read_time)
                
                if not retL or not retR:
                    print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                    break

                # 2ï¸âƒ£ ê±°ìš¸ëª¨ë“œ ì ìš© ì‹œê°„ ì¸¡ì •
                mirror_start = time.time()
                if mirror_mode:
                    frameL = cv2.flip(frameL, 1)
                    frameR = cv2.flip(frameR, 1)
                mirror_time = time.time() - mirror_start
                self.step_times['mirror_flip'].append(mirror_time)

                # 3ï¸âƒ£ ğŸš€ ì´ˆê³ ì† ìŠ¤í‹°ì¹­ ì²˜ë¦¬ (ëª¨ë“  ìµœì í™” ì ìš©)
                # ğŸ†• ë°©ë²• 2: ì…ë ¥ë¶€í„° 1ì±„ë„ë¡œ ë³€í™˜í•˜ì—¬ ìŠ¤í‹°ì¹­ ì²˜ë¦¬
                gray_start = time.time()
                frameL_gray = cv2.cvtColor(frameL, cv2.COLOR_BGR2GRAY)
                frameR_gray = cv2.cvtColor(frameR, cv2.COLOR_BGR2GRAY)
                gray_time = time.time() - gray_start
                self.step_times['input_grayscale'].append(gray_time)
                
                # ğŸš€ 1ì±„ë„ë¡œ ìŠ¤í‹°ì¹­ ì²˜ë¦¬ (ê·¹í•œ ë©”ëª¨ë¦¬ ì ˆì•½!)
                stitched = self.stitch_frame_pair_ultra_optimized(frameL_gray, frameR_gray)
                
                # ğŸš€ ì „ì²´ ìŠ¤í‹°ì¹­ ì‹œê°„ ì¸¡ì • ì™„ë£Œ
                total_stitching_time = time.time() - total_start_time
                self.step_times['total_stitching'].append(total_stitching_time)
                
                # ğŸš€ ğŸ†• ë°©ë²• 2: ìŠ¤í‹°ì¹­ë¶€í„° 1ì±„ë„ë¡œ ì²˜ë¦¬ (ê·¹í•œ ë©”ëª¨ë¦¬ ì ˆì•½!)
                grayscale_start = time.time()
                
                # ğŸ†• ìŠ¤í‹°ì¹­ ê²°ê³¼ê°€ ì´ë¯¸ 1ì±„ë„ì´ë¯€ë¡œ ë³€í™˜ ë¶ˆí•„ìš”
                stitched_gray = stitched  # ì´ë¯¸ 1ì±„ë„ ê·¸ë ˆì´ìŠ¤ì¼€ì¼
                
                # ğŸ†• 1ì±„ë„ì„ 3ì±„ë„ë¡œ ë³µì œ (YOLO ëª¨ë¸ í˜¸í™˜ì„±)
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì„ 3ì±„ë„ë¡œ ë³µì œ: [H,W] â†’ [H,W,3] (ê°™ì€ ê°’ìœ¼ë¡œ 3ë²ˆ ë³µì œ)
                stitched_gray_3ch = np.stack([stitched_gray] * 3, axis=-1)
                
                grayscale_time = time.time() - grayscale_start
                self.step_times['output_grayscale'].append(grayscale_time)
                
                # ğŸš€ ë¹„ë™ê¸° ì•„ì´íŠ¸ë˜í‚¹ ìš”ì²­ (ë¸”ë¡œí‚¹ ì—†ìŒ) - 1ì±„ë„ ê¸°ë°˜ 3ì±„ë„ë¡œ ìš”ì²­
                self.process_gaze_tracking_async(stitched_gray_3ch)  # 1ì±„ë„ ê¸°ë°˜ 3ì±„ë„ ì‚¬ìš©
                
                # ğŸš€ ë¹„ë™ê¸° ì•„ì´íŠ¸ë˜í‚¹ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                stitched_with_gaze, gaze_data = self.get_gaze_result_async()
                if stitched_with_gaze is None:
                    stitched_with_gaze = stitched_gray  # ì•„ì´íŠ¸ë˜í‚¹ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì‚¬ìš©
                    gaze_data = []
                else:
                    # ğŸ†• ì•„ì´íŠ¸ë˜í‚¹ ê²°ê³¼ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (ì´ë¯¸ 1ì±„ë„ ê¸°ë°˜ì´ë¯€ë¡œ ë¹ ë¦„)
                    stitched_with_gaze = cv2.cvtColor(stitched_with_gaze, cv2.COLOR_BGR2GRAY)
                
                # 4ï¸âƒ£ ë””ìŠ¤í”Œë ˆì´ ë¦¬ì‚¬ì´ì¦ˆ ì‹œê°„ ì¸¡ì •
                resize_start = time.time()
                if DISPLAY_SCALE != 1.0:
                    # ì²« í”„ë ˆì„ì—ì„œë§Œ í¬ê¸° ê³„ì‚° (ìºì‹±) - í•˜ë“œì½”ë”©
                    if not hasattr(self, '_display_width') or not hasattr(self, '_display_height'):
                        self._display_width = int(stitched_with_gaze.shape[1] * DISPLAY_SCALE)
                        self._display_height = int(stitched_with_gaze.shape[0] * DISPLAY_SCALE)
                        print(f"ğŸ”§ ë””ìŠ¤í”Œë ˆì´ í¬ê¸° ìºì‹œ: {self._display_width}x{self._display_height}")
                    
                    # í•˜ë“œì½”ë”©ëœ í¬ê¸° ì‚¬ìš©
                    stitched_display = cv2.resize(stitched_with_gaze, (self._display_width, self._display_height), 
                                                interpolation=cv2.INTER_LINEAR)
                else:
                    stitched_display = stitched_with_gaze
                
                resize_time = time.time() - resize_start
                self.step_times['display_resize'].append(resize_time)
            
                # ğŸš€ ìµœì í™”ëœ FPS ê³„ì‚°
                self.update_fps()
                
                # ğŸš€ ì‹¤í—˜ ëª¨ë“œì—ì„œ ì‹œì„  ì¶”ì  ì •ë³´ëŠ” ì •ë³´ íŒ¨ë„ì—ë§Œ í‘œì‹œ (í™”ë©´ì—ëŠ” ì  ì—†ìŒ)
                
                # ğŸš€ ì •ë³´ íŒ¨ë„ ìƒì„± (í™”ë©´ ìœ„ì— ìƒˆë¡œìš´ ê³µê°„)
                info_panel_height = 200  # ì •ë³´ íŒ¨ë„ ë†’ì´
                total_height = stitched_display.shape[0] + info_panel_height
                total_width = stitched_display.shape[1]
                
                # ì „ì²´ ìº”ë²„ìŠ¤ ìƒì„± (ì •ë³´ íŒ¨ë„ + ë¹„ë””ì˜¤ í™”ë©´)
                combined_display = np.zeros((total_height, total_width, 3), dtype=np.uint8)
                
                # ë¹„ë””ì˜¤ í™”ë©´ì„ í•˜ë‹¨ì— ë°°ì¹˜
                combined_display[info_panel_height:, :] = cv2.cvtColor(stitched_display, cv2.COLOR_GRAY2BGR)
                
                # ğŸš€ ì‹¤í—˜ ëª¨ë“œ ì •ë³´ íŒ¨ë„ í‘œì‹œ
                if hasattr(self, 'show_experiment_info') and self.show_experiment_info:
                    # ì‹¤í—˜ ëª¨ë“œ ìƒíƒœ í‘œì‹œ
                    if hasattr(self, 'experiment_mode') and self.experiment_mode:
                        # ì‹¤í—˜ ì§„í–‰ ìƒí™© í‘œì‹œ
                        if hasattr(self, 'gaze_experiment') and self.gaze_experiment.is_recording:
                            # ğŸš€ ì‹¤ì‹œê°„ íƒ€ì´ë¨¸ í‘œì‹œ (ì •ë³´ íŒ¨ë„ì—)
                            elapsed_time, progress_percent = self.gaze_experiment.get_recording_progress()
                            remaining_time = max(0, GAZE_EXPERIMENT_CONFIG['recording_duration'] - elapsed_time)
                            
                            # íƒ€ì´ë¨¸ ë°” í‘œì‹œ (ì •ë³´ íŒ¨ë„ì—)
                            timer_bar_width = 300
                            timer_bar_height = 25
                            timer_x, timer_y = 20, 20
                            
                            # íƒ€ì´ë¨¸ ë°°ê²½
                            cv2.rectangle(combined_display, (timer_x, timer_y), 
                                        (timer_x + timer_bar_width, timer_y + timer_bar_height), (64, 64, 64), -1)
                            
                            # íƒ€ì´ë¨¸ ì§„í–‰ë¥ 
                            progress_width = int(timer_bar_width * (progress_percent / 100))
                            cv2.rectangle(combined_display, (timer_x, timer_y), 
                                        (timer_x + progress_width, timer_y + timer_bar_height), (0, 255, 0), -1)
                            
                            # íƒ€ì´ë¨¸ í…ìŠ¤íŠ¸
                            timer_text = f"Recording Target {self.gaze_experiment.current_target}: {remaining_time:.1f}s / {GAZE_EXPERIMENT_CONFIG['recording_duration']}s"
                            cv2.putText(combined_display, timer_text, (timer_x, timer_y + 18), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                            
                            # ì§„í–‰ë¥  í¼ì„¼íŠ¸
                            progress_text = f"Progress: {progress_percent:.1f}%"
                            cv2.putText(combined_display, progress_text, (timer_x + timer_bar_width + 20, timer_y + 18), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                            
                            # ğŸš€ ì‹¤ì‹œê°„ ì¢Œí‘œ í‘œì‹œ (ì •ë³´ íŒ¨ë„ì—ë§Œ)
                            current_x, current_y = self.get_current_gaze_coordinates()
                            if current_x is not None and current_y is not None:
                                coord_text = f"Current Gaze: ({current_x:.1f}, {current_y:.1f})"
                                cv2.putText(combined_display, coord_text, (timer_x, timer_y + 50), 
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                            
                            # ì‹¤í—˜ ì§€ì‹œì‚¬í•­
                            instruction_text = "Keep your gaze steady on the target for 10 seconds"
                            cv2.putText(combined_display, instruction_text, (timer_x, timer_y + 80), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
                            
                            # í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ ì•ˆë‚´
                            key_info = "Press 'x' to stop recording manually"
                            cv2.putText(combined_display, key_info, (timer_x, timer_y + 100), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)
                        
                        # ì‹¤í—˜ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ì œê±° (íƒ€ê²Ÿì€ ì‚¬ìš©ìê°€ ì§ì ‘ ë§Œë“¦)
                        
                        # ì‹¤í—˜ ëª¨ë“œ ìƒíƒœ í‘œì‹œ
                        status_text = "EXPERIMENT MODE: ACTIVE"
                        cv2.putText(combined_display, status_text, (20, 180), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                        
                        # ì‹¤í—˜ ì§„í–‰ ë‹¨ê³„ í‘œì‹œ
                        if hasattr(self, 'target_a_stats') and self.target_a_stats:
                            if hasattr(self, 'target_b_stats') and self.target_b_stats:
                                stage_text = "Stage: COMPLETED - Press 'c' to compare results"
                                cv2.putText(combined_display, stage_text, (400, 180), 
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                            else:
                                stage_text = "Stage: Target A completed - Press 'b' for Target B"
                                cv2.putText(combined_display, stage_text, (400, 180), 
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                        else:
                            stage_text = "Stage: Ready - Press 'a' to start Target A"
                            cv2.putText(combined_display, stage_text, (400, 180), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                    else:
                        # ì‹¤í—˜ ëª¨ë“œ ë¹„í™œì„±í™” ìƒíƒœ (ì •ë³´ íŒ¨ë„ì—)
                        status_text = "EXPERIMENT MODE: INACTIVE"
                        cv2.putText(combined_display, status_text, (20, 50), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)
                        
                        # ì‹¤í—˜ ëª¨ë“œ í™œì„±í™” ì•ˆë‚´
                        instruction_text = "Press 'e' to activate experiment mode"
                        cv2.putText(combined_display, instruction_text, (20, 80), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
                        
                        # í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ ì•ˆë‚´
                        key_info = "Available keys: 'e' (experiment), 'm' (mirror), 'q' (quit)"
                        cv2.putText(combined_display, key_info, (20, 100), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)
                
                # ì¤‘ì²© ì˜ì—­ í’ˆì§ˆ í™•ì¸ (ì²« í”„ë ˆì„ì—ì„œë§Œ)
                if frame_count == 0:
                    if hasattr(self, 'left_blend_mask') and hasattr(self, 'right_blend_mask'):
                        overlap_quality = np.sum((self.left_blend_mask > 0) & (self.right_blend_mask > 0))
                        print(f"   ğŸ“Š ì¤‘ì²© ì˜ì—­ í’ˆì§ˆ: {overlap_quality} í”½ì…€")
                        print(f"   ğŸ¯ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì ìš© ì™„ë£Œ")
                        print(f"   ğŸ¨ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ ì™„ë£Œ")
                        print(f"   ğŸ“Š ì±„ë„ ì •ë³´: ì…ë ¥ 3ì±„ë„ â†’ ì²˜ë¦¬ 1ì±„ë„ â†’ ì¶œë ¥ 1ì±„ë„")
                
                # ê²°ê³¼ í‘œì‹œ (ì •ë³´ íŒ¨ë„ + ë¹„ë””ì˜¤ í™”ë©´)
                cv2.imshow('Grayscale Real-time Stitching + Experiment Panel', combined_display)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ğŸ’¡ 'q' í‚¤ ì…ë ¥ìœ¼ë¡œ ì¢…ë£Œ")
                    break
                elif key == ord('m'):
                    mirror_mode = not mirror_mode
                    print(f"ğŸ”„ ê±°ìš¸ëª¨ë“œ: {'í™œì„±í™”' if mirror_mode else 'ë¹„í™œì„±í™”'}")
                    
                    # ê±°ìš¸ëª¨ë“œ ì „í™˜ (ì´ë¯¸ì§€ ì¢Œìš° ë°˜ì „ë§Œ)
                    self.switch_to_mirror_mode(mirror_mode)
                
                # ğŸš€ ì‹¤í—˜ ëª¨ë“œ í‚¤ ì…ë ¥ ì²˜ë¦¬
                elif key == ord('e'):
                    self.toggle_experiment_mode()
                elif key == ord('a') and self.experiment_mode:
                    self.start_target_recording('A')
                elif key == ord('b') and self.experiment_mode:
                    self.start_target_recording('B')
                elif key == ord('s') and self.experiment_mode:
                    self.show_experiment_statistics()
                elif key == ord('c') and self.experiment_mode:
                    self.compare_experiment_targets()
                elif key == ord('f') and self.experiment_mode:
                    self.save_experiment_results()
                elif key == ord('r') and self.experiment_mode:
                    self.reset_experiment_data()
                elif key == ord('x') and self.experiment_mode:
                    # ğŸš€ ìˆ˜ë™ìœ¼ë¡œ ê¸°ë¡ ì¤‘ì§€
                    if hasattr(self, 'gaze_experiment') and self.gaze_experiment.is_recording:
                        print(f"ğŸ›‘ ìˆ˜ë™ìœ¼ë¡œ ê¸°ë¡ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.")
                        self.stop_target_recording()
                    else:
                        print(f"âš ï¸ í˜„ì¬ ê¸°ë¡ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
                elif key == ord('p'):
                    # ğŸš€ ì„±ëŠ¥ ì¸¡ì • í† ê¸€
                    self.performance_measurement = not self.performance_measurement
                    if self.performance_measurement:
                        print(f"ğŸ“Š ì„±ëŠ¥ ì¸¡ì • í™œì„±í™” - FPS ë¡œê·¸ì™€ ì„±ëŠ¥ í†µê³„ê°€ ì¶œë ¥ë©ë‹ˆë‹¤")
                    else:
                        print(f"ğŸ“Š ì„±ëŠ¥ ì¸¡ì • ë¹„í™œì„±í™” - ì½˜ì†” ì¶œë ¥ì´ ìµœì†Œí™”ë©ë‹ˆë‹¤")
                
                # ğŸš€ í”„ë ˆì„ ì¹´ìš´í„° ì¦ê°€ (ì„±ëŠ¥ ì¸¡ì •ìš©)
                frame_count += 1
                self.frame_count = frame_count  # í´ë˜ìŠ¤ ë³€ìˆ˜ì— í• ë‹¹
                

                
        except KeyboardInterrupt:
            print("\nğŸ’¡ Ctrl+Cë¡œ ì¢…ë£Œ")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            # ì •ë¦¬
            if self.cap_left:
                self.cap_left.release()
            if self.cap_right:
                self.cap_right.release()
            
            # ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ìŠ¤ë ˆë“œ ì •ë¦¬
            self.gaze_thread_running = False
            if hasattr(self, 'gaze_thread') and self.gaze_thread.is_alive():
                self.gaze_thread.join(timeout=1.0)
            
            cv2.destroyAllWindows()
            print("âœ… ì •ë¦¬ ì™„ë£Œ")

    def release(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        if hasattr(self, 'cap_left') and self.cap_left:
            self.cap_left.release()
        if hasattr(self, 'cap_right') and self.cap_right:
            self.cap_right.release()
        
        # ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ìŠ¤ë ˆë“œ ì •ë¦¬
        if hasattr(self, 'gaze_thread_running'):
            self.gaze_thread_running = False
        if hasattr(self, 'gaze_thread') and self.gaze_thread.is_alive():
            self.gaze_thread.join(timeout=1.0)
        
        cv2.destroyAllWindows()
        print("âœ… ëª¨ë“  ë¦¬ì†ŒìŠ¤ í•´ì œ ì™„ë£Œ")

    def process_gaze_tracking(self, frame: np.ndarray) -> Tuple[np.ndarray, list]:
        """ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì²˜ë¦¬ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ í˜¸í™˜)"""
        if self.yolo_model is None or self.face_mesh is None:
            return frame, []
        
        try:
            # YOLO ì–¼êµ´ ê²€ì¶œ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì—ì„œë„ ì‘ë™)
            yolo_results = self.yolo_model.predict(frame, verbose=False)[0]
            face_boxes = [
                box for box in yolo_results.boxes.data.cpu().numpy()
                if int(box[5]) == 0  # í´ë˜ìŠ¤ 0: ì–¼êµ´
            ]
            
            results = []
            h, w = frame.shape[:2]  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼: shape[0], shape[1]
            
            for face_idx, box in enumerate(face_boxes):
                if face_idx >= GAZE_TRACKING_CONFIG['max_faces']:
                    break
                    
                x1, y1, x2, y2, conf, cls = box.astype(int)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)

                face_roi = frame[y1:y2, x1:x2]
                # ğŸ†• ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì„ RGBë¡œ ë³€í™˜ (MediaPipeëŠ” RGB í•„ìš”)
                if len(face_roi.shape) == 2:  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì¸ ê²½ìš°
                    face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_GRAY2RGB)
                else:  # ì´ë¯¸ RGBì¸ ê²½ìš°
                    face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)
                mesh_result = self.face_mesh.process(face_rgb)

                if mesh_result.multi_face_landmarks:
                    for face_landmarks in mesh_result.multi_face_landmarks:
                        # ê¸°ë³¸ ëœë“œë§ˆí¬ í¬ì¸íŠ¸ë“¤
                        image_points = []
                        for idx in self.landmark_ids:
                            lm = face_landmarks.landmark[idx]
                            x_lm = int(lm.x * (x2 - x1)) + x1
                            y_lm = int(lm.y * (y2 - y1)) + y1
                            image_points.append((x_lm, y_lm))
                            # ğŸ†• ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì—ì„œëŠ” í°ìƒ‰ìœ¼ë¡œ í‘œì‹œ
                            cv2.circle(frame, (x_lm, y_lm), 2, 255, -1)

                        # í™ì±„ ì¤‘ì‹¬ì  ì¶”ì¶œ
                        iris_coords = []
                        for idx in self.iris_ids:
                            if idx < len(face_landmarks.landmark):
                                lm = face_landmarks.landmark[idx]
                                x_lm = int(lm.x * (x2 - x1)) + x1
                                y_lm = int(lm.y * (y2 - y1)) + y1
                                iris_coords.append((x_lm, y_lm))
                                # ğŸ†• ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì—ì„œëŠ” í°ìƒ‰ìœ¼ë¡œ í‘œì‹œ
                                cv2.circle(frame, (x_lm, y_lm), 3, 255, -1)
                        
                        # ëˆˆ ì¤‘ì‹¬ ê³„ì‚°
                        roi_info = (x1, y1, x2-x1, y2-y1)
                        left_eye_center = self._get_eye_center_from_landmarks(face_landmarks, self.left_eye_landmarks, roi_info)
                        right_eye_center = self._get_eye_center_from_landmarks(face_landmarks, self.right_eye_landmarks, roi_info)
                        
                        eye_centers = []
                        if left_eye_center:
                            eye_centers.append(left_eye_center)
                        if right_eye_center:
                            eye_centers.append(right_eye_center)

                        # ì‹œì„  ì¶”ì •
                        gaze = (0, 0)
                        
                        # í™ì±„ ì¤‘ì‹¬ ê¸°ë°˜
                        if len(iris_coords) == 2:
                            lx, ly = iris_coords[0]
                            rx, ry = iris_coords[1]
                            iris_gaze = ((rx + lx) / 2 / w - 0.5, (ry + ly) / 2 / h - 0.5)
                        else:
                            iris_gaze = None
                        
                        # ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ ì¤‘ì‹¬ ê¸°ë°˜
                        if len(eye_centers) == 2:
                            lx, ly = eye_centers[0]
                            rx, ry = eye_centers[1]
                            landmark_gaze = ((rx + lx) / 2 / w - 0.5, (ry + ly) / 2 / h - 0.5)
                        else:
                            landmark_gaze = None
                        
                        # ë‘ ë°©ë²•ì˜ ê°€ì¤‘ í‰ê· 
                        if iris_gaze and landmark_gaze:
                            gaze = (
                                0.7 * iris_gaze[0] + 0.3 * landmark_gaze[0],
                                0.7 * iris_gaze[1] + 0.3 * landmark_gaze[1]
                            )
                        elif iris_gaze:
                            gaze = iris_gaze
                        elif landmark_gaze:
                            gaze = landmark_gaze

                        # ì‹œê°„ì  ì•ˆì •í™” ì ìš©
                        if face_idx < len(self.gaze_stabilizers):
                            stabilizer = self.gaze_stabilizers[face_idx]
                            stabilizer.add_sample(gaze, iris_coords)
                            stable_gaze = stabilizer.get_stabilized_gaze()
                            gaze = stable_gaze

                        # ğŸš€ ì‹¤í—˜ ë°ì´í„° ê¸°ë¡ (ì²« ë²ˆì§¸ ì–¼êµ´ë§Œ)
                        if face_idx == 0 and hasattr(self, 'gaze_experiment'):
                            # ì‹œì„  ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                            gaze_x = (gaze[0] + 0.5) * w  # -0.5~0.5 â†’ 0~w
                            gaze_y = (gaze[1] + 0.5) * h  # -0.5~0.5 â†’ 0~h
                            
                            # ì‹¤í—˜ ê¸°ë¡ê¸°ì— ì‹œì„  ì¢Œí‘œ ì¶”ê°€
                            self.gaze_experiment.add_gaze_sample(gaze_x, gaze_y)

                        results.append({"gaze": gaze, "iris_coords": iris_coords})

            return frame, results
            
        except Exception as e:
            print(f"âš ï¸ ì•„ì´íŠ¸ë˜í‚¹ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return frame, []

    def _get_eye_center_from_landmarks(self, face_landmarks, eye_landmark_ids, roi_offset):
        """ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ë“¤ì˜ í‰ê· ìœ¼ë¡œ ëˆˆ ì¤‘ì‹¬ ê³„ì‚°"""
        eye_points = []
        x1, y1, roi_width, roi_height = roi_offset
        
        for idx in eye_landmark_ids:
            if idx < len(face_landmarks.landmark):
                lm = face_landmarks.landmark[idx]
                x_lm = int(lm.x * roi_width) + x1
                y_lm = int(lm.y * roi_height) + y1
                eye_points.append((x_lm, y_lm))
        
        if eye_points:
            center_x = int(np.mean([p[0] for p in eye_points]))
            center_y = int(np.mean([p[1] for p in eye_points]))
            return (center_x, center_y)
        return None

    def draw_gaze_info(self, frame: np.ndarray, gaze_data: list, fps: float):
        """ğŸš€ ì‹œì„  ì¶”ì  ì •ë³´ í‘œì‹œ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ í˜¸í™˜)"""
        # FPS í‘œì‹œ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì—ì„œëŠ” í°ìƒ‰)
        cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 255, 2)
        
        # ì¶”ì  ìƒíƒœ í‘œì‹œ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì—ì„œëŠ” í°ìƒ‰/ê²€ì€ìƒ‰)
        status_color = 255 if len(gaze_data) > 0 else 0
        cv2.putText(frame, f"Faces: {len(gaze_data)}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)
        
        # ê° ì–¼êµ´ì˜ ì‹œì„  ì •ë³´ í‘œì‹œ
        for i, data in enumerate(gaze_data):
            y_offset = 90 + i * 120
            cv2.putText(frame, f"Person {i+1}", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, 255, 1)
            cv2.putText(frame, f"Gaze: ({data['gaze'][0]:.3f}, {data['gaze'][1]:.3f})", (10, y_offset+25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 255, 1)
            
            # ì‹œì„  ë°©í–¥ í‘œì‹œ (í™”ë©´ ì¤‘ì•™ ê¸°ì¤€)
            h, w = frame.shape[:2]
            center_x, center_y = w // 2, h // 2
            gaze_x = int(center_x + data['gaze'][0] * 200)  # ìŠ¤ì¼€ì¼ ì¡°ì •
            gaze_y = int(center_y + data['gaze'][1] * 200)
            cv2.arrowedLine(frame, (center_x, center_y), (gaze_x, gaze_y), 255, 3)
        
        # ì•„ì´íŠ¸ë˜í‚¹ í™œì„±í™” í‘œì‹œ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì—ì„œëŠ” í°ìƒ‰)
        cv2.putText(frame, "EYE TRACKING: ON", (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, 255, 2)

    def stitch_frame_pair_ultra_optimized(self, left_frame: np.ndarray, right_frame: np.ndarray) -> np.ndarray:
        """ğŸš€ ì´ˆê³ ì† í”„ë ˆì„ ìŠ¤í‹°ì¹­ (ROI ê¸°ë°˜ ê·¹í•œ ìµœì í™”!)"""
        
        # ğŸš€ ROI ëª¨ë“œì™€ ì „ì²´ ëª¨ë“œ ë¶„ê¸° ì²˜ë¦¬
        if hasattr(self, 'use_roi') and self.use_roi and hasattr(self, 'roi_homography_coords'):
            return self._stitch_frame_pair_roi_optimized(left_frame, right_frame)
        else:
            return self._stitch_frame_pair_full_optimized(left_frame, right_frame)
    
    def _stitch_frame_pair_roi_optimized(self, left_frame: np.ndarray, right_frame: np.ndarray) -> np.ndarray:
        """ğŸš€ ROI ê¸°ë°˜ ê·¹í•œ ìµœì í™” ìŠ¤í‹°ì¹­ (ì „ì²´ ìŠ¤í‹°ì¹­ í›„ ROI í¬ë¡­!)"""
        # ğŸ¯ ì‚¬ìš©ìê°€ ì›í•œ ë°©ì‹: ì „ì²´ ìŠ¤í‹°ì¹­ í›„ ROI í¬ë¡­!
        
        # ğŸš€ 1ë‹¨ê³„: ì „ì²´ ìŠ¤í‹°ì¹­ (ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©!)
        full_stitched = self._stitch_frame_pair_full_optimized(left_frame, right_frame)
        
        # ğŸš€ 2ë‹¨ê³„: ROI í¬ë¡­ (ì‚¬ìš©ìê°€ ì„ íƒí•œ ROI ì˜ì—­ë§Œ!)
        roi_start = time.time()
        
        # ğŸš¨ ROI ì¢Œí‘œëŠ” ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # (Stitching_Engineì—ì„œ 0.5 ìŠ¤ì¼€ì¼ë¡œ ì„ íƒí–ˆì§€ë§Œ, ì¢Œí‘œëŠ” ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€)
        roi_image = full_stitched[self.roi_y1:self.roi_y2, self.roi_x1:self.roi_x2]
        
        roi_time = time.time() - roi_start
        self.step_times['roi_crop'].append(roi_time)
        
        return roi_image
    
    def _stitch_frame_pair_full_optimized(self, left_frame: np.ndarray, right_frame: np.ndarray) -> np.ndarray:
        """ğŸš€ ì „ì²´ í¬ê¸° ê¸°ì¡´ ìµœì í™” ìŠ¤í‹°ì¹­ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        # ğŸš€ 1ë‹¨ê³„: ë Œì¦ˆ ë³´ì • ì‹œê°„ ì¸¡ì • (ì›ë˜ ë°©ì‹ìœ¼ë¡œ ë³µì›!)
        lens_start = time.time()
        left_rectified = self.rectify_left(left_frame)
        right_rectified = self.rectify_right(right_frame)
        lens_time = time.time() - lens_start
        self.step_times['lens_rectification'].append(lens_time)
        
        # ğŸš€ 2ë‹¨ê³„: ìº”ë²„ìŠ¤ ë³µì‚¬ ì‹œê°„ ì¸¡ì •
        canvas_start = time.time()
        
        # ğŸš¨ í•­ìƒ ì „ì²´ í¬ê¸° ìº”ë²„ìŠ¤ ì‚¬ìš© (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
        # ğŸ†• ê·¸ë ˆì´ìŠ¤ì¼€ì¼ìš© 1ì±„ë„ ìº”ë²„ìŠ¤ ìƒì„±
        canvas = np.zeros((self.canvas_size[1], self.canvas_size[0]), dtype=np.uint8)
        canvas[self.left_slice_y, self.left_slice_x] = left_rectified
        
        canvas_time = time.time() - canvas_start
        self.step_times['canvas_copy'].append(canvas_time)
        
        # ğŸš€ 3ë‹¨ê³„: í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜ ì‹œê°„ ì¸¡ì • (ì›ë˜ ë°©ì‹ìœ¼ë¡œ ë³µì›!)
        homography_start = time.time()
        
        # ğŸš¨ ì›ë˜ ë°©ì‹: cv2.warpPerspective ì‚¬ìš© (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
        warped_right = cv2.warpPerspective(right_rectified, self.homography_matrix_opt, 
                                       self.homography_size, flags=cv2.INTER_LINEAR)
        
        homography_time = time.time() - homography_start
        self.step_times['homography_warp'].append(homography_time)
        
        # ğŸš€ 4ë‹¨ê³„: ë¸”ë Œë”© ì‹œê°„ ì¸¡ì •
        blending_start = time.time()
        
        # ğŸš€ ë¹ ë¥¸ ë¸”ë Œë”© ì‚¬ìš© (ë§ˆìŠ¤í¬ ê³„ì‚° ì•ˆí•¨!)
        final_image = self._blend_images_ultra_fast(canvas, warped_right)
        
        blending_time = time.time() - blending_start
        self.step_times['blending'].append(blending_time)
        
        return final_image

    def _blend_images_roi_fast(self, left_roi: np.ndarray, right_roi: np.ndarray) -> np.ndarray:
        """ğŸš€ ROI í¬ê¸° ì´ë¯¸ì§€ ì´ˆê³ ì† ë¸”ë Œë”© (ROI ì „ìš©!)"""
        # ROI í¬ê¸°ë¡œ ê°„ë‹¨í•œ ë¸”ë Œë”© (ê°€ì¥ ë¹ ë¥¸ ë°©ì‹)
        # ì™¼ìª½ ì´ë¯¸ì§€ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ê³ , ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ê°€ ìˆëŠ” ë¶€ë¶„ë§Œ ë®ì–´ì“°ê¸°
        result = left_roi.copy()
        mask = right_roi > 10  # ì„ê³„ê°’ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„±
        result[mask] = right_roi[mask]
        return result

    def _blend_images_ultra_fast_full(self, left_image: np.ndarray, right_image: np.ndarray) -> np.ndarray:
        """ğŸš€ ì „ì²´ í¬ê¸° ì´ë¯¸ì§€ ì´ˆê³ ì† ë¸”ë Œë”© (ROI ì²˜ë¦¬ìš©)"""
        if hasattr(self, 'use_precomputed_blending') and self.use_precomputed_blending:
            # ì›ë³¸ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì‚¬ìš© (ì „ì²´ í¬ê¸°)
            full_left_mask = self.left_blend_mask_original
            full_right_mask = self.right_blend_mask_original
            
            # 3ì±„ë„ ë§ˆìŠ¤í¬ ìƒì„±
            full_left_mask_3ch = np.stack([full_left_mask] * 3, axis=-1)
            full_right_mask_3ch = np.stack([full_right_mask] * 3, axis=-1)
            
            # ê°€ì¤‘ í‰ê·  ë¸”ë Œë”©
            blended = (left_image * full_left_mask_3ch + right_image * full_right_mask_3ch).astype(np.uint8)
        else:
            # ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ë‹¨ìˆœ ë§ì…ˆ
            blended = np.clip(left_image + right_image, 0, 255).astype(np.uint8)
        
        return blended

    def _apply_lens_correction_fast(self, frame, is_left=True):
        """ğŸš€ LUT ê¸°ë°˜ ë¹ ë¥¸ ë Œì¦ˆ ë³´ì •"""
        if is_left:
            return cv2.remap(frame, self.left_lut_x, self.left_lut_y, 
                            cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        else:
            return cv2.remap(frame, self.right_lut_x, self.right_lut_y, 
                            cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)

    def _apply_homography_fast(self, frame):
        """ğŸš€ ì‚¬ì „ ê³„ì‚°ëœ ì¢Œí‘œ ë§µìœ¼ë¡œ ë¹ ë¥¸ í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜"""
        # ğŸš¨ ì¤‘ìš”: frameì€ ì¹´ë©”ë¼ í•´ìƒë„(1080x1920), ì¢Œí‘œ ë§µì€ ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°(1342x3633)
        frame_h, frame_w = frame.shape[:2]
        canvas_h, canvas_w = self.canvas_size[1], self.canvas_size[0]
        
        # ì¢Œí‘œë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ìƒ˜í”Œë§
        coords = self.homography_coords  # 1342x3633x2
        
        # ì¢Œí‘œ ë²”ìœ„ ê²€ì¦ ë° í´ë¦¬í•‘
        coords_int = coords.astype(np.int32)
        coords_int[:, :, 0] = np.clip(coords_int[:, :, 0], 0, frame_w-1)  # x ì¢Œí‘œ
        coords_int[:, :, 1] = np.clip(coords_int[:, :, 1], 0, frame_h-1)  # y ì¢Œí‘œ
        
        # ë²¡í„°í™”ëœ ìƒ˜í”Œë§ (í›¨ì”¬ ë¹ ë¦„!)
        warped = frame[coords_int[:, :, 1], coords_int[:, :, 0]]
        
        return warped


if __name__ == "__main__":
# ğŸš€ ë‹¨ìƒ‰(ê·¸ë ˆì´ìŠ¤ì¼€ì¼) ì´ˆê³ ì† ìµœì í™”ëœ ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ ì‹¤í–‰
    print("ğŸš€ Ultra-Fast Video Stitcher Grayscale + Eye Tracking v4.0 ì‹œì‘")
    print("ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 10 FPS â†’ 40+ FPS (300%+ í–¥ìƒ)")
    print("ğŸ‘ï¸ ì•„ì´íŠ¸ë˜í‚¹: YOLO ì–¼êµ´ ê²€ì¶œ + MediaPipe FaceMesh + ì‹œì„  ì•ˆì •í™” (ê·¸ë ˆì´ìŠ¤ì¼€ì¼)")
    print("ğŸ¯ ROI ìµœì í™”: ì‚¬ìš©ì ì •ì˜ ROI ì§€ì›ìœ¼ë¡œ ì¶”ê°€ FPS í–¥ìƒ")
    print("ğŸš« ë Œì¦ˆ ë³´ì • LUT: ì‚¬ì „ ê³„ì‚° ì œê±° (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)")
    print("ğŸš« í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ: ì‚¬ì „ ê³„ì‚° ì œê±° (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)")
    print("ğŸ¨ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ëª¨ë“œ: 1ì±„ë„ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ 1/3, ì²˜ë¦¬ ì†ë„ 2-3ë°° í–¥ìƒ")
    
    # ğŸš€ ë™ê³µ ì¶”ì  ì‹¤í—˜ ê¸°ëŠ¥ ì•ˆë‚´
    print("\nğŸ”¬ ë™ê³µ ì¶”ì  ì‹¤í—˜ ê¸°ëŠ¥:")
    print("   - 0.5ë„ ì‹œì„  ì¶”ì  ì •í™•ë„ ì¸¡ì •")
    print("   - íƒ€ê²Ÿ A, B ì‘ì‹œ ë°ì´í„° ìˆ˜ì§‘")
    print("   - ì‹¤ì‹œê°„ í†µê³„ ë¶„ì„ ë° ë¹„êµ")
    print("   - ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ì €ì¥")
    print("   - í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤:")
    print("     'e': ì‹¤í—˜ ëª¨ë“œ í† ê¸€")
    print("     'a': íƒ€ê²Ÿ A ì‘ì‹œ ê¸°ë¡ ì‹œì‘")
    print("     'b': íƒ€ê²Ÿ B ì‘ì‹œ ê¸°ë¡ ì‹œì‘")
    print("     'x': ìˆ˜ë™ ê¸°ë¡ ì¤‘ì§€")
    print("     's': í˜„ì¬ í†µê³„ ì¶œë ¥")
    print("     'c': íƒ€ê²Ÿ A vs B ë¹„êµ")
    print("     'f': ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥")
    print("     'r': ì‹¤í—˜ ë°ì´í„° ì´ˆê¸°í™”")
    print("     'm': ê±°ìš¸ëª¨ë“œ í† ê¸€")
    print("     'p': ì„±ëŠ¥ ì¸¡ì • í† ê¸€ (ì½˜ì†” ì¶œë ¥ ì œì–´)")
    print("     'q': ì¢…ë£Œ")
    print("   - ì‹¤í—˜ ì§„í–‰:")
    print("     1. 'e'ë¡œ ì‹¤í—˜ ëª¨ë“œ í™œì„±í™”")
    print("     2. 'a'ë¡œ íƒ€ê²Ÿ A ì‘ì‹œ ì‹œì‘ (10ì´ˆ ìë™ ê¸°ë¡)")
    print("     3. 'b'ë¡œ íƒ€ê²Ÿ B ì‘ì‹œ ì‹œì‘ (10ì´ˆ ìë™ ê¸°ë¡)")
    print("     4. 'c'ë¡œ ê²°ê³¼ ë¹„êµ ë¶„ì„")
    print("     5. 'f'ë¡œ ê²°ê³¼ íŒŒì¼ ì €ì¥")
    print("   - í™”ë©´ í‘œì‹œ:")
    print("     - ëª¨ë“  í…ìŠ¤íŠ¸ ì œê±°ë¨ (ê¹”ë”í•œ í™”ë©´)")
    print("     - ìƒë‹¨ì— íƒ€ì´ë¨¸ ë°”ë§Œ í‘œì‹œ")
    print("     - í˜„ì¬ ì‹œì„  ìœ„ì¹˜ë¥¼ ë¹¨ê°„ìƒ‰ ì ìœ¼ë¡œ í‘œì‹œ")
    print("     - ì½˜ì†”ì—ì„œ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í™•ì¸")
    print("   - ì„±ëŠ¥ ì¸¡ì •:")
    print("     - ê¸°ë³¸ê°’: ë¹„í™œì„±í™” (ì½˜ì†” ì¶œë ¥ ìµœì†Œí™”)")
    print("     - 'p' í‚¤ë¡œ í•„ìš”ì‹œì—ë§Œ í™œì„±í™” ê°€ëŠ¥")
    print("     - í™œì„±í™” ì‹œ: FPS ë¡œê·¸ 10ì´ˆë§ˆë‹¤, ì„±ëŠ¥ í†µê³„ 30ì´ˆë§ˆë‹¤")
    
    stitcher = UltraFastVideoStitcherGrayscale("./data/config/homography_params.json")
    
    # ğŸ†• ROI ëª¨ë“œ ìƒíƒœ í™•ì¸
    if hasattr(stitcher, 'use_roi') and stitcher.use_roi:
        print(f"âœ… ROI ëª¨ë“œ í™œì„±í™”: {stitcher.roi_width} x {stitcher.roi_height}")
        print("   ğŸš€ ROI ê¸°ë°˜ ìµœì í™” + ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ê·¹í•œ FPS í–¥ìƒ ì˜ˆìƒ")
    else:
        print("â„¹ï¸ ROI ëª¨ë“œ ë¹„í™œì„±í™”: ì „ì²´ í™”ë©´ ëª¨ë“œ")
        print("   ğŸ’¡ ROIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Stitching_Engine_3.pyì—ì„œ ROIë¥¼ ì„ íƒí•˜ì„¸ìš”")
    
    # ì¹´ë©”ë¼ ì„¤ì •
    if not stitcher.setup_cameras():
        print("âŒ ì¹´ë©”ë¼ ì„¤ì • ì‹¤íŒ¨")
        exit(1)
    
    stitcher.show_stitched_video_grayscale()
