1. 옵티마이저 최적화
AdamW 매개변수: betas=(0.9, 0.999), eps=1e-8 설정
더 안정적인 수렴을 위한 표준 설정
2. 스케줄러 개선
OneCycleLR → CosineAnnealingWarmRestarts
주기적 재시작으로 local minima 탈출
T_0=10, T_mult=2: 10→20→40 에포크 주기
3. 손실함수 고도화
채널별 가중치: [1.0, 1.1, 1.0] (180도를 조금 더 중요하게)
더 정교한 학습 가능
4. 데이터 증강
수평/수직 플리핑: 50% 확률
90도 회전: 25% 확률
훈련 데이터 다양성 대폭 증가
5. 네트워크 아키텍처 강화
Channel Attention 메커니즘 추가
중요한 특징에 집중하는 능력 향상
디코더 각 단계에 attention 적용
6. 하이퍼파라미터 최적화
배치 크기: 8 → 16 (안정성 향상)
에포크: 80 → 120 (충분한 학습)
학습률: 1e-3 → 2e-3 (빠른 수렴)
가중치 감쇠: 1e-4 → 5e-5 (과적합 방지)
워커 수: 4 → 6 (데이터 로딩 속도 향상)