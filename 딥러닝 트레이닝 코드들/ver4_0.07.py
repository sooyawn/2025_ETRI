# FINCH/DLPS — Best-Practice Trainer (ACS head, RMSE eval)\n# ---------------------------------------------------------\n# 목표\n#  - 입력: 0° 간섭무늬 I0 (세기 이미지, 1×H×W)\n#  - 출력: (A, C, S) 파라미터 3채널\n#  - 학습: 예측 (Â, Ĉ, Ŝ)로 90°/180°/270° 세기 프레임을 합성(폐형식) 후,\n#          라벨 I90/I180/I270과의 **채널별 RMSE**만으로 학습 (논문과 동일 지표)\n#  - 평가: 채널별 RMSE와 합을 로그/베스트 저장 (.pth: state_dict)\n#\n# 왜 ACS?\n#  - I_δ = A + C cos δ − S sin δ (픽셀별 PSI 표준식) 로 세기를 파라미터화 → 해공간 축소\n#  - 논문도 예측/라벨 모두 intensity 프레임이므로 모순 없음\n#  - 보고 지표는 RMSE이므로 공정 비교 그대로 유지\n#\n# 안전 장치\n#  - degree_to_index = {0:0, 90:1, 180:2, 270:3} (버그 방지)\n#  - 누락 프레임은 학습에서 제외(제로패딩 금지)\n#  - 입력/라벨 0~1 스케일 고정 (감마 불명 이미지라면 사전 linearize 권장)\n#\n# 선택 기능\n#  - I0 일관성(합성한 I0와 입력 I0의 RMSE)을 아주 작은 가중치로 추가 가능 (기본 0.0=끄기)\n#  - 업샘플+Conv로 체크보드 방지 (Deconv 미사용)\n#  - AMP / Gradient Clipping / OneCycleLR 지원\n\nimport os, glob, math, random\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple, Optional\n\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# --------------------\n# Utils\n# --------------------\n\ndef set_seed(seed: int = 42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\n@torch.no_grad()\ndef to_tensor01(img: Image.Image) -> torch.Tensor:\n    """Convert PIL image to float tensor in [0,1], shape (H,W). Support 8/16-bit."""\n    # Preserve linear domain; no gamma.\n    if img.mode in ("I;16", "I;16B", "I"):\n        arr = np.array(img, dtype=np.uint16).astype(np.float32) / 65535.0\n    else:\n        arr = np.array(img.convert('L'), dtype=np.uint8).astype(np.float32) / 255.0\n    return torch.from_numpy(arr)\n\n# --------------------\n# Dataset\n# --------------------\nclass HologramDataset(Dataset):\n    """Expect folder layout:\n    root/\n      train|validation|test/\n        sample_XXXX/phase_0.png  # 0°\n                           /phase_1.png  # 90°\n                           /phase_2.png  # 180°\n                           /phase_3.png  # 270°\n    """\n    def __init__(self, root: str, split: str, ensure_all_four: bool = True):\n        self.data_dir = os.path.join(root, split)\n        self.sample_dirs = sorted(glob.glob(os.path.join(self.data_dir, "sample_*")))\n        self.ensure_all_four = ensure_all_four\n        print(f"[{split}] {len(self.sample_dirs)} samples @ {self.data_dir}")\n\n    def __len__(self): return len(self.sample_dirs)\n\n    def __getitem__(self, idx):\n        sd = self.sample_dirs[idx]\n        paths = [os.path.join(sd, f"phase_{i}.png") for i in range(4)]\n        exists = [os.path.exists(p) for p in paths]\n        if self.ensure_all_four and not all(exists):\n            missing = [i for i,b in enumerate(exists) if not b]\n            raise FileNotFoundError(f"Missing phase file(s) {missing} in {sd}")\n        imgs = [to_tensor01(Image.open(p)) for p in paths]\n        # x: I0, y: [I90, I180, I270]\n        x = imgs[0].unsqueeze(0)                 # (1,H,W)\n        y = torch.stack([imgs[1], imgs[2], imgs[3]], dim=0)  # (3,H,W)\n        return x, y\n\nclass Holo3PhaseDataset(Dataset):\n    def __init__(self, root: str, split: str, degree_to_index: Dict[int,int]):\n        self.base = HologramDataset(root, split)\n        self.d2i = dict(degree_to_index)\n        assert self.d2i == {0:0, 90:1, 180:2, 270:3}, \
            f"degree_to_index must be {{0:0,90:1,180:2,270:3}}, got {self.d2i}"\n    def __len__(self): return len(self.base)\n    def __getitem__(self, idx):\n        x, y = self.base[idx]\n        # x: (1,H,W), y: (3,H,W) for (90,180,270) already\n        return x.float(), y.float()\n\n# --------------------\n# Model (UNet-ish, UpSample+Conv, 3ch output = (A,C,S))\n# --------------------\nclass ConvBlock(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_c, out_c, 3, 1, 1, bias=False),\n            nn.GroupNorm(1, out_c),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False),\n            nn.GroupNorm(1, out_c),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n    def forward(self, x): return self.block(x)\n\nclass Down(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_c, out_c, 3, 2, 1, bias=False),\n            nn.GroupNorm(1, out_c),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n    def forward(self, x): return self.block(x)\n\nclass Up(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        self.conv = ConvBlock(in_c, out_c)\n    def forward(self, x, skip):\n        x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, skip], dim=1)\n        return self.conv(x)\n\nclass FINCH_ACS_UNet(nn.Module):\n    def __init__(self, in_channels=1):\n        super().__init__()\n        c1, c2, c3, c4 = 64, 128, 256, 512\n        self.stem = ConvBlock(in_channels, c1)\n        self.d1 = Down(c1, c2)\n        self.d2 = Down(c2, c3)\n        self.d3 = Down(c3, c4)\n        self.u1 = Up(c4 + c3, c3)\n        self.u2 = Up(c3 + c2, c2)\n        self.u3 = Up(c2 + c1, c1)\n        self.head = nn.Conv2d(c1, 3, 3, 1, 1)  # → (A,C,S)\n        self.softplus = nn.Softplus(beta=1.0)  # A ≥ 0\n\n    def forward(self, x):\n        s0 = self.stem(x)         # H\n        s1 = self.d1(s0)          # H/2\n        s2 = self.d2(s1)          # H/4\n        s3 = self.d3(s2)          # H/8\n        u1 = self.u1(s3, s2)\n        u2 = self.u2(u1, s1)\n        u3 = self.u3(u2, s0)\n        acs = self.head(u3)\n        A = self.softplus(acs[:, 0:1])\n        C = acs[:, 1:2]\n        S = acs[:, 2:3]\n        return A, C, S\n\n# --------------------\n# Physics: synthesize intensity at given phase in radians\n# --------------------\n@torch.no_grad()\ndef phase_rads(deg: float) -> float:\n    return float(deg * math.pi / 180.0)\n\ndef synth_intensity(A, C, S, delta_rad: float):\n    # I_δ = A + C cosδ − S sinδ\n    return A + C * math.cos(delta_rad) - S * math.sin(delta_rad)\n\n# --------------------\n# Loss & Metrics (RMSE per channel)\n# --------------------\nclass RMSEThree(nn.Module):\n    def __init__(self): super().__init__()\n    def forward(self, pred3: torch.Tensor, tgt3: torch.Tensor):\n        # pred3,tgt3: (B,3,H,W) for (90,180,270)\n        assert pred3.shape == tgt3.shape\n        rmses = []\n        for c in range(3):\n            mse = F.mse_loss(pred3[:, c], tgt3[:, c], reduction='mean')\n            rmses.append(torch.sqrt(mse + 1e-8))\n        loss = torch.stack(rmses).sum()\n        return loss, {f"rmse_ch{c}": float(rmses[c].detach().cpu()) for c in range(3)} | {"rmse_sum": float(loss.detach().cpu())}\n\n# --------------------\n# Train / Val\n# --------------------\n@dataclass\nclass TrainConfig:\n    dataset_root: str = "./hologram_dataset_images_clean"\n    degree_to_index: Dict[int,int] = None\n    batch_size: int = 8\n    epochs: int = 120\n    lr: float = 1e-3\n    weight_decay: float = 1e-4\n    max_grad_norm: float = 1.0\n    num_workers: int = 4\n    amp: bool = True\n    seed: int = 42\n    patience: int = 15\n    pct_start: float = 0.1\n    lambda_I0_consistency: float = 0.0   # 0.0이면 끔 (공정 비교용 기본값)\n    save_path: str = "finch_acs_best.pth"\n\ndef make_loaders(cfg: TrainConfig):\n    if cfg.degree_to_index is None:\n        cfg.degree_to_index = {0:0, 90:1, 180:2, 270:3}\n    train_ds = Holo3PhaseDataset(cfg.dataset_root, "train", cfg.degree_to_index)\n    val_ds   = Holo3PhaseDataset(cfg.dataset_root, "validation", cfg.degree_to_index)\n    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n                              num_workers=cfg.num_workers, drop_last=True, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n                            num_workers=cfg.num_workers, drop_last=False, pin_memory=True)\n    return train_loader, val_loader\n\n@torch.no_grad()\ndef validate(model: nn.Module, loader: DataLoader, device: torch.device, amp: bool = True) -> float:\n    model.eval()\n    metric = RMSEThree()\n    d90, d180, d270 = map(phase_rads, (90, 180, 270))\n    total, n = 0.0, 0\n    for x, y in loader:  # x: (B,1,H,W), y: (B,3,H,W)\n        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n        with torch.amp.autocast('cuda', enabled=amp):\n            A, C, S = model(x)\n            I90  = synth_intensity(A, C, S, d90)\n            I180 = synth_intensity(A, C, S, d180)\n            I270 = synth_intensity(A, C, S, d270)\n            pred = torch.cat([I90, I180, I270], dim=1)\n            loss, _ = metric(pred, y)\n        total += loss.item() * x.size(0)\n        n += x.size(0)\n    return total / max(1, n)\n\ndef train(cfg: TrainConfig):\n    set_seed(cfg.seed)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f"[Device] {device}")\n    train_loader, val_loader = make_loaders(cfg)\n    steps = len(train_loader)\n    print(f"[Data] steps/epoch={steps}  batch={cfg.batch_size}")\n\n    model = FINCH_ACS_UNet(in_channels=1).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(0.9, 0.99))\n    if steps > 0:\n        sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=cfg.lr, epochs=cfg.epochs, steps_per_epoch=steps,\n                                                    pct_start=cfg.pct_start, div_factor=25.0, final_div_factor=100.0,\n                                                    anneal_strategy='cos')\n    else:\n        sched = None\n    scaler = torch.amp.GradScaler('cuda', enabled=cfg.amp)\n    metric = RMSEThree()\n    d0, d90, d180, d270 = map(phase_rads, (0, 90, 180, 270))\n\n    best_val, no_imp = float('inf'), 0\n\n    for ep in range(1, cfg.epochs + 1):\n        model.train()\n        run = 0.0\n        for bi, (x, y) in enumerate(train_loader, 1):\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            opt.zero_grad(set_to_none=True)\n            with torch.amp.autocast('cuda', enabled=cfg.amp):\n                A, C, S = model(x)\n                I90  = synth_intensity(A, C, S, d90)\n                I180 = synth_intensity(A, C, S, d180)\n                I270 = synth_intensity(A, C, S, d270)\n                pred = torch.cat([I90, I180, I270], dim=1)\n                loss, det = metric(pred, y)  # pure RMSE (논문과 동일 지표)\n                # Optional: I0 self-consistency (off by default)\n                if cfg.lambda_I0_consistency > 0.0:\n                    I0 = synth_intensity(A, C, S, d0)\n                    mse0 = F.mse_loss(I0, x, reduction='mean')\n                    rmse0 = torch.sqrt(mse0 + 1e-8)\n                    loss = loss + cfg.lambda_I0_consistency * rmse0\n\n            scaler.scale(loss).backward()\n            if cfg.max_grad_norm and cfg.max_grad_norm > 0:\n                scaler.unscale_(opt)\n                nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n            scaler.step(opt); scaler.update()\n            if sched is not None: sched.step()\n            run += loss.item()\n\n            if bi % 1 == 0:\n                lr = opt.param_groups[0]['lr']\n                print(f"   [Ep {ep:03d} / {cfg.epochs} | {bi:04d}/{steps}] loss={loss.item():.6f} rmse_sum={det['rmse_sum']:.6f} lr={lr:.6f}")\n\n        avg_train = run / max(1, steps)\n        val = validate(model, val_loader, device, cfg.amp)\n        print(f"\n✅ Epoch {ep:03d} | train={avg_train:.6f}  val={val:.6f}  lr={opt.param_groups[0]['lr']:.6f}")\n\n        if val < best_val - 1e-6:\n            best_val, no_imp = val, 0\n            torch.save(model.state_dict(), cfg.save_path)\n            print(f"   🏆 NEW BEST saved → {cfg.save_path}")\n        else:\n            no_imp += 1\n            print(f"   ⏳ no improve {no_imp}/{cfg.patience}")\n            if no_imp >= cfg.patience:\n                print("\n🛑 Early stop: patience exceeded")\n                break\n\n    print(f"\n🎉 Done. Best val RMSE-sum: {best_val:.6f}")\n\nif __name__ == "__main__":\n    cfg = TrainConfig(\n        dataset_root = "./hologram_dataset_images_clean",\n        degree_to_index = {0:0, 90:1, 180:2, 270:3},\n        batch_size = 8,\n        epochs = 120,\n        lr = 1e-3,\n        weight_decay = 1e-4,\n        max_grad_norm = 1.0,\n        num_workers = 4,\n        amp = True,\n        seed = 42,\n        patience = 15,\n        pct_start = 0.1,\n        lambda_I0_consistency = 0.0,  # 공정비교용 기본값: 오직 RMSE로만 학습\n        save_path = "finch_acs_best.pth",\n    )\n    print(cfg)\n    train(cfg)\n
