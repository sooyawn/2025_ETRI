import cv2
import numpy as np
import time
import torch
from ultralytics import YOLO
import mediapipe as mp
from collections import deque
import threading
from queue import Queue
import copy
from system_params import FRAME_WIDTH, FRAME_HEIGHT, FPS, CAMERA_MATRIX, DIST_COEFFS, CAMERA_ANGLES

# GPU ê°€ì† ì„¤ì •
if torch.cuda.is_available():
    device = torch.device('cuda')
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    print(f"Using GPU: {torch.cuda.get_device_name()}")
else:
    device = torch.device('cpu')
    print("Using CPU")

# YOLO ëª¨ë¸ ë¡œë“œ (GPU ê°€ì†)
model = YOLO("yolov8n-face.pt")  # ì–¼êµ´ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©
model.to(device)

# MediaPipe FaceMesh ì„¤ì • (ì„±ëŠ¥ í•œê³„ í…ŒìŠ¤íŠ¸: 5ëª…ê¹Œì§€ ëœë“œë§ˆí¬ ì¶”ì¶œ)
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False, 
    max_num_faces=5,  # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 5ëª…ìœ¼ë¡œ ì¦ê°€
    refine_landmarks=True,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

# 3D ì–¼êµ´ ëª¨ë¸ í¬ì¸íŠ¸
face_3d_model_points = np.array([
    [0.0, 0.0, 0.0], [0.0, -330.0, -65.0],
    [-225.0, 170.0, -135.0], [225.0, 170.0, -135.0],
    [-150.0, -150.0, -125.0], [150.0, -150.0, -125.0]
], dtype=np.float32)

landmark_ids = [1, 152, 33, 263, 61, 291]

# ğŸ”¥ ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ IDë“¤ (ë” ì •í™•í•œ ë™ê³µ ì¶”ì •ì„ ìœ„í•´)
left_eye_landmarks = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]
right_eye_landmarks = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]

# í™ì±„ ì¤‘ì‹¬ì  ID
iris_ids = [468, 473]  # ì™¼ìª½, ì˜¤ë¥¸ìª½ í™ì±„ ì¤‘ì‹¬

# ğŸ”¥ ì•ˆì •í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬ ë²„í¼ (ê° ì¹´ë©”ë¼ë³„, ê° ì‚¬ëŒë³„)
class GazeStabilizer:
    def __init__(self, buffer_size=5, outlier_threshold=0.1):
        self.buffer_size = buffer_size
        self.outlier_threshold = outlier_threshold
        self.gaze_history = deque(maxlen=buffer_size)
        self.iris_history = deque(maxlen=buffer_size)
        
    def add_sample(self, gaze, iris_coords):
        """ìƒˆë¡œìš´ ìƒ˜í”Œ ì¶”ê°€"""
        self.gaze_history.append(gaze)
        self.iris_history.append(iris_coords)
    
    def get_stabilized_gaze(self):
        """ì•ˆì •í™”ëœ ì‹œì„  ì¢Œí‘œ ë°˜í™˜"""
        if len(self.gaze_history) < 2:
            return self.gaze_history[-1] if self.gaze_history else (0, 0)
        
        # ì•„ì›ƒë¼ì´ì–´ ì œê±°
        valid_gazes = []
        for gaze in self.gaze_history:
            if self._is_valid_gaze(gaze):
                valid_gazes.append(gaze)
        
        if not valid_gazes:
            return self.gaze_history[-1]
        
        # ê°€ì¤‘ í‰ê·  (ìµœê·¼ ê°’ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        weights = np.linspace(0.5, 1.0, len(valid_gazes))
        weights = weights / np.sum(weights)
        
        avg_x = np.average([g[0] for g in valid_gazes], weights=weights)
        avg_y = np.average([g[1] for g in valid_gazes], weights=weights)
        
        return (avg_x, avg_y)
    
    def _is_valid_gaze(self, gaze):
        """ì•„ì›ƒë¼ì´ì–´ ê²€ì¶œ"""
        if len(self.gaze_history) < 2:
            return True
        
        recent_gazes = list(self.gaze_history)[-3:]  # ìµœê·¼ 3ê°œ ìƒ˜í”Œ
        avg_x = np.mean([g[0] for g in recent_gazes])
        avg_y = np.mean([g[1] for g in recent_gazes])
        
        distance = np.sqrt((gaze[0] - avg_x)**2 + (gaze[1] - avg_y)**2)
        return distance < self.outlier_threshold

# ğŸ”¥ ë©€í‹°ìŠ¤ë ˆë”© ì²˜ë¦¬ í´ë˜ìŠ¤
class ThreadedProcessor:
    def __init__(self):
        self.yolo_queue = Queue(maxsize=4)  # í í¬ê¸° ì¦ê°€
        self.mediapipe_queue = Queue(maxsize=4)  # í í¬ê¸° ì¦ê°€  
        self.result_queue = Queue(maxsize=4)  # í í¬ê¸° ì¦ê°€
        self.running = True
        
        # YOLO ì²˜ë¦¬ ìŠ¤ë ˆë“œ
        self.yolo_thread = threading.Thread(target=self._yolo_worker, daemon=True)
        self.yolo_thread.start()
        
        # MediaPipe ì²˜ë¦¬ ìŠ¤ë ˆë“œ
        self.mediapipe_thread = threading.Thread(target=self._mediapipe_worker, daemon=True)
        self.mediapipe_thread.start()
    
    def _yolo_worker(self):
        """YOLO ì–¼êµ´ ê²€ì¶œ ì „ìš© ìŠ¤ë ˆë“œ"""
        while self.running:
            try:
                frame, camera_id = self.yolo_queue.get(timeout=0.1)
                yolo_results = model.predict(frame, verbose=False)[0]
                face_boxes = [
                    box for box in yolo_results.boxes.data.cpu().numpy()
                    if int(box[5]) == 0
                ]
                self.mediapipe_queue.put((frame, face_boxes, camera_id))
                self.yolo_queue.task_done()
            except:
                continue
    
    def _mediapipe_worker(self):
        """MediaPipe ëœë“œë§ˆí¬ ì²˜ë¦¬ ì „ìš© ìŠ¤ë ˆë“œ"""
        while self.running:
            try:
                frame, face_boxes, camera_id = self.mediapipe_queue.get(timeout=0.1)
                frame_processed, results = self._process_mediapipe(frame, face_boxes, camera_id)
                self.result_queue.put((frame_processed, results, camera_id))
                self.mediapipe_queue.task_done()
            except:
                continue
    
    def _process_mediapipe(self, frame, face_boxes, camera_id):
        """MediaPipe ëœë“œë§ˆí¬ ì²˜ë¦¬ (ê¸°ì¡´ process_faces_yolo_mediapipe ë¡œì§ ì‚¬ìš©)"""
        results = []
        h, w = frame.shape[:2]
        
        for face_idx, box in enumerate(face_boxes):
            if face_idx >= 5:  # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: ìµœëŒ€ 5ëª…ê¹Œì§€ ì²˜ë¦¬
                break
                
            x1, y1, x2, y2, conf, cls = box.astype(int)
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w, x2), min(h, y2)

            face_roi = frame[y1:y2, x1:x2]
            face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)
            mesh_result = face_mesh.process(face_rgb)

            if mesh_result.multi_face_landmarks:
                for face_landmarks in mesh_result.multi_face_landmarks:
                    # ê¸°ë³¸ ëœë“œë§ˆí¬ í¬ì¸íŠ¸ë“¤
                    image_points = []
                    for idx in landmark_ids:
                        lm = face_landmarks.landmark[idx]
                        x_lm = int(lm.x * (x2 - x1)) + x1
                        y_lm = int(lm.y * (y2 - y1)) + y1
                        image_points.append((x_lm, y_lm))
                        cv2.circle(frame, (x_lm, y_lm), 2, (0, 0, 255), -1)

                    # ğŸ”¥ ê°œì„ ëœ ë™ê³µ ì¶”ì •: í™ì±„ ì¤‘ì‹¬ + ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ í‰ê· 
                    iris_coords = []
                    eye_centers = []
                    
                    # í™ì±„ ì¤‘ì‹¬ì  ì¶”ì¶œ
                    for idx in iris_ids:
                        if idx < len(face_landmarks.landmark):
                            lm = face_landmarks.landmark[idx]
                            x_lm = int(lm.x * (x2 - x1)) + x1
                            y_lm = int(lm.y * (y2 - y1)) + y1
                            iris_coords.append((x_lm, y_lm))
                            cv2.circle(frame, (x_lm, y_lm), 3, (0, 255, 0), -1)
                    
                    # ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ ê¸°ë°˜ ëˆˆ ì¤‘ì‹¬ ê³„ì‚°
                    roi_info = (x1, y1, x2-x1, y2-y1)
                    
                    left_eye_center = get_eye_center_from_landmarks(face_landmarks, left_eye_landmarks, roi_info)
                    right_eye_center = get_eye_center_from_landmarks(face_landmarks, right_eye_landmarks, roi_info)
                    
                    if left_eye_center:
                        eye_centers.append(left_eye_center)
                    if right_eye_center:
                        eye_centers.append(right_eye_center)

                    # 3D í¬ì¦ˆ ì¶”ì •
                    rot = (0, 0, 0)
                    pos = (0, 0, 0)
                    if len(image_points) == 6:
                        image_points_np = np.array(image_points, dtype=np.float32)
                        success, rotation_vector, translation_vector = cv2.solvePnP(
                            face_3d_model_points, image_points_np, CAMERA_MATRIX, DIST_COEFFS)
                        if success:
                            rot = rotation_vector.flatten()
                            pos = translation_vector.flatten()

                    # ğŸ”¥ ê°œì„ ëœ ì‹œì„  ì¶”ì •: ë‹¤ì¤‘ ë°©ë²• ê²°í•©
                    gaze = (0, 0)
                    
                    # ë°©ë²• 1: í™ì±„ ì¤‘ì‹¬ ê¸°ë°˜
                    if len(iris_coords) == 2:
                        lx, ly = iris_coords[0]
                        rx, ry = iris_coords[1]
                        iris_gaze = ((rx + lx) / 2 / w - 0.5, (ry + ly) / 2 / h - 0.5)
                    else:
                        iris_gaze = None
                    
                    # ë°©ë²• 2: ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ ì¤‘ì‹¬ ê¸°ë°˜
                    if len(eye_centers) == 2:
                        lx, ly = eye_centers[0]
                        rx, ry = eye_centers[1]
                        landmark_gaze = ((rx + lx) / 2 / w - 0.5, (ry + ly) / 2 / h - 0.5)
                    else:
                        landmark_gaze = None
                    
                    # ğŸ”¥ ë‘ ë°©ë²•ì˜ ê°€ì¤‘ í‰ê·  (í™ì±„ê°€ ë” ì •í™•í•˜ë¯€ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜)
                    if iris_gaze and landmark_gaze:
                        gaze = (
                            0.7 * iris_gaze[0] + 0.3 * landmark_gaze[0],
                            0.7 * iris_gaze[1] + 0.3 * landmark_gaze[1]
                        )
                    elif iris_gaze:
                        gaze = iris_gaze
                    elif landmark_gaze:
                        gaze = landmark_gaze

                    # ğŸ”¥ ì‹œê°„ì  ì•ˆì •í™” ì ìš©
                    if camera_id in stabilizers and face_idx < len(stabilizers[camera_id]):
                        stabilizer = stabilizers[camera_id][face_idx]
                        stabilizer.add_sample(gaze, iris_coords)
                        stable_gaze = stabilizer.get_stabilized_gaze()
                        gaze = stable_gaze

                    results.append({"pos": pos, "rot": rot, "gaze": gaze})

        return frame, results
    
    def process_frame(self, frame, camera_id):
        """í”„ë ˆì„ ì²˜ë¦¬ ìš”ì²­"""
        try:
            self.yolo_queue.put_nowait((frame.copy(), camera_id))
        except:
            # Queueê°€ ê°€ë“ì°¬ ê²½ìš° ì´ì „ í•­ëª© ì œê±° í›„ ìƒˆë¡œìš´ ê²ƒ ì¶”ê°€
            try:
                self.yolo_queue.get_nowait()
                self.yolo_queue.task_done()
                self.yolo_queue.put_nowait((frame.copy(), camera_id))
            except:
                pass
    
    def get_result(self):
        """ì²˜ë¦¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.result_queue.get_nowait()
        except:
            return None
    
    def stop(self):
        """ìŠ¤ë ˆë“œ ì •ë¦¬"""
        self.running = False

# ê° ì¹´ë©”ë¼ë³„ ì•ˆì •í™” ê°ì²´ (ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: ìµœëŒ€ 5ëª…ê¹Œì§€)
stabilizers = {
    0: [GazeStabilizer() for _ in range(5)],  # ì¹´ë©”ë¼ 0: 5ëª…
    1: [GazeStabilizer() for _ in range(5)]   # ì¹´ë©”ë¼ 1: 5ëª…
}

# ë©€í‹°ìŠ¤ë ˆë”© í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
processor = ThreadedProcessor()

# ì¹´ë©”ë¼ ì´ˆê¸°í™” (DirectShow ë°±ì—”ë“œë¡œ ìµœì í™”)
cap0 = cv2.VideoCapture(0, cv2.CAP_DSHOW)
cap1 = cv2.VideoCapture(1, cv2.CAP_DSHOW)

# ì¹´ë©”ë¼ 1ì´ ì‹¤íŒ¨í•˜ë©´ ì¹´ë©”ë¼ 0ìœ¼ë¡œ ëŒ€ì²´
if not cap1.isOpened():
    cap1 = cv2.VideoCapture(0, cv2.CAP_DSHOW)

for cap in [cap0, cap1]:
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)  
    cap.set(cv2.CAP_PROP_FPS, FPS)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

prev_frame_time = time.time()

def rotate_frame(frame, angle_rad):
    if angle_rad == 0:
        return frame
    angle_deg = np.rad2deg(angle_rad)
    h, w = frame.shape[:2]
    center = (w//2, h//2)
    rot_mat = cv2.getRotationMatrix2D(center, angle_deg, 1.0)
    return cv2.warpAffine(frame, rot_mat, (w, h))

def get_eye_center_from_landmarks(face_landmarks, eye_landmark_ids, roi_offset):
    """ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ë“¤ì˜ í‰ê· ìœ¼ë¡œ ëˆˆ ì¤‘ì‹¬ ê³„ì‚°"""
    eye_points = []
    x1, y1, roi_width, roi_height = roi_offset
    
    for idx in eye_landmark_ids:
        if idx < len(face_landmarks.landmark):
            lm = face_landmarks.landmark[idx]
            x_lm = int(lm.x * roi_width) + x1
            y_lm = int(lm.y * roi_height) + y1
            eye_points.append((x_lm, y_lm))
    
    if eye_points:
        # ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ë“¤ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
        center_x = int(np.mean([p[0] for p in eye_points]))
        center_y = int(np.mean([p[1] for p in eye_points]))
        return (center_x, center_y)
    return None

# ê¸°ì¡´ process_faces_yolo_mediapipe í•¨ìˆ˜ëŠ” ThreadedProcessor í´ë˜ìŠ¤ ë‚´ë¶€ë¡œ ì´ë™ë¨

def draw_info(frame, data, fps, camera_id):
    # ì¹´ë©”ë¼ ìƒíƒœ í‘œì‹œ
    status_color = (0, 255, 0) if len(data) > 0 else (0, 0, 255)  # ì´ˆë¡ìƒ‰: ì¶”ì  ì¤‘, ë¹¨ê°„ìƒ‰: ì¶”ì  ì•ˆë¨
    cv2.putText(frame, f"Tracking: {'ON' if len(data) > 0 else 'OFF'}", (10, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 2)
    
    for i, d in enumerate(data):
        y_offset = 40 + i * 120  # ìœ„ì¹˜ ì¡°ì •
        cv2.putText(frame, f"Person {i+1}", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1)
        cv2.putText(frame, f"Pos: {np.round(d['pos'],1)}", (10, y_offset+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
        cv2.putText(frame, f"Rot: {np.round(d['rot'],2)}", (10, y_offset+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
        cv2.putText(frame, f"Gaze: ({d['gaze'][0]:.3f}, {d['gaze'][1]:.3f})", (10, y_offset+60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1)
        cv2.putText(frame, f"Multi-threaded", (10, y_offset+80), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,255,255), 1)
    
    cv2.putText(frame, f"FPS: {fps:.1f}", (10, frame.shape[0]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1)

# ì´ì „ í”„ë ˆì„ ë°ì´í„° ì €ì¥ (ë©€í‹°ìŠ¤ë ˆë”© ëŒ€ê¸°ìš©)
last_frame_data = {0: [], 1: []}


try:
    while True: #ë©”ì¸ ë£¨í”„ ì‹œì‘ì 
        ret0, frame0 = cap0.read()
        ret1, frame1 = cap1.read()
        
        if not ret0:
            break
        if not ret1:
            frame1 = frame0.copy()

        frame0 = rotate_frame(frame0, CAMERA_ANGLES[0])
        frame1 = rotate_frame(frame1, CAMERA_ANGLES[1])

        # ğŸ”¥ ë©€í‹°ìŠ¤ë ˆë”© ì²˜ë¦¬: YOLOì™€ MediaPipe ë³‘ë ¬ ì‹¤í–‰
        processor.process_frame(frame0, 0)
        processor.process_frame(frame1, 1)

        # ê²°ê³¼ ìˆ˜ì§‘
        data0 = last_frame_data[0]
        data1 = last_frame_data[1]
        
        # ì²˜ë¦¬ ì™„ë£Œëœ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ë‘ ì¹´ë©”ë¼ ëª¨ë‘ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°)
        processed_cameras = set()
        max_attempts = 20  # ìµœëŒ€ 20íšŒ ì‹œë„ -> ìµœëŒ€ 20ms ëŒ€ê¸°
        
        for attempt in range(max_attempts):
            result = processor.get_result()
            if result is None:
                time.sleep(0.001)  # 1ms ëŒ€ê¸°
                continue
                
            frame_processed, results, camera_id = result
            processed_cameras.add(camera_id)
            
            last_frame_data[camera_id] = results
            if camera_id == 0:
                frame0 = frame_processed
                data0 = results
            else:
                frame1 = frame_processed
                data1 = results
            
            # ë‘ ì¹´ë©”ë¼ ëª¨ë‘ ì²˜ë¦¬ë˜ë©´ ì¢…ë£Œ
            if len(processed_cameras) >= 2:
                break

        # FPS ê³„ì‚°
        now = time.time()
        fps = 1.0 / (now - prev_frame_time)
        prev_frame_time = now

        # ì •ë³´ í‘œì‹œ
        draw_info(frame0, data0, fps, 0)
        draw_info(frame1, data1, fps, 1)

        cv2.putText(frame0, "Camera 0 (Left)", (10, frame0.shape[0]-30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
        cv2.putText(frame1, "Camera 1 (Right)", (10, frame1.shape[0]-30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)

        combined = np.hstack((frame0, frame1))
        cv2.imshow("ğŸ”¥ Multi-threaded Gaze Tracking (GPU + Parallel Processing)", combined)

        if cv2.waitKey(1) & 0xFF == 27:
            break

except KeyboardInterrupt:
    pass

finally:
    processor.stop() 
    cap0.release()
    cap1.release()
    cv2.destroyAllWindows()
