import os
import json
import cv2
import numpy as np
from datetime import datetime
import time
from typing import Tuple, Optional
import torch
from ultralytics import YOLO
import mediapipe as mp
from collections import deque
import threading
from queue import Queue

# ========================================
# í•´ìƒë„ ì„¤ì • (Camera_1.pyì™€ ë™ì¼)
# ========================================
DEFAULT_WIDTH = 1920          # ê¸°ë³¸ í•´ìƒë„ ë„ˆë¹„
DEFAULT_HEIGHT = 1080         # ê¸°ë³¸ í•´ìƒë„ ë†’ì´
DEFAULT_FPS = 60              # ê¸°ë³¸ FPS

# ========================================
# ì¹´ë©”ë¼ ì¸ë±ìŠ¤ ì„¤ì • (Camera_1.pyì™€ ë™ì¼)
# ========================================
LEFT_CAMERA_INDEX = 2         # ì™¼ìª½ ì¹´ë©”ë¼ (Camera_1.py: cap_L = open_cam(2, selected_res))
RIGHT_CAMERA_INDEX = 0        # ì˜¤ë¥¸ìª½ ì¹´ë©”ë¼ (Camera_1.py: cap_R = open_cam(0, selected_res))
# ========================================

# ========================================
# ë””ìŠ¤í”Œë ˆì´ ìŠ¤ì¼€ì¼ ì„¤ì •
# ========================================
DISPLAY_SCALE = 0.5          # í™”ë©´ í‘œì‹œìš© ìŠ¤ì¼€ì¼ (0.5 = 50% í¬ê¸°, 1.0 = 100% í¬ê¸°)

# ========================================
# ì•„ì´íŠ¸ë˜í‚¹ ì„¤ì •
# ========================================
GAZE_TRACKING_CONFIG = {
    'max_faces': 3,  # ìµœëŒ€ ì¶”ì í•  ì–¼êµ´ ìˆ˜
    'detection_confidence': 0.7,
    'tracking_confidence': 0.7,
    'stabilizer_buffer_size': 5,
    'outlier_threshold': 0.1
}

# ========================================

class GazeStabilizer:
    """ì‹œì„  ì¶”ì  ì•ˆì •í™” í´ë˜ìŠ¤"""
    
    def __init__(self, buffer_size=None, outlier_threshold=None):
        self.buffer_size = buffer_size or GAZE_TRACKING_CONFIG['stabilizer_buffer_size']
        self.outlier_threshold = outlier_threshold or GAZE_TRACKING_CONFIG['outlier_threshold']
        self.gaze_history = deque(maxlen=self.buffer_size)
        self.iris_history = deque(maxlen=self.buffer_size)
        
    def add_sample(self, gaze, iris_coords):
        """ìƒˆë¡œìš´ ìƒ˜í”Œ ì¶”ê°€"""
        self.gaze_history.append(gaze)
        self.iris_history.append(iris_coords)
    
    def get_stabilized_gaze(self):
        """ì•ˆì •í™”ëœ ì‹œì„  ì¢Œí‘œ ë°˜í™˜"""
        if len(self.gaze_history) < 2:
            return self.gaze_history[-1] if self.gaze_history else (0, 0)
        
        # ì•„ì›ƒë¼ì´ì–´ ì œê±°
        valid_gazes = []
        for gaze in self.gaze_history:
            if self._is_valid_gaze(gaze):
                valid_gazes.append(gaze)
        
        if not valid_gazes:
            return self.gaze_history[-1]
        
        # ê°€ì¤‘ í‰ê·  (ìµœê·¼ ê°’ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        weights = np.linspace(0.5, 1.0, len(valid_gazes))
        weights = weights / np.sum(weights)
        
        avg_x = np.average([g[0] for g in valid_gazes], weights=weights)
        avg_y = np.average([g[1] for g in valid_gazes], weights=weights)
        
        return (avg_x, avg_y)
    
    def _is_valid_gaze(self, gaze):
        """ì•„ì›ƒë¼ì´ì–´ ê²€ì¶œ"""
        if len(self.gaze_history) < 2:
            return True
        
        recent_gazes = list(self.gaze_history)[-3:]
        avg_x = np.mean([g[0] for g in recent_gazes])
        avg_y = np.mean([g[1] for g in recent_gazes])
        
        distance = np.sqrt((gaze[0] - avg_x)**2 + (gaze[1] - avg_y)**2)
        return distance < self.outlier_threshold


class UltraFastVideoStitcherNoVisual:
    """ì´ˆê³ ì† ìµœì í™”ëœ ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ìŠ¤í‹°ì²˜ (FPS í–¥ìƒ ë²„ì „)"""
    
    def __init__(self, calibration_config_path: str):
        self.config_path = calibration_config_path
        
        # FPS ì¸¡ì •ìš© ë³€ìˆ˜ë“¤
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0.0
        
        # ğŸš€ ìµœì í™”: í•œ ë²ˆë§Œ ëª¨ë“  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë“œ
        self._load_all_calibration_data_once()
        
        # ğŸš€ ìµœì í™”: ëª¨ë“  ì‚¬ì „ ê³„ì‚° ì ìš©
        self._apply_ultra_optimizations()
        
        # ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™”
        self._initialize_gaze_tracking()
        
        # ğŸš€ ë©€í‹°ìŠ¤ë ˆë”© ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™”
        self._initialize_threaded_gaze_tracking()
        
        # ì¹´ë©”ë¼ ê°ì²´ ì´ˆê¸°í™”
        self.cap_left = None
        self.cap_right = None
        
        # í”„ë ˆì„ ì¹´ìš´í„°
        self.frame_count = 0
        
        # ğŸš€ ì„±ëŠ¥ ì¸¡ì •ìš© ë³€ìˆ˜ë“¤
        self.step_times = {
            'frame_read': [],
            'mirror_flip': [],
            'lens_rectification': [],
            'canvas_copy': [],
            'homography_warp': [],
            'left_placement': [],
            'blending': [],
            'roi_crop': [],
            'display_resize': [],
            'total_stitching': []
        }
        self.performance_measurement = True  # ì„±ëŠ¥ ì¸¡ì • í™œì„±í™”
        print(f"âœ… ì„±ëŠ¥ ì¸¡ì • ì´ˆê¸°í™” ì™„ë£Œ: {len(self.step_times)} ë‹¨ê³„")

    def _load_all_calibration_data_once(self):
        """ğŸš€ ìµœì í™”: JSON íŒŒì¼ì„ í•œ ë²ˆë§Œ ë¡œë”©í•˜ì—¬ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬"""
        print("ğŸ”§ í•œ ë²ˆì— ëª¨ë“  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        config_dir = os.path.dirname(self.config_path)
        homography_file = os.path.join(config_dir, "homography_params.json")
        
        try:
            with open(homography_file, 'r') as f:
                self.homo_data = json.load(f)  # í•œ ë²ˆë§Œ ë¡œë”©!
            
            print("âœ… homography_params.json ë¡œë“œ ì™„ë£Œ")
            
            # ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
            self._process_homography_data()
            self._process_roi_data()  # ğŸ†• ROI ë°ì´í„° ì²˜ë¦¬ ì¶”ê°€
            self._process_blend_masks()  
            self._process_rectification_maps()
            
        except FileNotFoundError as e:
            print(f"homography_params.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("ë¨¼ì € Stitching_Engine.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”")
            raise
        except Exception as e:
            print(f"JSON ë¡œë“œ ì˜¤ë¥˜: {e}")
            raise

    def _process_homography_data(self):
        """í˜¸ëª¨ê·¸ë˜í”¼ ë°ì´í„° ì²˜ë¦¬"""
        data = self.homo_data
        self.homography_matrix = np.array(data['homography_matrix'])
        self.canvas_size = tuple(data['final_size'])
        self.left_offset = tuple(data['left_image_offset'])
        self.camera_resolution = data.get('camera_resolution', [1920, 1080])
        print(f"âœ… í˜¸ëª¨ê·¸ë˜í”¼ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {self.camera_resolution}")

    def _process_roi_data(self):
        """ğŸ†• ì‚¬ìš©ìê°€ ì„ íƒí•œ ROI ë°ì´í„° ì²˜ë¦¬"""
        print("ğŸ”§ ROI ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        # ROI ì •ë³´ íŒŒì¼ ê²½ë¡œ - ì˜¬ë°”ë¥¸ ê²½ë¡œ ê³„ì‚°
        config_dir = os.path.dirname(self.config_path)  # "./data/config"
        data_dir = os.path.dirname(config_dir)          # "./data"
        params_dir = os.path.join(data_dir, "params")   # "./data/params"
        roi_info_path = os.path.join(params_dir, "user_roi_info.json")
        roi_mask_path = os.path.join(params_dir, "user_roi_mask.npy")
        
        print(f"ğŸ” ROI íŒŒì¼ ê²½ë¡œ í™•ì¸:")
        print(f"   config_path: {self.config_path}")
        print(f"   config_dir: {config_dir}")
        print(f"   data_dir: {data_dir}")
        print(f"   params_dir: {params_dir}")
        print(f"   roi_info_path: {roi_info_path}")
        print(f"   roi_mask_path: {roi_mask_path}")
        
        # ROI ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(roi_info_path) and os.path.exists(roi_mask_path):
            try:
                # ROI ì •ë³´ ë¡œë“œ
                with open(roi_info_path, 'r') as f:
                    self.roi_info = json.load(f)
                
                # ROI ë§ˆìŠ¤í¬ ë¡œë“œ
                self.roi_mask = np.load(roi_mask_path).astype(np.uint8)
                
                # ROI ì¢Œí‘œ ì¶”ì¶œ (ì›ë³¸ í•´ìƒë„ ê¸°ì¤€)
                self.roi_x1 = self.roi_info['x1']
                self.roi_y1 = self.roi_info['y1']
                self.roi_x2 = self.roi_info['x2']
                self.roi_y2 = self.roi_info['y2']
                
                # ğŸš¨ ROI ì¢Œí‘œëŠ” ì´ë¯¸ ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€ì´ë¯€ë¡œ ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš”!
                # Stitching_Engineì—ì„œ ìƒì„±ëœ ROI ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                # (ì‚¬ìš©ìê°€ 0.5 ìŠ¤ì¼€ì¼ë¡œ ì„ íƒí–ˆì§€ë§Œ, ì¢Œí‘œëŠ” ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€)
                
                self.roi_width = self.roi_x2 - self.roi_x1
                self.roi_height = self.roi_y2 - self.roi_y1
                
                print(f"ğŸ”§ ROI ì¢Œí‘œ ìŠ¤ì¼€ì¼ ì¡°ì •:")
                print(f"   - ì›ë³¸ ROI: ({self.roi_info['x1']}, {self.roi_info['y1']}) -> ({self.roi_info['x2']}, {self.roi_info['y2']})")
                print(f"   - ìŠ¤ì¼€ì¼ ì¡°ì • í›„: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
                print(f"   - ROI í¬ê¸°: {self.roi_width} x {self.roi_height}")
                
                # ğŸš¨ ROI ì¢Œí‘œ ë²”ìœ„ ê²€ì¦ (ì›ë³¸ í•´ìƒë„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸)
                if hasattr(self, 'canvas_size'):
                    canvas_w, canvas_h = self.canvas_size[0], self.canvas_size[1]
                    if (self.roi_x1 < 0 or self.roi_y1 < 0 or 
                        self.roi_x2 > canvas_w or self.roi_y2 > canvas_h):
                        print(f"âš ï¸ ROI ì¢Œí‘œê°€ ì›ë³¸ í•´ìƒë„({canvas_w}x{canvas_h})ë¥¼ ë²—ì–´ë‚¨!")
                        print(f"   ROI ë²”ìœ„: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
                        # ì¢Œí‘œë¥¼ ì›ë³¸ í•´ìƒë„ ë‚´ë¡œ í´ë¨í”„
                        self.roi_x1 = max(0, min(self.roi_x1, canvas_w-1))
                        self.roi_y1 = max(0, min(self.roi_y1, canvas_h-1))
                        self.roi_x2 = max(self.roi_x1+1, min(self.roi_x2, canvas_w))
                        self.roi_y2 = max(self.roi_y1+1, min(self.roi_y2, canvas_h))
                        self.roi_width = self.roi_x2 - self.roi_x1
                        self.roi_height = self.roi_y2 - self.roi_y1
                        print(f"   í´ë¨í”„ í›„ ROI: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
                        print(f"   í´ë¨í”„ í›„ í¬ê¸°: {self.roi_width} x {self.roi_height}")
                
                self.use_roi = True
                
                # ğŸ†• ROIê°€ ìˆì„ ë•Œ ì›ë³¸ ìº”ë²„ìŠ¤ í¬ê¸° ë³´ì¡´ (ì „ì²´ ìŠ¤í‹°ì¹­ í›„ í¬ë¡­)
                if hasattr(self, 'canvas_size'):
                    self.original_canvas_size = self.canvas_size  # ì›ë³¸ í¬ê¸° ë³´ì¡´
                    print(f"âœ… ì›ë³¸ ìº”ë²„ìŠ¤ í¬ê¸° ë³´ì¡´: {self.canvas_size[0]}x{self.canvas_size[1]}")
                    print(f"   ROI í¬ê¸°: {self.roi_width}x{self.roi_height}")
                
                print(f"âœ… ì‚¬ìš©ì ì •ì˜ ROI ë¡œë“œ ì™„ë£Œ: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
                print(f"   ROI í¬ê¸°: {self.roi_width} x {self.roi_height}")
                
                # ğŸ†• ROI ë¸”ë Œë”© íŒŒë¼ë¯¸í„° ë¡œë“œ
                roi_blending_file = os.path.join(params_dir, 'roi_blending_params.json')
                if os.path.exists(roi_blending_file):
                    with open(roi_blending_file, 'r') as f:
                        self.roi_blending_params = json.load(f)
                    
                    # ROI ë¸”ë Œë”© ë§ˆìŠ¤í¬ ë¡œë“œ
                    roi_left_mask_file = os.path.join(params_dir, 'roi_left_blend_mask.npy')
                    roi_right_mask_file = os.path.join(params_dir, 'roi_right_blend_mask.npy')
                    
                    if os.path.exists(roi_left_mask_file) and os.path.exists(roi_right_mask_file):
                        self.roi_left_blend_mask = np.load(roi_left_mask_file)
                        self.roi_right_blend_mask = np.load(roi_right_mask_file)
                        print(f"âœ… ROI ë¸”ë Œë”© íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ:")
                        print(f"   - ë¸”ë Œë”© ë§ˆìŠ¤í¬: {self.roi_left_blend_mask.shape}")
                        print(f"   - ROI ì™¼ìª½ ì˜¤í”„ì…‹: {self.roi_blending_params['roi_left_offset']}")
                        print(f"   - ROI ì™¼ìª½ í¬ê¸°: {self.roi_blending_params['roi_left_size']}")
                    else:
                        print(f"âš ï¸ ROI ë¸”ë Œë”© ë§ˆìŠ¤í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        print(f"   ì°¾ëŠ” íŒŒì¼: {roi_left_mask_file}, {roi_right_mask_file}")
                else:
                    print(f"â„¹ï¸ ROI ë¸”ë Œë”© íŒŒë¼ë¯¸í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {roi_blending_file}")
                    print(f"   Stitching_Engine_3.pyì—ì„œ ROIë¥¼ ì„ íƒí•˜ì—¬ ë¸”ë Œë”© íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”")
                
            except Exception as e:
                print(f"âš ï¸ ROI ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.use_roi = False
        else:
            print("â„¹ï¸ ì‚¬ìš©ì ì •ì˜ ROIê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í™”ë©´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print(f"   íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:")
            print(f"     roi_info_path: {os.path.exists(roi_info_path)}")
            print(f"     roi_mask_path: {os.path.exists(roi_mask_path)}")
            self.use_roi = False

    def _process_blend_masks(self):
        """ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì²˜ë¦¬"""
        self.use_precomputed_blending = False
        
        blend_info = self.homo_data.get("blending_optimization", None)
        if blend_info is not None:
            data_dir = os.path.dirname(os.path.dirname(self.config_path))
            params_dir = os.path.join(data_dir, "params")
            left_mask_path = os.path.join(params_dir, blend_info["left_mask_file"])
            right_mask_path = os.path.join(params_dir, blend_info["right_mask_file"])
            
            if os.path.exists(left_mask_path) and os.path.exists(right_mask_path):
                # ì›ë³¸ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ë¡œë“œ (ROI ì¡°ì • ì „)
                self.left_blend_mask_original = np.load(left_mask_path).astype(np.float32)
                self.right_blend_mask_original = np.load(right_mask_path).astype(np.float32)
                self.use_precomputed_blending = True
                print(f"âœ… ì›ë³¸ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ë¡œë“œ ì™„ë£Œ: {self.left_blend_mask_original.shape}")
                
                # í•­ìƒ ì›ë³¸ ë§ˆìŠ¤í¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ROIëŠ” í‘œì‹œ ì‹œì—ë§Œ ì ìš©)
                self.left_blend_mask = self.left_blend_mask_original
                self.right_blend_mask = self.right_blend_mask_original
        else:
            print("â„¹ï¸ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def _process_rectification_maps(self):
        """ë Œì¦ˆ ì™œê³¡ ë³´ì • ë§µ ì²˜ë¦¬"""
        rect_maps = self.homo_data.get("rectification_maps", None)
        if rect_maps is None:
            raise ValueError("homography_params.jsonì— 'rectification_maps' ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        data_dir = os.path.dirname(os.path.dirname(self.config_path))
        params_dir = os.path.join(data_dir, "params")
        
        map_left_x_path = os.path.join(params_dir, rect_maps["map_left_x"])
        map_right_x_path = os.path.join(params_dir, rect_maps["map_right_x"])
        
        if not os.path.exists(map_left_x_path) or not os.path.exists(map_right_x_path):
            raise FileNotFoundError("ë Œì¦ˆ ì™œê³¡ ë³´ì • ë§µ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        self.map_left_x = np.load(map_left_x_path).astype(np.float32)
        self.map_right_x = np.load(map_right_x_path).astype(np.float32)
        
        # 2ì±„ë„ ë§µ ì²˜ë¦¬
        if len(self.map_left_x.shape) == 3 and self.map_left_x.shape[2] == 2:
            # 2ì±„ë„ ë§µì„ x, yë¡œ ë¶„ë¦¬
            self.map_left_y = self.map_left_x[:, :, 1].astype(np.float32)
            self.map_left_x = self.map_left_x[:, :, 0].astype(np.float32)
            self.map_right_y = self.map_right_x[:, :, 1].astype(np.float32)
            self.map_right_x = self.map_right_x[:, :, 0].astype(np.float32)
            print("âœ… 2ì±„ë„ ë Œì¦ˆ ì™œê³¡ ë³´ì • ë§µì„ x, yë¡œ ë¶„ë¦¬í•˜ì—¬ ë¡œë“œ ì™„ë£Œ")
        else:
            # 1ì±„ë„ ë§µ ì²˜ë¦¬
            map_left_y_path = os.path.join(params_dir, rect_maps["map_left_y"])
            map_right_y_path = os.path.join(params_dir, rect_maps["map_right_y"])
            self.map_left_y = np.load(map_left_y_path).astype(np.float32)
            self.map_right_y = np.load(map_right_y_path).astype(np.float32)
            print("âœ… 1ì±„ë„ ë Œì¦ˆ ì™œê³¡ ë³´ì • ë§µ ë¡œë“œ ì™„ë£Œ")
        
        print(f"   ì™¼ìª½ ë§µ: {self.map_left_x.shape}")
        print(f"   ì˜¤ë¥¸ìª½ ë§µ: {self.map_right_x.shape}")
        print(f"   ì™¼ìª½ map2: {self.map_left_y.shape}")
        print(f"   ì˜¤ë¥¸ìª½ map2: {self.map_right_y.shape}")

    def _apply_ultra_optimizations(self):
        """ğŸš€ ëª¨ë“  ì´ˆê³ ì† ìµœì í™”ë¥¼ í•œ ë²ˆì— ì ìš©"""
        print("ğŸš€ ì´ˆê³ ì† ìµœì í™” ì ìš© ì¤‘...")
        
        # ğŸš€ 1. ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ë¯¸ë¦¬ ìƒì„± (í•­ìƒ ì „ì²´ í¬ê¸°!)
        # ğŸš¨ í•­ìƒ ì „ì²´ í¬ê¸°ë¡œ ìƒì„± (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
        self.canvas_template = np.zeros((self.canvas_size[1], self.canvas_size[0], 3), dtype=np.uint8)
        print(f"âœ… ì „ì²´ í¬ê¸° ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ìƒì„±: {self.canvas_template.shape}")
        
        # ğŸš¨ í•­ìƒ ì¹´ë©”ë¼ í•´ìƒë„ ê¸°ì¤€ìœ¼ë¡œ ìŠ¬ë¼ì´ìŠ¤ ì¸ë±ìŠ¤ ì„¤ì •
        self.left_slice_y = slice(self.left_offset[1], self.left_offset[1] + self.camera_resolution[1])
        self.left_slice_x = slice(self.left_offset[0], self.left_offset[0] + self.camera_resolution[0])
        print(f"âœ… ì¹´ë©”ë¼ í•´ìƒë„ ê¸°ë°˜ ìŠ¬ë¼ì´ìŠ¤ ì¸ë±ìŠ¤ ê³„ì‚°: Y{self.left_slice_y}, X{self.left_slice_x}")
        
        # ğŸ†• ROIê°€ ìˆëŠ” ê²½ìš° ROI ìµœì í™” ê³„ì‚°
        if hasattr(self, 'use_roi') and self.use_roi:
            self._calculate_roi_optimizations()
        
        # ğŸš€ 3. ë Œì¦ˆ ë³´ì • í•¨ìˆ˜ ì‚¬ì „ ì„¤ì • (ì¡°ê±´ë¬¸ ì œê±°) - ë” í•˜ë“œì½”ë”©
        # lambda ëŒ€ì‹  ì§ì ‘ í•¨ìˆ˜ ì •ì˜ë¡œ ì„±ëŠ¥ í–¥ìƒ
        def rectify_left_hardcoded(img):
            return cv2.remap(img, self.map_left_x, self.map_left_y,
                           cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        
        def rectify_right_hardcoded(img):
            return cv2.remap(img, self.map_right_x, self.map_right_y,
                           cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        
        self.rectify_left = rectify_left_hardcoded
        self.rectify_right = rectify_right_hardcoded
        
        # ğŸš€ 4. ë¸”ë Œë”© ë§ˆìŠ¤í¬ ìµœì¢… í¬ê¸°ë¡œ ë¯¸ë¦¬ ë¦¬ì‚¬ì´ì¦ˆ (í•­ìƒ ì „ì²´ í¬ê¸°!)
        if hasattr(self, 'use_precomputed_blending') and self.use_precomputed_blending:
            # ğŸš¨ í•­ìƒ ì „ì²´ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
            self.left_mask_final = cv2.resize(
                self.left_blend_mask, 
                self.canvas_size, 
                interpolation=cv2.INTER_LINEAR
            ).astype(np.float32)
            
            self.right_mask_final = cv2.resize(
                self.right_blend_mask, 
                self.canvas_size, 
                interpolation=cv2.INTER_LINEAR
            ).astype(np.float32)
            
            print(f"âœ… ì „ì²´ í¬ê¸° ë¸”ë Œë”© ë§ˆìŠ¤í¬ ìƒì„±: {self.left_mask_final.shape}")
            
            # 3ì±„ë„ ë§ˆìŠ¤í¬ë„ ë¯¸ë¦¬ ìƒì„±
            self.left_mask_3ch = np.stack([self.left_mask_final] * 3, axis=-1)
            self.right_mask_3ch = np.stack([self.right_mask_final] * 3, axis=-1)
        else:
            # ğŸš€ ë¸”ë Œë”© ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
            self.left_mask_3ch = None
            self.right_mask_3ch = None
        
        # ğŸš€ 5. í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬ ìµœì í™” (ë” í•˜ë“œì½”ë”©)
        self.homography_matrix_opt = self.homography_matrix.astype(np.float32)  # float32ë¡œ ê³ ì •
        
        # ğŸš€ 6. ìº”ë²„ìŠ¤ í¬ê¸° í•˜ë“œì½”ë”© (í•­ìƒ ì „ì²´ í¬ê¸°!)
        # ğŸš¨ í•­ìƒ ì „ì²´ í¬ê¸° ì‚¬ìš© (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
        self.canvas_width = int(self.canvas_size[0])
        self.canvas_height = int(self.canvas_size[1])
        print(f"âœ… ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸° ì„¤ì •: {self.canvas_width}x{self.canvas_height}")
        
        # ğŸš€ 7. í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜ í¬ê¸° ë¯¸ë¦¬ ê³„ì‚° (ìºì‹œ!)
        self.homography_size = (self.canvas_width, self.canvas_height)
        print(f"âœ… í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜ í¬ê¸° ìºì‹œ: {self.homography_size}")
        
        # ğŸš€ 8. ROI ê¸°ë°˜ ì‚¬ì „ ê³„ì‚° ì¢Œí‘œ ë§µ ìƒì„± (ê·¹í•œ ìµœì í™”!)
        if hasattr(self, 'use_roi') and self.use_roi:
            self._precompute_roi_homography_coordinates()
        
        # ğŸš€ 9. ì™¼ìª½ ì˜¤í”„ì…‹ í•˜ë“œì½”ë”©
        self.left_offset_x = int(self.left_offset[0])
        self.left_offset_y = int(self.left_offset[1])
        
        # ğŸš€ 8. ì¹´ë©”ë¼ í•´ìƒë„ í•˜ë“œì½”ë”©
        self.camera_width = int(self.camera_resolution[0])
        self.camera_height = int(self.camera_resolution[1])

        # ğŸš« ë Œì¦ˆ ë³´ì • LUT ìƒì„± ì œê±° (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)
        # self._create_lens_correction_lut()
        
        # ğŸš« í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚° ì œê±° (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)
        # self._precompute_homography_coordinates()

        print("ğŸš€ ì´ˆê³ ì† ìµœì í™” ì™„ë£Œ! ì˜ˆìƒ FPS í–¥ìƒ: 50-100%")
        
        # ğŸš€ ìºì‹œ ìƒíƒœ í™•ì¸
        print(f"   ğŸ“Š ìºì‹œ ìƒíƒœ:")
        print(f"      - ìº”ë²„ìŠ¤ í…œí”Œë¦¿: {self.canvas_template.shape}")
        print(f"      - ìŠ¬ë¼ì´ìŠ¤ ì¸ë±ìŠ¤: Y{self.left_slice_y}, X{self.left_slice_x}")
        print(f"      - ë Œì¦ˆ ë³´ì • í•¨ìˆ˜: {self.rectify_left.__name__}, {self.rectify_right.__name__}")
        print(f"      - ë¸”ë Œë”© ë§ˆìŠ¤í¬: {'í™œì„±í™”' if hasattr(self, 'left_mask_3ch') and self.left_mask_3ch is not None else 'ë¹„í™œì„±í™”'}")
        print(f"      - í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬: {self.homography_matrix_opt.shape}")
        print(f"      - í•˜ë“œì½”ë”© í¬ê¸°: {self.canvas_width}x{self.canvas_height}")
        if hasattr(self, 'use_roi') and self.use_roi:
            print(f"      - ROI ëª¨ë“œ: í™œì„±í™” ({self.roi_width}x{self.roi_height})")
        else:
            print(f"      - ROI ëª¨ë“œ: ë¹„í™œì„±í™” (ì „ì²´ í™”ë©´)")
        print(f"      - ğŸš« ë Œì¦ˆ ë³´ì • LUT: ë¹„í™œì„±í™” (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)")
        print(f"      - ğŸš« í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ: ë¹„í™œì„±í™” (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)")

    def _calculate_roi_optimizations(self):
        """ğŸ†• ROI ì˜ì—­ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ í•˜ë“œì½”ë”© (í•œ ë²ˆë§Œ!)"""
        print("ğŸ”§ ROI ìµœì í™” ê³„ì‚° ì¤‘...")
        
        # ROI ì˜ì—­ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ í•˜ë“œì½”ë”© (ì „ì²´ ìŠ¤í‹°ì¹­ í›„ í¬ë¡­ìš©)
        self.roi_extract_y = slice(self.roi_y1, self.roi_y2)
        self.roi_extract_x = slice(self.roi_x1, self.roi_x2)
        
        print(f"âœ… ROI ìµœì í™” ê³„ì‚° ì™„ë£Œ:")
        print(f"   - ROI ì¶”ì¶œ ì˜ì—­: Y{self.roi_extract_y}, X{self.roi_extract_x}")
        print(f"   - í•˜ë“œì½”ë”© ì™„ë£Œ: ë§¤ í”„ë ˆì„ë§ˆë‹¤ ê³„ì‚°í•˜ì§€ ì•ŠìŒ!")

    def _create_lens_correction_lut(self):
        """ğŸš€ ë Œì¦ˆ ë³´ì • LUT ìƒì„±ìœ¼ë¡œ ë§¤ í”„ë ˆì„ ê³„ì‚° ì œê±°"""
        print("ğŸ”§ ë Œì¦ˆ ë³´ì • LUT ìƒì„± ì¤‘...")
        
        h, w = self.camera_resolution[1], self.camera_resolution[0]
        
        # ì¢Œí‘œ ê·¸ë¦¬ë“œ ìƒì„±
        y_coords, x_coords = np.mgrid[0:h, 0:w].astype(np.float32)
        
        # ë Œì¦ˆ ë³´ì • ë§µ ì ìš©í•˜ì—¬ LUT ìƒì„±
        self.left_lut_x = cv2.remap(x_coords, self.map_left_x, self.map_left_y, 
                                    cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        self.left_lut_y = cv2.remap(y_coords, self.map_left_x, self.map_left_y, 
                                    cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        
        self.right_lut_x = cv2.remap(x_coords, self.map_right_x, self.map_right_y, 
                                     cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        self.right_lut_y = cv2.remap(y_coords, self.map_right_x, self.map_right_y, 
                                     cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        
        print(f"âœ… ë Œì¦ˆ ë³´ì • LUT ìƒì„± ì™„ë£Œ: {self.left_lut_x.shape}")
        print(f"   ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 26-38ms â†’ 2-5ms (5-10ë°° í–¥ìƒ!)")

    def _precompute_homography_coordinates(self):
        """ğŸš€ í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚°ìœ¼ë¡œ ë§¤ í”„ë ˆì„ ë³€í™˜ ì œê±°"""
        print("ğŸ”§ í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚° ì¤‘...")
        
        # ğŸš¨ ì¤‘ìš”: ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°ë¡œ ì¢Œí‘œ ë§µ ìƒì„± (ì¹´ë©”ë¼ í•´ìƒë„ê°€ ì•„ë‹˜!)
        canvas_h, canvas_w = self.canvas_size[1], self.canvas_size[0]
        
        # ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€ ì¢Œí‘œ ê·¸ë¦¬ë“œ
        y_coords, x_coords = np.mgrid[0:canvas_h, 0:canvas_w].astype(np.float32)
        
        # í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬ë¡œ ë³€í™˜ëœ ì¢Œí‘œ ê³„ì‚° (í•œ ë²ˆë§Œ!)
        coords = np.stack([x_coords, y_coords, np.ones_like(x_coords)], axis=-1)
        transformed_coords = (self.homography_matrix_opt @ coords.reshape(-1, 3).T).T
        
        # 2D ì¢Œí‘œë¡œ ë³€í™˜
        transformed_coords = transformed_coords[:, :2] / transformed_coords[:, 2:]
        self.homography_coords = transformed_coords.reshape(canvas_h, canvas_w, 2)
        
        print(f"âœ… í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ìƒì„± ì™„ë£Œ: {self.homography_coords.shape}")
        print(f"   - ìº”ë²„ìŠ¤ í¬ê¸°: {canvas_w}x{canvas_h}")
        print(f"   - ì¹´ë©”ë¼ í•´ìƒë„: {self.camera_resolution[0]}x{self.camera_resolution[1]}")
        print(f"   ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 35-58ms â†’ 3-8ms (5-10ë°° í–¥ìƒ!)")

    def _precompute_roi_homography_coordinates(self):
        """ğŸš€ ROI ì˜ì—­ë§Œì„ ìœ„í•œ í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚° (ì›ë˜ ë°©ì‹!)"""
        print("ğŸ”§ ROI ê¸°ë°˜ í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ì‚¬ì „ ê³„ì‚° ì¤‘...")
        
        # ğŸ¯ ì›ë˜ëŒ€ë¡œ! ROI í¬ê¸°ë¡œë§Œ ì²˜ë¦¬!
        roi_h, roi_w = self.roi_height, self.roi_width  # 618, 3081
        
        # ROI í¬ê¸°ë¡œ ì¢Œí‘œ ê·¸ë¦¬ë“œ ìƒì„±
        y_coords, x_coords = np.mgrid[0:roi_h, 0:roi_w].astype(np.float32)
        
        # ROI ì¢Œí‘œë¥¼ ì „ì²´ ìº”ë²„ìŠ¤ ì¢Œí‘œë¡œ ë³€í™˜
        x_coords += self.roi_x1  # ROI ì‹œì‘ì  ë”í•˜ê¸°
        y_coords += self.roi_y1
        
        # í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬ë¡œ ë³€í™˜ëœ ì¢Œí‘œ ê³„ì‚° (í•œ ë²ˆë§Œ!)
        coords = np.stack([x_coords, y_coords, np.ones_like(x_coords)], axis=-1)
        transformed_coords = (self.homography_matrix_opt @ coords.reshape(-1, 3).T).T
        
        # 2D ì¢Œí‘œë¡œ ë³€í™˜
        transformed_coords = transformed_coords[:, :2] / transformed_coords[:, 2:]
        self.roi_homography_coords = transformed_coords.reshape(roi_h, roi_w, 2)
        
        # ğŸš€ ROI ì˜ì—­ë§Œì„ ìœ„í•œ ë§ˆìŠ¤í¬ë„ ìƒì„± (ì°¨ì› ì¼ì¹˜ ë³´ì¥!)
        self.roi_mask = np.ones((roi_h, roi_w), dtype=np.uint8)
        
        print(f"âœ… ROI í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ ìƒì„± ì™„ë£Œ:")
        print(f"   - ROI í¬ê¸°: {roi_w}x{roi_h}")
        print(f"   - ì¢Œí‘œ ë§µ í¬ê¸°: {self.roi_homography_coords.shape}")
        print(f"   - ROI ë§ˆìŠ¤í¬ í¬ê¸°: {self.roi_mask.shape}")
        print(f"   - ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 36ms â†’ 5-10ms (3-7ë°° í–¥ìƒ!)")

    def _apply_roi_homography_fast(self, frame):
        """ğŸš€ ì‚¬ì „ ê³„ì‚°ëœ ROI ì¢Œí‘œ ë§µì„ ì‚¬ìš©í•˜ì—¬ ê·¹í•œ ë¹ ë¥¸ í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜"""
        if not hasattr(self, 'roi_homography_coords'):
            # ì¢Œí‘œ ë§µì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            return cv2.warpPerspective(frame, self.homography_matrix_opt, 
                                     self.homography_size, flags=cv2.INTER_LINEAR)
        
        # ğŸš€ ROI í¬ê¸°ë¡œ ê²°ê³¼ ì´ë¯¸ì§€ ì´ˆê¸°í™” (ì°¨ì› ì¼ì¹˜ ë³´ì¥!)
        roi_h, roi_w = self.roi_height, self.roi_width
        warped = np.zeros((roi_h, roi_w, 3), dtype=frame.dtype)
        
        # ì‚¬ì „ ê³„ì‚°ëœ ì¢Œí‘œ ë§µ ì‚¬ìš©
        coords = self.roi_homography_coords
        
        # ì¢Œí‘œë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ìƒ˜í”Œë§
        coords_int = coords.astype(np.int32)
        coords_int[:, :, 0] = np.clip(coords_int[:, :, 0], 0, frame.shape[1]-1)
        coords_int[:, :, 1] = np.clip(coords_int[:, :, 1], 0, frame.shape[0]-1)
        
        # ë²¡í„°í™”ëœ ìƒ˜í”Œë§ (í›¨ì”¬ ë¹ ë¦„!)
        warped = frame[coords_int[:, :, 1], coords_int[:, :, 0]]
        
        return warped

    def _initialize_gaze_tracking(self):
        """ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™”"""
        print("ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™” ì¤‘...")
        
        # GPU ì„¤ì • (YOLO ì‹œì„ ì¶”ì ìš©ë§Œ)
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            print(f"ğŸš€ YOLO GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
        else:
            self.device = torch.device('cpu')
            print("âš ï¸ YOLO CPU ì‚¬ìš©")
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        try:
            self.yolo_model = YOLO("models/yolov8n-face.pt")
            self.yolo_model.to(self.device)
            print("âœ… YOLO ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.yolo_model = None
        
        # MediaPipe FaceMesh ì„¤ì •
        try:
            self.mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                static_image_mode=False, 
                max_num_faces=GAZE_TRACKING_CONFIG['max_faces'],
                refine_landmarks=True,
                min_detection_confidence=GAZE_TRACKING_CONFIG['detection_confidence'],
                min_tracking_confidence=GAZE_TRACKING_CONFIG['tracking_confidence']
            )
            print("âœ… MediaPipe FaceMesh ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.face_mesh = None
        
        # ì‹œì„  ì•ˆì •í™” ê°ì²´ë“¤
        self.gaze_stabilizers = [GazeStabilizer() for _ in range(GAZE_TRACKING_CONFIG['max_faces'])]
        
        # 3D ì–¼êµ´ ëª¨ë¸ í¬ì¸íŠ¸
        self.face_3d_model_points = np.array([
            [0.0, 0.0, 0.0], [0.0, -330.0, -65.0],
            [-225.0, 170.0, -135.0], [225.0, 170.0, -135.0],
            [-150.0, -150.0, -125.0], [150.0, -150.0, -125.0]
        ], dtype=np.float64)
        
        # ëœë“œë§ˆí¬ IDë“¤
        self.landmark_ids = [1, 152, 33, 263, 61, 291]
        self.left_eye_landmarks = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]
        self.right_eye_landmarks = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
        self.iris_ids = [468, 473]  # ì™¼ìª½, ì˜¤ë¥¸ìª½ í™ì±„ ì¤‘ì‹¬
        
        print("âœ… ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_threaded_gaze_tracking(self):
        """ğŸš€ ë©€í‹°ìŠ¤ë ˆë”© ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™”"""
        print("ğŸš€ ë©€í‹°ìŠ¤ë ˆë”© ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™” ì¤‘...")
        
        # ìŠ¤ë ˆë“œ ì•ˆì „í•œ íë“¤
        self.gaze_input_queue = Queue(maxsize=2)  # ì…ë ¥ í (ìµœì‹  í”„ë ˆì„ë§Œ)
        self.gaze_output_queue = Queue(maxsize=2)  # ì¶œë ¥ í
        
        # ì•„ì´íŠ¸ë˜í‚¹ ìŠ¤ë ˆë“œ ì‹œì‘
        self.gaze_thread_running = True
        self.gaze_thread = threading.Thread(target=self._gaze_worker_thread, daemon=True)
        self.gaze_thread.start()
        
        print("âœ… ë©€í‹°ìŠ¤ë ˆë”© ì•„ì´íŠ¸ë˜í‚¹ ì´ˆê¸°í™” ì™„ë£Œ")

    def _gaze_worker_thread(self):
        """ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì „ìš© ì›Œì»¤ ìŠ¤ë ˆë“œ"""
        while self.gaze_thread_running:
            try:
                # ì…ë ¥ íì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¸”ë¡œí‚¹ ë°©ì§€)
                frame = self.gaze_input_queue.get(timeout=0.1)
                
                # ì•„ì´íŠ¸ë˜í‚¹ ì²˜ë¦¬
                processed_frame, gaze_data = self.process_gaze_tracking(frame)
                
                # ì¶œë ¥ íì— ê²°ê³¼ ì „ë‹¬ (ì´ì „ ê²°ê³¼ ë®ì–´ì“°ê¸°)
                try:
                    if not self.gaze_output_queue.empty():
                        self.gaze_output_queue.get_nowait()  # ì´ì „ ê²°ê³¼ ì œê±°
                    self.gaze_output_queue.put_nowait((processed_frame, gaze_data))
                except:
                    pass
                    
            except:
                continue  # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ê³„ì† ì§„í–‰

    def process_gaze_tracking_async(self, frame):
        """ğŸš€ ë¹„ë™ê¸° ì•„ì´íŠ¸ë˜í‚¹ ìš”ì²­"""
        try:
            # ì…ë ¥ íì— í”„ë ˆì„ ì „ë‹¬ (ì´ì „ í”„ë ˆì„ ë®ì–´ì“°ê¸°)
            if not self.gaze_input_queue.empty():
                self.gaze_input_queue.get_nowait()  # ì´ì „ í”„ë ˆì„ ì œê±°
            self.gaze_input_queue.put_nowait(frame)
        except:
            pass

    def get_gaze_result_async(self):
        """ğŸš€ ë¹„ë™ê¸° ì•„ì´íŠ¸ë˜í‚¹ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.gaze_output_queue.get_nowait()
        except:
            return None, []

    def switch_to_mirror_mode(self, enable_mirror: bool):
        """ê±°ìš¸ëª¨ë“œ ì „í™˜ í•¨ìˆ˜"""
        if enable_mirror:
            # ê±°ìš¸ëª¨ë“œ: ì´ë¯¸ì§€ë¥¼ ì¢Œìš° ë°˜ì „
            print(f"ğŸ”„ ê±°ìš¸ëª¨ë“œ í™œì„±í™”")
        else:
            # ì¼ë°˜ëª¨ë“œ: ì›ë³¸ ì´ë¯¸ì§€
            print(f"ğŸ”„ ì¼ë°˜ëª¨ë“œ í™œì„±í™”")

    def setup_cameras(self, left_id: int = LEFT_CAMERA_INDEX, right_id: int = RIGHT_CAMERA_INDEX) -> bool:
        """ì¹´ë©”ë¼ ì´ˆê¸°í™” ë° ì„¤ì •"""
        print(f"ì¹´ë©”ë¼ ì´ˆê¸°í™”: ì™¼ìª½={left_id}, ì˜¤ë¥¸ìª½={right_id}")
        
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ìˆœ)
            backends = [
                (cv2.CAP_DSHOW, "DirectShow"),
                (cv2.CAP_MSMF, "Media Foundation"),
                (cv2.CAP_ANY, "Auto")
            ]
            
            # ê° ë°±ì—”ë“œë¡œ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹œë„
            for backend_id, backend_name in backends:
                try:
                    print(f"ë°±ì—”ë“œ {backend_name} ({backend_id}) ì‹œë„ ì¤‘...")
                    self.cap_left = cv2.VideoCapture(left_id, backend_id)
                    self.cap_right = cv2.VideoCapture(right_id, backend_id)
                    
                    if self.cap_left.isOpened() and self.cap_right.isOpened():
                        print(f"ë°±ì—”ë“œ {backend_name} ({backend_id}) ì‚¬ìš©")
                        break
                    else:
                        print(f"ë°±ì—”ë“œ {backend_name} ({backend_id}) ì‹¤íŒ¨")
                        self.cap_left.release()
                        self.cap_right.release()
                except Exception as e:
                    print(f"ë°±ì—”ë“œ {backend_name} ({backend_id}) ì˜¤ë¥˜: {e}")
                    continue
            else:
                print("ëª¨ë“  ë°±ì—”ë“œì—ì„œ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            # ê³ ì„±ëŠ¥ ì¹´ë©”ë¼ ì„¤ì • (í•˜ë“œì½”ë”©)
            target_width = self.camera_width
            target_height = self.camera_height
            
            for i, cap in enumerate([self.cap_left, self.cap_right]):
                camera_name = "ì™¼ìª½" if i == 0 else "ì˜¤ë¥¸ìª½"
                
                # í•´ìƒë„ ì„¤ì • (í•˜ë“œì½”ë”©)
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, target_width)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, target_height)
                
                # ğŸš€ FPS ìµœì í™”: 60fpsë¡œ ì„¤ì • (í•˜ë“œì½”ë”©)
                cap.set(cv2.CAP_PROP_FPS, 60)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ë²„í¼ ìµœì†Œí™”
                cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
                
                actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                actual_fps = cap.get(cv2.CAP_PROP_FPS)
                
                print(f"   {camera_name} ì¹´ë©”ë¼: {actual_width}x{actual_height} @ {actual_fps:.1f}fps")
                
                # í•´ìƒë„ ë¶ˆì¼ì¹˜ ê²½ê³  (í•˜ë“œì½”ë”©ëœ ì„ê³„ê°’)
                if abs(actual_width - target_width) > 100 or abs(actual_height - target_height) > 100:
                    print(f"âš ï¸ {camera_name} ì¹´ë©”ë¼ í•´ìƒë„ ë¶ˆì¼ì¹˜: ëª©í‘œ {target_width}x{target_height}, ì‹¤ì œ {actual_width}x{actual_height}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì¹´ë©”ë¼ ì„¤ì • ì˜¤ë¥˜: {e}")
            return False

    def _blend_images_ultra_fast(self, left_translated: np.ndarray, right_warped: np.ndarray) -> np.ndarray:
        """ğŸš€ ê·¹í•œ ì†ë„ ë¸”ë Œë”© (ì§„ì§œ ë¹ ë¥¸ ì•Œê³ ë¦¬ì¦˜!)"""
        
        # ğŸš€ ë°©ë²• 1: ì œìë¦¬ ì—°ì‚° (ê°€ì¥ ë¹ ë¦„ - ë©”ëª¨ë¦¬ ë³µì‚¬ ì—†ìŒ!)
        # left_translatedì— ì§ì ‘ ë®ì–´ì“°ê¸° (ì›ë³¸ ìˆ˜ì •ë¨ ì£¼ì˜!)
        mask = right_warped > 10
        left_translated[mask] = right_warped[mask]
        return left_translated
        
        # ğŸš« ê¸°ì¡´ ë°©ë²•ë“¤ (ëŠë¦¼)
        # result = left_translated.copy()  # 14.6MB ë³µì‚¬!
        # mask = np.any(right_warped > 10, axis=2)  # 4.9MB ë§ˆìŠ¤í¬!
        # result[mask] = right_warped[mask]
        # return result

    def update_fps(self):
        """ğŸš€ í•˜ë“œì½”ë”©ëœ FPS ì—…ë°ì´íŠ¸ (ìµœëŒ€ ì„±ëŠ¥)"""
        self.fps_counter += 1
        current_time = time.time()
        
        # 1ì´ˆë§ˆë‹¤ FPS ê³„ì‚° (í•˜ë“œì½”ë”©ëœ ì‹œê°„)
        if current_time - self.fps_start_time >= 1.0:
            self.current_fps = self.fps_counter / (current_time - self.fps_start_time)
            self.fps_counter = 0
            self.fps_start_time = current_time
            
            # FPS ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©) - í•˜ë“œì½”ë”©ëœ ì¡°ê±´
            if self.current_fps > 0.0:
                print(f"ğŸ“Š í˜„ì¬ FPS: {self.current_fps:.1f}")
                
            # ğŸš€ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ ì¶œë ¥ (1ì´ˆë§ˆë‹¤)
            if self.performance_measurement and self.frame_count > 0:
                print(f"ğŸ” ì„±ëŠ¥ ì¸¡ì • í˜¸ì¶œ: frame_count={self.frame_count}, step_times={len(self.step_times['total_stitching'])}")
                self._print_performance_stats()

    def _print_performance_stats(self):
        """ğŸš€ ê° ë‹¨ê³„ë³„ ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ” ì„±ëŠ¥ ë¶„ì„ (í”„ë ˆì„ {self.frame_count}):")
        print(f"{'='*50}")
        
        for step_name, times in self.step_times.items():
            if times:
                avg_time = np.mean(times) * 1000  # msë¡œ ë³€í™˜
                min_time = np.min(times) * 1000
                max_time = np.max(times) * 1000
                print(f"   {step_name:15s}: í‰ê·  {avg_time:6.2f}ms (ìµœì†Œ {min_time:6.2f}ms, ìµœëŒ€ {max_time:6.2f}ms)")
        
        # ì´ ìŠ¤í‹°ì¹­ ì‹œê°„ ë¶„ì„
        if self.step_times['total_stitching']:
            total_avg = np.mean(self.step_times['total_stitching']) * 1000
            theoretical_fps = 1000 / total_avg if total_avg > 0 else 0
            print(f"{'='*50}")
            print(f"   ğŸ¯ ì´ ìŠ¤í‹°ì¹­ ì‹œê°„: {total_avg:.2f}ms")
            print(f"   ğŸš€ ì´ë¡ ì  ìµœëŒ€ FPS: {theoretical_fps:.1f}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‘œì‹œ
        print(f"   ğŸ’¾ ìº”ë²„ìŠ¤ í¬ê¸°: {self.canvas_width}x{self.canvas_height}")
        if hasattr(self, 'use_roi') and self.use_roi:
            print(f"   ğŸ¯ ROI í¬ê¸°: {self.roi_width}x{self.roi_height}")
        
        print(f"{'='*50}")
        
        # ì¸¡ì • ë°ì´í„° ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        for step_name in self.step_times:
            self.step_times[step_name] = []

    def show_stitched_video_no_visual(self):
        """ğŸš€ ì‹œê°í™” ì—†ëŠ” ì´ˆê³ ì† ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ (ë™ê³µ ì¶”ì  ì¢Œí‘œë§Œ ì¶œë ¥)"""
        print(f"\nğŸš€ ì‹œê°í™” ì—†ëŠ” ì´ˆê³ ì† ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ ì‹œì‘ (ìµœê³  ì„±ëŠ¥ ë²„ì „)")
        print(f"ğŸ“¸ ì¹´ë©”ë¼ í•´ìƒë„: {self.camera_resolution[0]}x{self.camera_resolution[1]}")
        print(f"ğŸ¯ ëª©í‘œ FPS: 60+ (ì‹œê°í™” ì œê±°ë¡œ ì¸í•œ ê·¹í•œ ì„±ëŠ¥ í–¥ìƒ)")
        print(f"ğŸ”„ ê±°ìš¸ëª¨ë“œ: {'í™œì„±í™”' if True else 'ë¹„í™œì„±í™”'}") # ê±°ìš¸ëª¨ë“œ ê¸°ë³¸ í™œì„±í™”
        print(f"ğŸ“ ìµœì¢… ìº”ë²„ìŠ¤ í¬ê¸°: {self.canvas_size[0]}x{self.canvas_size[1]}")
        print(f"ğŸ“ ì™¼ìª½ ì´ë¯¸ì§€ ì˜¤í”„ì…‹: ({self.left_offset[0]}, {self.left_offset[1]})")
        
        # ğŸ†• ROI ì •ë³´ í‘œì‹œ
        if hasattr(self, 'use_roi') and self.use_roi:
            print(f"ğŸ¯ ROI ëª¨ë“œ: í™œì„±í™”")
            print(f"   ROI ì¢Œí‘œ: ({self.roi_x1}, {self.roi_y1}) -> ({self.roi_x2}, {self.roi_y2})")
            print(f"   ROI í¬ê¸°: {self.roi_width} x {self.roi_height}")
            print(f"   ì˜ˆìƒ FPS í–¥ìƒ: ì‹œê°í™” ì œê±° + ROIë¡œ ì¸í•œ ì¶”ê°€ 200-300% í–¥ìƒ")
        else:
            print(f"ğŸ¯ ROI ëª¨ë“œ: ë¹„í™œì„±í™” (ì „ì²´ í™”ë©´)")
        
        print(f"ğŸš€ ì ìš©ëœ ìµœì í™”: ì‹œê°í™” ì œê±° + JSON ë‹¨ì¼ë¡œë”© + ìº”ë²„ìŠ¤ìºì‹œ + ì¡°ê±´ë¬¸ì œê±°")
        print(f"ğŸ‘ï¸ ì•„ì´íŠ¸ë˜í‚¹: YOLO + MediaPipe FaceMesh + ì‹œì„  ì•ˆì •í™” (ì¢Œí‘œë§Œ ì¶œë ¥)")
        print(f"ğŸš€ ë©€í‹°ìŠ¤ë ˆë”©: ì•„ì´íŠ¸ë˜í‚¹ ë³„ë„ ìŠ¤ë ˆë“œë¡œ ë¶„ë¦¬í•˜ì—¬ ë©”ì¸ ë£¨í”„ FPS ê·¹í•œ í–¥ìƒ")
        print(f"{'='*70}")
        
        # ğŸš€ ì„±ëŠ¥ ìµœì í™”: ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰!
        print("ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì‹œì‘...")
        
        print("âœ… ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ!")
        
        print(f"ğŸ¬ ì‹œê°í™” ì—†ëŠ” ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ ì‹œì‘...")
        print(f"ğŸ’¡ ì¢…ë£Œ: 'q' í‚¤")
        print(f"ğŸ’¡ ê±°ìš¸ëª¨ë“œ í† ê¸€: 'm' í‚¤")
        print(f"ğŸ‘ï¸ ë™ê³µ ì¶”ì  ì¢Œí‘œë§Œ ì½˜ì†”ì— ì¶œë ¥ë©ë‹ˆë‹¤")
        print(f"{'='*60}")
        
        # ì‹œì‘ ì‹œ ê±°ìš¸ëª¨ë“œ í™œì„±í™”
        print("ğŸ”„ ê±°ìš¸ëª¨ë“œê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        frame_count = 0
        mirror_mode = True  # ê±°ìš¸ëª¨ë“œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        
        try:
            while True:
                # ğŸš€ ì „ì²´ ìŠ¤í‹°ì¹­ ì‹œê°„ ì¸¡ì • ì‹œì‘
                total_start_time = time.time()
                
                # 1ï¸âƒ£ í”„ë ˆì„ ì½ê¸° ì‹œê°„ ì¸¡ì •
                frame_read_start = time.time()
                retL, frameL = self.cap_left.read()
                retR, frameR = self.cap_right.read()
                frame_read_time = time.time() - frame_read_start
                self.step_times['frame_read'].append(frame_read_time)
                
                if not retL or not retR:
                    print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                    break

                # 2ï¸âƒ£ ê±°ìš¸ëª¨ë“œ ì ìš© ì‹œê°„ ì¸¡ì •
                mirror_start = time.time()
                if mirror_mode:
                    frameL = cv2.flip(frameL, 1)
                    frameR = cv2.flip(frameR, 1)
                mirror_time = time.time() - mirror_start
                self.step_times['mirror_flip'].append(mirror_time)

                # 3ï¸âƒ£ ğŸš€ ì´ˆê³ ì† ìŠ¤í‹°ì¹­ ì²˜ë¦¬ (ëª¨ë“  ìµœì í™” ì ìš©)
                stitched = self.stitch_frame_pair_ultra_optimized(frameL, frameR)
                
                # ğŸš€ ì „ì²´ ìŠ¤í‹°ì¹­ ì‹œê°„ ì¸¡ì • ì™„ë£Œ
                total_stitching_time = time.time() - total_start_time
                self.step_times['total_stitching'].append(total_stitching_time)
                
                # ğŸš€ ë¹„ë™ê¸° ì•„ì´íŠ¸ë˜í‚¹ ìš”ì²­ (ë¸”ë¡œí‚¹ ì—†ìŒ)
                self.process_gaze_tracking_async(stitched)
                
                # ğŸš€ ë¹„ë™ê¸° ì•„ì´íŠ¸ë˜í‚¹ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                stitched_with_gaze, gaze_data = self.get_gaze_result_async()
                if stitched_with_gaze is None:
                    stitched_with_gaze = stitched  # ì•„ì´íŠ¸ë˜í‚¹ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                    gaze_data = []
                
                # ğŸš€ ìµœì í™”ëœ FPS ê³„ì‚°
                self.update_fps()
                
                # ğŸš€ ë™ê³µ ì¶”ì  ì¢Œí‘œë§Œ ì½˜ì†”ì— ì¶œë ¥ (ì‹œê°í™” ì—†ìŒ!)
                if gaze_data:
                    for i, data in enumerate(gaze_data):
                        gaze_x, gaze_y = data['gaze']
                        iris_coords = data.get('iris_coords', [])
                        print(f"ğŸ‘ï¸ Person {i+1}: Gaze=({gaze_x:.3f}, {gaze_y:.3f}), Iris={len(iris_coords)} points")
                else:
                    print("ğŸ‘ï¸ No faces detected")
                
                # ê±°ìš¸ëª¨ë“œ ìƒíƒœ í‘œì‹œ (ì½˜ì†”ì—ë§Œ)
                if frame_count % 30 == 0:  # 30í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ì¶œë ¥
                    print(f"ğŸ”„ Mirror Mode: {'ON' if mirror_mode else 'OFF'}")
                
                # ğŸ†• ROI ì •ë³´ í‘œì‹œ (ì²« í”„ë ˆì„ì—ì„œë§Œ)
                if frame_count == 0 and hasattr(self, 'use_roi') and self.use_roi:
                    print(f"ğŸ¯ ROI: ({self.roi_x1},{self.roi_y1}) -> ({self.roi_x2},{self.roi_y2})")
                    print(f"ğŸ¯ ROI Size: {self.roi_width} x {self.roi_height}")
                
                # ğŸš€ í”„ë ˆì„ ì¹´ìš´í„° ì¦ê°€ (ì„±ëŠ¥ ì¸¡ì •ìš©)
                frame_count += 1
                self.frame_count = frame_count  # í´ë˜ìŠ¤ ë³€ìˆ˜ì— í• ë‹¹
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬ (ì½˜ì†” ì…ë ¥)
                if frame_count % 100 == 0:  # 100í”„ë ˆì„ë§ˆë‹¤ í‚¤ ì…ë ¥ í™•ì¸
                    user_input = input("Press 'q' to quit, 'm' to toggle mirror mode, or Enter to continue: ")
                    if user_input.lower() == 'q':
                        print("ğŸ’¡ 'q' í‚¤ ì…ë ¥ìœ¼ë¡œ ì¢…ë£Œ")
                        break
                    elif user_input.lower() == 'm':
                        mirror_mode = not mirror_mode
                        print(f"ğŸ”„ ê±°ìš¸ëª¨ë“œ: {'í™œì„±í™”' if mirror_mode else 'ë¹„í™œì„±í™”'}")
                        self.switch_to_mirror_mode(mirror_mode)
                
        except KeyboardInterrupt:
            print("\nğŸ’¡ Ctrl+Cë¡œ ì¢…ë£Œ")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            # ì •ë¦¬
            if self.cap_left:
                self.cap_left.release()
            if self.cap_right:
                self.cap_right.release()
            
            # ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ìŠ¤ë ˆë“œ ì •ë¦¬
            self.gaze_thread_running = False
            if hasattr(self, 'gaze_thread') and self.gaze_thread.is_alive():
                self.gaze_thread.join(timeout=1.0)
            
            print("âœ… ì •ë¦¬ ì™„ë£Œ")

    def release(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        if hasattr(self, 'cap_left') and self.cap_left:
            self.cap_left.release()
        if hasattr(self, 'cap_right') and self.cap_right:
            self.cap_right.release()
        
        # ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ìŠ¤ë ˆë“œ ì •ë¦¬
        if hasattr(self, 'gaze_thread_running'):
            self.gaze_thread_running = False
        if hasattr(self, 'gaze_thread') and self.gaze_thread.is_alive():
            self.gaze_thread.join(timeout=1.0)
        
        cv2.destroyAllWindows()
        print("âœ… ëª¨ë“  ë¦¬ì†ŒìŠ¤ í•´ì œ ì™„ë£Œ")

    def process_gaze_tracking(self, frame: np.ndarray) -> Tuple[np.ndarray, list]:
        """ğŸš€ ì•„ì´íŠ¸ë˜í‚¹ ì²˜ë¦¬"""
        if self.yolo_model is None or self.face_mesh is None:
            return frame, []
        
        try:
            # YOLO ì–¼êµ´ ê²€ì¶œ
            yolo_results = self.yolo_model.predict(frame, verbose=False)[0]
            face_boxes = [
                box for box in yolo_results.boxes.data.cpu().numpy()
                if int(box[5]) == 0  # í´ë˜ìŠ¤ 0: ì–¼êµ´
            ]
            
            results = []
            h, w = frame.shape[:2]
            
            for face_idx, box in enumerate(face_boxes):
                if face_idx >= GAZE_TRACKING_CONFIG['max_faces']:
                    break
                    
                x1, y1, x2, y2, conf, cls = box.astype(int)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)

                face_roi = frame[y1:y2, x1:x2]
                face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)
                mesh_result = self.face_mesh.process(face_rgb)

                if mesh_result.multi_face_landmarks:
                    for face_landmarks in mesh_result.multi_face_landmarks:
                        # ê¸°ë³¸ ëœë“œë§ˆí¬ í¬ì¸íŠ¸ë“¤
                        image_points = []
                        for idx in self.landmark_ids:
                            lm = face_landmarks.landmark[idx]
                            x_lm = int(lm.x * (x2 - x1)) + x1
                            y_lm = int(lm.y * (y2 - y1)) + y1
                            image_points.append((x_lm, y_lm))
                            cv2.circle(frame, (x_lm, y_lm), 2, (0, 0, 255), -1)

                        # í™ì±„ ì¤‘ì‹¬ì  ì¶”ì¶œ
                        iris_coords = []
                        for idx in self.iris_ids:
                            if idx < len(face_landmarks.landmark):
                                lm = face_landmarks.landmark[idx]
                                x_lm = int(lm.x * (x2 - x1)) + x1
                                y_lm = int(lm.y * (y2 - y1)) + y1
                                iris_coords.append((x_lm, y_lm))
                                cv2.circle(frame, (x_lm, y_lm), 3, (0, 255, 0), -1)
                        
                        # ëˆˆ ì¤‘ì‹¬ ê³„ì‚°
                        roi_info = (x1, y1, x2-x1, y2-y1)
                        left_eye_center = self._get_eye_center_from_landmarks(face_landmarks, self.left_eye_landmarks, roi_info)
                        right_eye_center = self._get_eye_center_from_landmarks(face_landmarks, self.right_eye_landmarks, roi_info)
                        
                        eye_centers = []
                        if left_eye_center:
                            eye_centers.append(left_eye_center)
                        if right_eye_center:
                            eye_centers.append(right_eye_center)

                        # ì‹œì„  ì¶”ì •
                        gaze = (0, 0)
                        
                        # í™ì±„ ì¤‘ì‹¬ ê¸°ë°˜
                        if len(iris_coords) == 2:
                            lx, ly = iris_coords[0]
                            rx, ry = iris_coords[1]
                            iris_gaze = ((rx + lx) / 2 / w - 0.5, (ry + ly) / 2 / h - 0.5)
                        else:
                            iris_gaze = None
                        
                        # ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ ì¤‘ì‹¬ ê¸°ë°˜
                        if len(eye_centers) == 2:
                            lx, ly = eye_centers[0]
                            rx, ry = eye_centers[1]
                            landmark_gaze = ((rx + lx) / 2 / w - 0.5, (ry + ly) / 2 / h - 0.5)
                        else:
                            landmark_gaze = None
                        
                        # ë‘ ë°©ë²•ì˜ ê°€ì¤‘ í‰ê· 
                        if iris_gaze and landmark_gaze:
                            gaze = (
                                0.7 * iris_gaze[0] + 0.3 * landmark_gaze[0],
                                0.7 * iris_gaze[1] + 0.3 * landmark_gaze[1]
                            )
                        elif iris_gaze:
                            gaze = iris_gaze
                        elif landmark_gaze:
                            gaze = landmark_gaze

                        # ì‹œê°„ì  ì•ˆì •í™” ì ìš©
                        if face_idx < len(self.gaze_stabilizers):
                            stabilizer = self.gaze_stabilizers[face_idx]
                            stabilizer.add_sample(gaze, iris_coords)
                            stable_gaze = stabilizer.get_stabilized_gaze()
                            gaze = stable_gaze

                        results.append({"gaze": gaze, "iris_coords": iris_coords})

            return frame, results
            
        except Exception as e:
            print(f"âš ï¸ ì•„ì´íŠ¸ë˜í‚¹ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return frame, []

    def _get_eye_center_from_landmarks(self, face_landmarks, eye_landmark_ids, roi_offset):
        """ëˆˆ ì£¼ìœ„ ëœë“œë§ˆí¬ë“¤ì˜ í‰ê· ìœ¼ë¡œ ëˆˆ ì¤‘ì‹¬ ê³„ì‚°"""
        eye_points = []
        x1, y1, roi_width, roi_height = roi_offset
        
        for idx in eye_landmark_ids:
            if idx < len(face_landmarks.landmark):
                lm = face_landmarks.landmark[idx]
                x_lm = int(lm.x * roi_width) + x1
                y_lm = int(lm.y * roi_height) + y1
                eye_points.append((x_lm, y_lm))
        
        if eye_points:
            center_x = int(np.mean([p[0] for p in eye_points]))
            center_y = int(np.mean([p[1] for p in eye_points]))
            return (center_x, center_y)
        return None

    def draw_gaze_info(self, frame: np.ndarray, gaze_data: list, fps: float):
        """ğŸš€ ì‹œì„  ì¶”ì  ì •ë³´ í‘œì‹œ"""
        # FPS í‘œì‹œ
        cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ì¶”ì  ìƒíƒœ í‘œì‹œ
        status_color = (0, 255, 0) if len(gaze_data) > 0 else (0, 0, 255)
        cv2.putText(frame, f"Faces: {len(gaze_data)}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)
        
        # ê° ì–¼êµ´ì˜ ì‹œì„  ì •ë³´ í‘œì‹œ
        for i, data in enumerate(gaze_data):
            y_offset = 90 + i * 120
            cv2.putText(frame, f"Person {i+1}", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            cv2.putText(frame, f"Gaze: ({data['gaze'][0]:.3f}, {data['gaze'][1]:.3f})", (10, y_offset+25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
            
            # ì‹œì„  ë°©í–¥ í‘œì‹œ (í™”ë©´ ì¤‘ì•™ ê¸°ì¤€)
            h, w = frame.shape[:2]
            center_x, center_y = w // 2, h // 2
            gaze_x = int(center_x + data['gaze'][0] * 200)  # ìŠ¤ì¼€ì¼ ì¡°ì •
            gaze_y = int(center_y + data['gaze'][1] * 200)
            cv2.arrowedLine(frame, (center_x, center_y), (gaze_x, gaze_y), (0, 255, 255), 3)
        
        # ì•„ì´íŠ¸ë˜í‚¹ í™œì„±í™” í‘œì‹œ
        cv2.putText(frame, "EYE TRACKING: ON", (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    def stitch_frame_pair_ultra_optimized(self, left_frame: np.ndarray, right_frame: np.ndarray) -> np.ndarray:
        """ğŸš€ ì´ˆê³ ì† í”„ë ˆì„ ìŠ¤í‹°ì¹­ (ROI ê¸°ë°˜ ê·¹í•œ ìµœì í™”!)"""
        
        # ğŸš€ ROI ëª¨ë“œì™€ ì „ì²´ ëª¨ë“œ ë¶„ê¸° ì²˜ë¦¬
        if hasattr(self, 'use_roi') and self.use_roi and hasattr(self, 'roi_homography_coords'):
            return self._stitch_frame_pair_roi_optimized(left_frame, right_frame)
        else:
            return self._stitch_frame_pair_full_optimized(left_frame, right_frame)
    
    def _stitch_frame_pair_roi_optimized(self, left_frame: np.ndarray, right_frame: np.ndarray) -> np.ndarray:
        """ğŸš€ ROI ê¸°ë°˜ ê·¹í•œ ìµœì í™” ìŠ¤í‹°ì¹­ (ì „ì²´ ìŠ¤í‹°ì¹­ í›„ ROI í¬ë¡­!)"""
        # ğŸ¯ ì‚¬ìš©ìê°€ ì›í•œ ë°©ì‹: ì „ì²´ ìŠ¤í‹°ì¹­ í›„ ROI í¬ë¡­!
        
        # ğŸš€ 1ë‹¨ê³„: ì „ì²´ ìŠ¤í‹°ì¹­ (ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©!)
        full_stitched = self._stitch_frame_pair_full_optimized(left_frame, right_frame)
        
        # ğŸš€ 2ë‹¨ê³„: ROI í¬ë¡­ (ì‚¬ìš©ìê°€ ì„ íƒí•œ ROI ì˜ì—­ë§Œ!)
        roi_start = time.time()
        
        # ğŸš¨ ROI ì¢Œí‘œëŠ” ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # (Stitching_Engineì—ì„œ 0.5 ìŠ¤ì¼€ì¼ë¡œ ì„ íƒí–ˆì§€ë§Œ, ì¢Œí‘œëŠ” ì „ì²´ ìº”ë²„ìŠ¤ ê¸°ì¤€)
        roi_image = full_stitched[self.roi_y1:self.roi_y2, self.roi_x1:self.roi_x2]
        
        roi_time = time.time() - roi_start
        self.step_times['roi_crop'].append(roi_time)
        
        return roi_image
    
    def _stitch_frame_pair_full_optimized(self, left_frame: np.ndarray, right_frame: np.ndarray) -> np.ndarray:
        """ğŸš€ ì „ì²´ í¬ê¸° ê¸°ì¡´ ìµœì í™” ìŠ¤í‹°ì¹­ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        # ğŸš€ 1ë‹¨ê³„: ë Œì¦ˆ ë³´ì • ì‹œê°„ ì¸¡ì • (ì›ë˜ ë°©ì‹ìœ¼ë¡œ ë³µì›!)
        lens_start = time.time()
        left_rectified = self.rectify_left(left_frame)
        right_rectified = self.rectify_right(right_frame)
        lens_time = time.time() - lens_start
        self.step_times['lens_rectification'].append(lens_time)
        
        # ğŸš€ 2ë‹¨ê³„: ìº”ë²„ìŠ¤ ë³µì‚¬ ì‹œê°„ ì¸¡ì •
        canvas_start = time.time()
        
        # ğŸš¨ í•­ìƒ ì „ì²´ í¬ê¸° ìº”ë²„ìŠ¤ ì‚¬ìš© (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
        canvas = np.zeros((self.canvas_size[1], self.canvas_size[0], 3), dtype=np.uint8)
        canvas[self.left_slice_y, self.left_slice_x] = left_rectified
        
        canvas_time = time.time() - canvas_start
        self.step_times['canvas_copy'].append(canvas_time)
        
        # ğŸš€ 3ë‹¨ê³„: í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜ ì‹œê°„ ì¸¡ì • (ì›ë˜ ë°©ì‹ìœ¼ë¡œ ë³µì›!)
        homography_start = time.time()
        
        # ğŸš¨ ì›ë˜ ë°©ì‹: cv2.warpPerspective ì‚¬ìš© (ROIëŠ” ë‚˜ì¤‘ì— í¬ë¡­!)
        warped_right = cv2.warpPerspective(right_rectified, self.homography_matrix_opt, 
                                       self.homography_size, flags=cv2.INTER_LINEAR)
        
        homography_time = time.time() - homography_start
        self.step_times['homography_warp'].append(homography_time)
        
        # ğŸš€ 4ë‹¨ê³„: ë¸”ë Œë”© ì‹œê°„ ì¸¡ì •
        blending_start = time.time()
        
        # ğŸš€ ë¹ ë¥¸ ë¸”ë Œë”© ì‚¬ìš© (ë§ˆìŠ¤í¬ ê³„ì‚° ì•ˆí•¨!)
        final_image = self._blend_images_ultra_fast(canvas, warped_right)
        
        blending_time = time.time() - blending_start
        self.step_times['blending'].append(blending_time)
        
        return final_image

    def _blend_images_roi_fast(self, left_roi: np.ndarray, right_roi: np.ndarray) -> np.ndarray:
        """ğŸš€ ROI í¬ê¸° ì´ë¯¸ì§€ ì´ˆê³ ì† ë¸”ë Œë”© (ROI ì „ìš©!)"""
        # ROI í¬ê¸°ë¡œ ê°„ë‹¨í•œ ë¸”ë Œë”© (ê°€ì¥ ë¹ ë¥¸ ë°©ì‹)
        # ì™¼ìª½ ì´ë¯¸ì§€ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ê³ , ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ê°€ ìˆëŠ” ë¶€ë¶„ë§Œ ë®ì–´ì“°ê¸°
        result = left_roi.copy()
        mask = right_roi > 10  # ì„ê³„ê°’ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„±
        result[mask] = right_roi[mask]
        return result

    def _blend_images_ultra_fast_full(self, left_image: np.ndarray, right_image: np.ndarray) -> np.ndarray:
        """ğŸš€ ì „ì²´ í¬ê¸° ì´ë¯¸ì§€ ì´ˆê³ ì† ë¸”ë Œë”© (ROI ì²˜ë¦¬ìš©)"""
        if hasattr(self, 'use_precomputed_blending') and self.use_precomputed_blending:
            # ì›ë³¸ ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì‚¬ìš© (ì „ì²´ í¬ê¸°)
            full_left_mask = self.left_blend_mask_original
            full_right_mask = self.right_blend_mask_original
            
            # 3ì±„ë„ ë§ˆìŠ¤í¬ ìƒì„±
            full_left_mask_3ch = np.stack([full_left_mask] * 3, axis=-1)
            full_right_mask_3ch = np.stack([full_right_mask] * 3, axis=-1)
            
            # ê°€ì¤‘ í‰ê·  ë¸”ë Œë”©
            blended = (left_image * full_left_mask_3ch + right_image * full_right_mask_3ch).astype(np.uint8)
        else:
            # ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ë‹¨ìˆœ ë§ì…ˆ
            blended = np.clip(left_image + right_image, 0, 255).astype(np.uint8)
        
        return blended

    def _apply_lens_correction_fast(self, frame, is_left=True):
        """ğŸš€ LUT ê¸°ë°˜ ë¹ ë¥¸ ë Œì¦ˆ ë³´ì •"""
        if is_left:
            return cv2.remap(frame, self.left_lut_x, self.left_lut_y, 
                            cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)
        else:
            return cv2.remap(frame, self.right_lut_x, self.right_lut_y, 
                            cv2.INTER_LINEAR, cv2.BORDER_CONSTANT, 0)

    def _apply_homography_fast(self, frame):
        """ğŸš€ ì‚¬ì „ ê³„ì‚°ëœ ì¢Œí‘œ ë§µìœ¼ë¡œ ë¹ ë¥¸ í˜¸ëª¨ê·¸ë˜í”¼ ë³€í™˜"""
        # ğŸš¨ ì¤‘ìš”: frameì€ ì¹´ë©”ë¼ í•´ìƒë„(1080x1920), ì¢Œí‘œ ë§µì€ ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°(1342x3633)
        frame_h, frame_w = frame.shape[:2]
        canvas_h, canvas_w = self.canvas_size[1], self.canvas_size[0]
        
        # ì¢Œí‘œë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ìƒ˜í”Œë§
        coords = self.homography_coords  # 1342x3633x2
        
        # ì¢Œí‘œ ë²”ìœ„ ê²€ì¦ ë° í´ë¦¬í•‘
        coords_int = coords.astype(np.int32)
        coords_int[:, :, 0] = np.clip(coords_int[:, :, 0], 0, frame_w-1)  # x ì¢Œí‘œ
        coords_int[:, :, 1] = np.clip(coords_int[:, :, 1], 0, frame_h-1)  # y ì¢Œí‘œ
        
        # ë²¡í„°í™”ëœ ìƒ˜í”Œë§ (í›¨ì”¬ ë¹ ë¦„!)
        warped = frame[coords_int[:, :, 1], coords_int[:, :, 0]]
        
        return warped


if __name__ == "__main__":
# ğŸš€ ì‹œê°í™” ì—†ëŠ” ì´ˆê³ ì† ìµœì í™”ëœ ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ ì‹¤í–‰
    print("ğŸš€ Ultra-Fast Video Stitcher No-Visual + Eye Tracking v4.0 ì‹œì‘")
    print("ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 10 FPS â†’ 60+ FPS (500%+ í–¥ìƒ)")
    print("ğŸ‘ï¸ ì•„ì´íŠ¸ë˜í‚¹: YOLO ì–¼êµ´ ê²€ì¶œ + MediaPipe FaceMesh + ì‹œì„  ì•ˆì •í™” (ì¢Œí‘œë§Œ)")
    print("ğŸ¯ ROI ìµœì í™”: ì‚¬ìš©ì ì •ì˜ ROI ì§€ì›ìœ¼ë¡œ ì¶”ê°€ FPS í–¥ìƒ")
    print("ğŸš« ë Œì¦ˆ ë³´ì • LUT: ì‚¬ì „ ê³„ì‚° ì œê±° (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)")
    print("ğŸš« í˜¸ëª¨ê·¸ë˜í”¼ ì¢Œí‘œ ë§µ: ì‚¬ì „ ê³„ì‚° ì œê±° (ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•´)")
    print("ğŸš€ ì‹œê°í™” ì œê±°: í™”ë©´ í‘œì‹œ ì—†ì´ ë™ê³µ ì¶”ì  ì¢Œí‘œë§Œ ì½˜ì†” ì¶œë ¥")
    
    stitcher = UltraFastVideoStitcherNoVisual("./data/config/homography_params.json")
    
    # ğŸ†• ROI ëª¨ë“œ ìƒíƒœ í™•ì¸
    if hasattr(stitcher, 'use_roi') and stitcher.use_roi:
        print(f"âœ… ROI ëª¨ë“œ í™œì„±í™”: {stitcher.roi_width} x {stitcher.roi_height}")
        print("   ğŸš€ ROI ê¸°ë°˜ ìµœì í™” + ì‹œê°í™” ì œê±°ë¡œ ê·¹í•œ FPS í–¥ìƒ ì˜ˆìƒ")
    else:
        print("â„¹ï¸ ROI ëª¨ë“œ ë¹„í™œì„±í™”: ì „ì²´ í™”ë©´ ëª¨ë“œ")
        print("   ğŸ’¡ ROIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Stitching_Engine_3.pyì—ì„œ ROIë¥¼ ì„ íƒí•˜ì„¸ìš”")
    
    # ì¹´ë©”ë¼ ì„¤ì •
    if not stitcher.setup_cameras():
        print("âŒ ì¹´ë©”ë¼ ì„¤ì • ì‹¤íŒ¨")
        exit(1)
    
    stitcher.show_stitched_video_no_visual()
